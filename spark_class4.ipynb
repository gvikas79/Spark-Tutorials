{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3cPtBet3m1y+qASZ2AZ25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvikas79/Spark-Tutorials/blob/main/spark_class4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "002f68e7"
      },
      "source": [
        "## `explode()` in PySpark\n",
        "\n",
        "The `explode()` function is used to create a new row for each element in an array or map column. It essentially transforms a single row with an array/map into multiple rows, with each new row containing one element from the original array/map.\n",
        "\n",
        "This is particularly useful when you have nested data structures (arrays or maps) in your DataFrame and you want to flatten them for further processing or analysis.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f20e6a1e",
        "outputId": "8b625c36-f338-4964-a2fb-8ed36ec6c252"
      },
      "source": [
        "# Example 1: explode() with an array column\n",
        "\n",
        "from pyspark.sql.functions import explode, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame with an array column\n",
        "data = [\n",
        "    (\"Alice\", [\"Math\", \"Science\"]),\n",
        "    (\"Bob\", [\"History\"]),\n",
        "    (\"Charlie\", []), # Empty array\n",
        "    (\"David\", None) # Null array\n",
        "]\n",
        "columns = [\"name\", \"subjects\"]\n",
        "df_array = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_array.show(truncate=False)\n",
        "\n",
        "# Use explode() on the 'subjects' array column\n",
        "df_exploded_array = df_array.select(col(\"name\"), explode(col(\"subjects\")).alias(\"subject\"))\n",
        "\n",
        "print(\"DataFrame after explode() on array column:\")\n",
        "df_exploded_array.show(truncate=False)\n",
        "\n",
        "# Note: Rows with empty or null arrays are dropped by default."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---------------+\n",
            "|name   |subjects       |\n",
            "+-------+---------------+\n",
            "|Alice  |[Math, Science]|\n",
            "|Bob    |[History]      |\n",
            "|Charlie|[]             |\n",
            "|David  |NULL           |\n",
            "+-------+---------------+\n",
            "\n",
            "DataFrame after explode() on array column:\n",
            "+-----+-------+\n",
            "|name |subject|\n",
            "+-----+-------+\n",
            "|Alice|Math   |\n",
            "|Alice|Science|\n",
            "|Bob  |History|\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d603b8f",
        "outputId": "6340178b-7fae-4ff0-f13b-574a9093fe1d"
      },
      "source": [
        "# Example 2: explode() with a map column\n",
        "\n",
        "from pyspark.sql.functions import explode, col, create_map, lit\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField\n",
        "\n",
        "# Sample DataFrame with a map column\n",
        "data = [\n",
        "    (\"Alice\", {\"Math\": 90, \"Science\": 85}),\n",
        "    (\"Bob\", {\"History\": 75}),\n",
        "    (\"Charlie\", {}), # Empty map\n",
        "    (\"David\", None) # Null map\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"scores\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "df_map = spark.createDataFrame(data, schema)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_map.show(truncate=False)\n",
        "\n",
        "# Use explode() on the 'scores' map column\n",
        "# explode() on a map results in two columns: 'key' and 'value'\n",
        "df_exploded_map = df_map.select(col(\"name\"), explode(col(\"scores\")))\n",
        "\n",
        "print(\"DataFrame after explode() on map column:\")\n",
        "df_exploded_map.show(truncate=False)\n",
        "\n",
        "# You can rename the resulting columns\n",
        "df_exploded_map_renamed = df_map.select(col(\"name\"), explode(col(\"scores\")).alias(\"course\", \"score\"))\n",
        "\n",
        "print(\"DataFrame after explode() on map column (renamed columns):\")\n",
        "df_exploded_map_renamed.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---------------------------+\n",
            "|name   |scores                     |\n",
            "+-------+---------------------------+\n",
            "|Alice  |{Science -> 85, Math -> 90}|\n",
            "|Bob    |{History -> 75}            |\n",
            "|Charlie|{}                         |\n",
            "|David  |NULL                       |\n",
            "+-------+---------------------------+\n",
            "\n",
            "DataFrame after explode() on map column:\n",
            "+-----+-------+-----+\n",
            "|name |key    |value|\n",
            "+-----+-------+-----+\n",
            "|Alice|Science|85   |\n",
            "|Alice|Math   |90   |\n",
            "|Bob  |History|75   |\n",
            "+-----+-------+-----+\n",
            "\n",
            "DataFrame after explode() on map column (renamed columns):\n",
            "+-----+-------+-----+\n",
            "|name |course |score|\n",
            "+-----+-------+-----+\n",
            "|Alice|Science|85   |\n",
            "|Alice|Math   |90   |\n",
            "|Bob  |History|75   |\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8edb1491",
        "outputId": "3a6fad0a-524f-4d91-afdd-17c88819cefc"
      },
      "source": [
        "# Example 3: explode_outer()\n",
        "\n",
        "from pyspark.sql.functions import explode_outer, col\n",
        "\n",
        "# Assume df_array is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_array.show(truncate=False)\n",
        "\n",
        "# Use explode_outer() on the 'subjects' array column\n",
        "df_exploded_outer_array = df_array.select(col(\"name\"), explode_outer(col(\"subjects\")).alias(\"subject\"))\n",
        "\n",
        "print(\"DataFrame after explode_outer() on array column:\")\n",
        "df_exploded_outer_array.show(truncate=False)\n",
        "\n",
        "# Assume df_map is already created from Example 2\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_map.show(truncate=False)\n",
        "\n",
        "# Use explode_outer() on the 'scores' map column\n",
        "df_exploded_outer_map = df_map.select(col(\"name\"), explode_outer(col(\"scores\")).alias(\"course\", \"score\"))\n",
        "\n",
        "print(\"DataFrame after explode_outer() on map column:\")\n",
        "df_exploded_outer_map.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---------------+\n",
            "|name   |subjects       |\n",
            "+-------+---------------+\n",
            "|Alice  |[Math, Science]|\n",
            "|Bob    |[History]      |\n",
            "|Charlie|[]             |\n",
            "|David  |NULL           |\n",
            "+-------+---------------+\n",
            "\n",
            "DataFrame after explode_outer() on array column:\n",
            "+-------+-------+\n",
            "|name   |subject|\n",
            "+-------+-------+\n",
            "|Alice  |Math   |\n",
            "|Alice  |Science|\n",
            "|Bob    |History|\n",
            "|Charlie|NULL   |\n",
            "|David  |NULL   |\n",
            "+-------+-------+\n",
            "\n",
            "Original DataFrame:\n",
            "+-------+---------------------------+\n",
            "|name   |scores                     |\n",
            "+-------+---------------------------+\n",
            "|Alice  |{Science -> 85, Math -> 90}|\n",
            "|Bob    |{History -> 75}            |\n",
            "|Charlie|{}                         |\n",
            "|David  |NULL                       |\n",
            "+-------+---------------------------+\n",
            "\n",
            "DataFrame after explode_outer() on map column:\n",
            "+-------+-------+-----+\n",
            "|name   |course |score|\n",
            "+-------+-------+-----+\n",
            "|Alice  |Science|85   |\n",
            "|Alice  |Math   |90   |\n",
            "|Bob    |History|75   |\n",
            "|Charlie|NULL   |NULL |\n",
            "|David  |NULL   |NULL |\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab01de83",
        "outputId": "7d15f70c-478b-4ea5-ac39-31e48b917fa7"
      },
      "source": [
        "df_exploded_array.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- subject: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d2c522"
      },
      "source": [
        "**Handling Nulls and Empty Arrays/Maps:**\n",
        "\n",
        "By default, `explode()` drops rows where the array or map column is null or empty.\n",
        "\n",
        "If you want to keep these rows and have nulls in the exploded columns, you can use `explode_outer()`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The explode_outer() function is used on the original DataFrame containing the array (or map) column, not on a DataFrame that has already been exploded.\n",
        "\n",
        "You use explode_outer() in the same way you would use explode(), but it will include rows where the array or map is NULL or empty, resulting in NULL values in the new exploded column(s).\n",
        "\n",
        "I demonstrated this in Example 3 (cell 8edb1491), where explode_outer(col(\"subjects\")) was applied to the original df_array DataFrame."
      ],
      "metadata": {
        "id": "3h805UC5PJc8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4894c9d9"
      },
      "source": [
        "## `create_map()` in PySpark\n",
        "\n",
        "The `create_map()` function in PySpark is used to create a new map column (key-value pairs) from existing columns or literal values. It's a function available in `pyspark.sql.functions`.\n",
        "\n",
        "This function is useful for structuring data into a map format, which can then be used for various operations, including working with `MapType` columns or preparing data for nested structures.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create_map(lit(key1), lit(value1), lit(key2), lit(value2))"
      ],
      "metadata": {
        "id": "0Fa77DecQSFD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5663cba",
        "outputId": "22945070-647f-4b42-e721-41a91db6eade"
      },
      "source": [
        "# Example 1: create_map() from existing columns\n",
        "\n",
        "from pyspark.sql.functions import create_map, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CreateMapExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"Alice\", \"Math\", 90, \"Science\", 85),\n",
        "    (\"Bob\", \"History\", 75, \"Art\", 88),\n",
        "    (\"Charlie\", \"Physics\", 92, \"Chemistry\", 80)\n",
        "]\n",
        "columns = [\"name\", \"subject1_name\", \"subject1_score\", \"subject2_name\", \"subject2_score\"]\n",
        "df_subjects = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_subjects.show()\n",
        "\n",
        "# Create a map column from subject name and score pairs\n",
        "df_with_map = df_subjects.withColumn(\"scores_map\",\n",
        "                                     create_map(\n",
        "                                         col(\"subject1_name\"), col(\"subject1_score\"),\n",
        "                                         col(\"subject2_name\"), col(\"subject2_score\")\n",
        "                                     ))\n",
        "\n",
        "print(\"DataFrame after creating a map column:\")\n",
        "df_with_map.show(truncate=False)\n",
        "\n",
        "df_with_map.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+-------------+--------------+-------------+--------------+\n",
            "|   name|subject1_name|subject1_score|subject2_name|subject2_score|\n",
            "+-------+-------------+--------------+-------------+--------------+\n",
            "|  Alice|         Math|            90|      Science|            85|\n",
            "|    Bob|      History|            75|          Art|            88|\n",
            "|Charlie|      Physics|            92|    Chemistry|            80|\n",
            "+-------+-------------+--------------+-------------+--------------+\n",
            "\n",
            "DataFrame after creating a map column:\n",
            "+-------+-------------+--------------+-------------+--------------+--------------------------------+\n",
            "|name   |subject1_name|subject1_score|subject2_name|subject2_score|scores_map                      |\n",
            "+-------+-------------+--------------+-------------+--------------+--------------------------------+\n",
            "|Alice  |Math         |90            |Science      |85            |{Math -> 90, Science -> 85}     |\n",
            "|Bob    |History      |75            |Art          |88            |{History -> 75, Art -> 88}      |\n",
            "|Charlie|Physics      |92            |Chemistry    |80            |{Physics -> 92, Chemistry -> 80}|\n",
            "+-------+-------------+--------------+-------------+--------------+--------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- subject1_name: string (nullable = true)\n",
            " |-- subject1_score: long (nullable = true)\n",
            " |-- subject2_name: string (nullable = true)\n",
            " |-- subject2_score: long (nullable = true)\n",
            " |-- scores_map: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: long (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d67d7fa",
        "outputId": "59df4004-0e5b-41b1-bd05-f9cd8107aac3"
      },
      "source": [
        "# Example 2: create_map() from literal values\n",
        "\n",
        "from pyspark.sql.functions import create_map, lit\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Assume spark is already created\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "columns = [\"name\", \"age\"]\n",
        "df_lit = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_lit.show()\n",
        "\n",
        "# Add a map column with fixed literal values\n",
        "df_with_literal_map = df_lit.withColumn(\"info\", create_map(\n",
        "    lit(\"city\"), lit(\"New York\"),\n",
        "    lit(\"country\"), lit(\"USA\")\n",
        "))\n",
        "\n",
        "print(\"DataFrame after adding a map column with literal values:\")\n",
        "df_with_literal_map.show(truncate=False)\n",
        "\n",
        "df_with_literal_map.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "+-----+---+\n",
            "\n",
            "DataFrame after adding a map column with literal values:\n",
            "+-----+---+----------------------------------+\n",
            "|name |age|info                              |\n",
            "+-----+---+----------------------------------+\n",
            "|Alice|25 |{city -> New York, country -> USA}|\n",
            "|Bob  |30 |{city -> New York, country -> USA}|\n",
            "+-----+---+----------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- info: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "478fc3ff",
        "outputId": "402ba089-070f-4b0d-81a0-e1f94cb890dd"
      },
      "source": [
        "# Example 3: create_map() with mixed columns and literals\n",
        "\n",
        "from pyspark.sql.functions import create_map, col, lit\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Assume spark is already created\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25, \"Engineer\"), (\"Bob\", 30, \"Doctor\")]\n",
        "columns = [\"name\", \"age\", \"occupation\"]\n",
        "df_mixed = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_mixed.show()\n",
        "\n",
        "# Create a map column using a mix of columns and literals\n",
        "df_with_mixed_map = df_mixed.withColumn(\"details\", create_map(\n",
        "    lit(\"age\"), col(\"age\").cast(\"string\"), # Cast age to string for consistency\n",
        "    lit(\"occupation\"), col(\"occupation\"),\n",
        "    lit(\"status\"), lit(\"active\")\n",
        "))\n",
        "\n",
        "print(\"DataFrame after creating a map column with mixed types:\")\n",
        "df_with_mixed_map.show(truncate=False)\n",
        "\n",
        "df_with_mixed_map.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-----+---+----------+\n",
            "| name|age|occupation|\n",
            "+-----+---+----------+\n",
            "|Alice| 25|  Engineer|\n",
            "|  Bob| 30|    Doctor|\n",
            "+-----+---+----------+\n",
            "\n",
            "DataFrame after creating a map column with mixed types:\n",
            "+-----+---+----------+-----------------------------------------------------+\n",
            "|name |age|occupation|details                                              |\n",
            "+-----+---+----------+-----------------------------------------------------+\n",
            "|Alice|25 |Engineer  |{age -> 25, occupation -> Engineer, status -> active}|\n",
            "|Bob  |30 |Doctor    |{age -> 30, occupation -> Doctor, status -> active}  |\n",
            "+-----+---+----------+-----------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- occupation: string (nullable = true)\n",
            " |-- details: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "paMl3SvyQXFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42d38ef"
      },
      "source": [
        "## `map_keys()` and `map_values()` in PySpark\n",
        "\n",
        "`map_keys()` and `map_values()` are PySpark SQL functions used to extract the keys and values, respectively, from a MapType column in a DataFrame.\n",
        "\n",
        "*   **`map_keys(col)`**: Returns an array containing all the keys in the MapType column. The order of keys in the array is not guaranteed.\n",
        "*   **`map_values(col)`**: Returns an array containing all the values in the MapType column. The order of values in the array corresponds to the order of keys returned by `map_keys()`.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8de9351",
        "outputId": "256184c7-870b-411c-8c10-35d562a57aa6"
      },
      "source": [
        "from pyspark.sql.functions import map_keys, map_values, col\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MapKeysValuesExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame with a MapType column (using the df_map from a previous example)\n",
        "data = [\n",
        "    (\"Alice\", {\"Math\": 90, \"Science\": 85}),\n",
        "    (\"Bob\", {\"History\": 75}),\n",
        "    (\"Charlie\", {}), # Empty map\n",
        "    (\"David\", None) # Null map\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"scores\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "df_map = spark.createDataFrame(data, schema)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_map.show(truncate=False)\n",
        "df_map.printSchema()\n",
        "\n",
        "# Example 1: Using map_keys()\n",
        "df_keys = df_map.select(col(\"name\"), map_keys(col(\"scores\")).alias(\"score_keys\"))\n",
        "\n",
        "print(\"DataFrame with score keys:\")\n",
        "df_keys.show(truncate=False)\n",
        "df_keys.printSchema()\n",
        "\n",
        "# Example 2: Using map_values()\n",
        "df_values = df_map.select(col(\"name\"), map_values(col(\"scores\")).alias(\"score_values\"))\n",
        "\n",
        "print(\"DataFrame with score values:\")\n",
        "df_values.show(truncate=False)\n",
        "df_values.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---------------------------+\n",
            "|name   |scores                     |\n",
            "+-------+---------------------------+\n",
            "|Alice  |{Science -> 85, Math -> 90}|\n",
            "|Bob    |{History -> 75}            |\n",
            "|Charlie|{}                         |\n",
            "|David  |NULL                       |\n",
            "+-------+---------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- scores: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n",
            "DataFrame with score keys:\n",
            "+-------+---------------+\n",
            "|name   |score_keys     |\n",
            "+-------+---------------+\n",
            "|Alice  |[Science, Math]|\n",
            "|Bob    |[History]      |\n",
            "|Charlie|[]             |\n",
            "|David  |NULL           |\n",
            "+-------+---------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- score_keys: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "DataFrame with score values:\n",
            "+-------+------------+\n",
            "|name   |score_values|\n",
            "+-------+------------+\n",
            "|Alice  |[85, 90]    |\n",
            "|Bob    |[75]        |\n",
            "|Charlie|[]          |\n",
            "|David  |NULL        |\n",
            "+-------+------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- score_values: array (nullable = true)\n",
            " |    |-- element: integer (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BCgEZIlMQ0f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65a0d01b",
        "outputId": "193c9297-ab48-41cc-efe4-5adf9deebb5b"
      },
      "source": [
        "from pyspark.sql.functions import map_keys, col\n",
        "\n",
        "# Assume df_with_literal_map is already created (from the create_map examples)\n",
        "\n",
        "# Use map_keys() on the 'info' column\n",
        "df_literal_map_keys = df_with_literal_map.select(col(\"name\"), map_keys(col(\"info\")).alias(\"info_keys\"))\n",
        "\n",
        "print(\"DataFrame with keys from the literal map:\")\n",
        "df_literal_map_keys.show(truncate=False)\n",
        "df_literal_map_keys.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with keys from the literal map:\n",
            "+-----+---------------+\n",
            "|name |info_keys      |\n",
            "+-----+---------------+\n",
            "|Alice|[city, country]|\n",
            "|Bob  |[city, country]|\n",
            "+-----+---------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- info_keys: array (nullable = false)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36bbadb5"
      },
      "source": [
        "## `collect_list()` and `collect_set()` in PySpark\n",
        "\n",
        "`collect_list()` and `collect_set()` are aggregation functions in PySpark that are used to gather elements from a column into a list or a set, respectively, within each group. They are often used after a `groupBy()` operation.\n",
        "\n",
        "*   **`collect_list(col)`**: Aggregates the elements of the specified column into a `list`. It includes duplicate values and the order of elements in the list is not guaranteed.\n",
        "*   **`collect_set(col)`**: Aggregates the elements of the specified column into a `set`. It only includes unique values and the order of elements in the set is not guaranteed (as sets are unordered collections).\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce207c7c",
        "outputId": "8a4bef58-4c0d-4dd9-a0fb-01f808d53014"
      },
      "source": [
        "# Example 1: Basic Usage with groupBy()\n",
        "\n",
        "from pyspark.sql.functions import collect_list, collect_set, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CollectListSetExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"A\", 1),\n",
        "    (\"B\", 2),\n",
        "    (\"A\", 3),\n",
        "    (\"C\", 4),\n",
        "    (\"B\", 2),\n",
        "    (\"A\", 1)\n",
        "]\n",
        "columns = [\"category\", \"value\"]\n",
        "df_agg = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_agg.show()\n",
        "\n",
        "# Group by 'category' and collect values into a list\n",
        "df_list = df_agg.groupBy(\"category\").agg(collect_list(\"value\").alias(\"list_of_values\"))\n",
        "\n",
        "print(\"DataFrame after groupBy() and collect_list():\")\n",
        "df_list.show()\n",
        "\n",
        "# Group by 'category' and collect unique values into a set\n",
        "df_set = df_agg.groupBy(\"category\").agg(collect_set(\"value\").alias(\"set_of_values\"))\n",
        "\n",
        "print(\"DataFrame after groupBy() and collect_set():\")\n",
        "df_set.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+--------+-----+\n",
            "|category|value|\n",
            "+--------+-----+\n",
            "|       A|    1|\n",
            "|       B|    2|\n",
            "|       A|    3|\n",
            "|       C|    4|\n",
            "|       B|    2|\n",
            "|       A|    1|\n",
            "+--------+-----+\n",
            "\n",
            "DataFrame after groupBy() and collect_list():\n",
            "+--------+--------------+\n",
            "|category|list_of_values|\n",
            "+--------+--------------+\n",
            "|       B|        [2, 2]|\n",
            "|       A|     [1, 3, 1]|\n",
            "|       C|           [4]|\n",
            "+--------+--------------+\n",
            "\n",
            "DataFrame after groupBy() and collect_set():\n",
            "+--------+-------------+\n",
            "|category|set_of_values|\n",
            "+--------+-------------+\n",
            "|       B|          [2]|\n",
            "|       A|       [1, 3]|\n",
            "|       C|          [4]|\n",
            "+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7146508",
        "outputId": "2930ba51-e1fb-4956-c315-46219fcd3ee0"
      },
      "source": [
        "# Example 2: Using collect_list() and collect_set() without groupBy()\n",
        "\n",
        "# When used without groupBy(), these functions will collect all values from the entire DataFrame into a single list or set.\n",
        "df_all_list = df_agg.agg(collect_list(\"value\").alias(\"all_values_list\"))\n",
        "print(\"DataFrame after collect_list() on entire DataFrame:\")\n",
        "df_all_list.show(truncate=False)\n",
        "\n",
        "df_all_set = df_agg.agg(collect_set(\"value\").alias(\"all_values_set\"))\n",
        "print(\"DataFrame after collect_set() on entire DataFrame:\")\n",
        "df_all_set.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after collect_list() on entire DataFrame:\n",
            "+------------------+\n",
            "|all_values_list   |\n",
            "+------------------+\n",
            "|[1, 2, 3, 4, 2, 1]|\n",
            "+------------------+\n",
            "\n",
            "DataFrame after collect_set() on entire DataFrame:\n",
            "+--------------+\n",
            "|all_values_set|\n",
            "+--------------+\n",
            "|[1, 2, 3, 4]  |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ab8cc78",
        "outputId": "a3f64ad5-874d-49ee-d3bc-2d7ecda7c7cf"
      },
      "source": [
        "# Example 3: Collecting multiple columns or complex types\n",
        "\n",
        "from pyspark.sql.functions import struct\n",
        "\n",
        "# Collect 'category' and 'value' as structs into a list\n",
        "df_struct_list = df_agg.groupBy(\"category\").agg(collect_list(struct(\"category\", \"value\")).alias(\"list_of_structs\"))\n",
        "print(\"DataFrame after collecting structs:\")\n",
        "df_struct_list.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after collecting structs:\n",
            "+--------+------------------------+\n",
            "|category|list_of_structs         |\n",
            "+--------+------------------------+\n",
            "|B       |[{B, 2}, {B, 2}]        |\n",
            "|A       |[{A, 1}, {A, 3}, {A, 1}]|\n",
            "|C       |[{C, 4}]                |\n",
            "+--------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample() and sampleBy() in PySpark"
      ],
      "metadata": {
        "id": "CfTjtay0JiPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47ceb17"
      },
      "source": [
        "## `sample()` in PySpark\n",
        "\n",
        "`sample()` is used for simple random sampling. It allows you to randomly select a fraction of rows from your DataFrame.\n",
        "\n",
        "You can perform sampling with or without replacement.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "e57f454a",
        "outputId": "49e1d313-38bb-41aa-8b12-aceb72b73b43"
      },
      "source": [
        "# Example: sample()\n",
        "\n",
        "# Assume df is already created from the previous examples (e.g., the employee dataframe)\n",
        "\n",
        "# Simple random sampling with replacement (sample 30% of data)\n",
        "sampled_df_with_replacement = df.sample(withReplacement=True, fraction=0.3, seed=123)\n",
        "\n",
        "print(\"Sampled DataFrame with Replacement:\")\n",
        "sampled_df_with_replacement.show()\n",
        "\n",
        "# Simple random sampling without replacement (sample 30% of data)\n",
        "sampled_df_without_replacement = df.sample(withReplacement=False, fraction=0.3, seed=123)\n",
        "\n",
        "print(\"Sampled DataFrame without Replacement:\")\n",
        "sampled_df_without_replacement.show()\n",
        "\n",
        "# Note: The exact number of rows in the sampled DataFrame might vary slightly\n",
        "# from fraction * total_rows due to the probabilistic nature of sampling."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2694891157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Simple random sampling with replacement (sample 30% of data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msampled_df_with_replacement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithReplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sampled DataFrame with Replacement:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f21e6af"
      },
      "source": [
        "## `sampleBy()` in PySpark\n",
        "\n",
        "`sampleBy()` allows you to perform stratified sampling. This means you can sample different fractions of data from different categories (strata) within a column.\n",
        "\n",
        "It's useful when you have an imbalanced dataset and want to ensure that each category is represented in your sample according to a specified proportion.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8776326"
      },
      "source": [
        "# Example: sampleBy()\n",
        "\n",
        "# Assume df is already created from the previous examples (e.g., the employee dataframe)\n",
        "\n",
        "# Define fractions for stratified sampling by 'gender'\n",
        "# Sample 50% of 'Male' and 100% of 'Female'\n",
        "gender_fractions = {\"Male\": 0.5, \"Female\": 1.0}\n",
        "\n",
        "# Perform stratified sampling\n",
        "sampled_df_by_gender = df.sampleBy(\"gender\", gender_fractions, seed=42)\n",
        "\n",
        "print(\"Sampled DataFrame by Gender:\")\n",
        "sampled_df_by_gender.show()\n",
        "\n",
        "# Example: sampleBy() by 'department_id'\n",
        "# Sample 80% from department 101, 50% from 102, and 100% from 103\n",
        "dept_fractions = {101: 0.8, 102: 0.5, 103: 1.0}\n",
        "\n",
        "# Perform stratified sampling\n",
        "sampled_df_by_dept = df.sampleBy(\"department_id\", dept_fractions, seed=42)\n",
        "\n",
        "print(\"Sampled DataFrame by Department ID:\")\n",
        "sampled_df_by_dept.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a70076b"
      },
      "source": [
        "Here are some additional examples to further illustrate `sample()` and `sampleBy()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b216da6"
      },
      "source": [
        "# More Examples for sample()\n",
        "\n",
        "# Sample 50% of data with replacement\n",
        "sampled_df_with_replacement_50 = df.sample(withReplacement=True, fraction=0.5, seed=456)\n",
        "print(\"Sampled DataFrame with Replacement (50%):\")\n",
        "sampled_df_with_replacement_50.show()\n",
        "\n",
        "# Sample 20% of data without replacement\n",
        "sampled_df_without_replacement_20 = df.sample(withReplacement=False, fraction=0.2, seed=789)\n",
        "print(\"Sampled DataFrame without Replacement (20%):\")\n",
        "sampled_df_without_replacement_20.show()\n",
        "\n",
        "# Sample 100% of data without replacement (should return the original DataFrame approximately)\n",
        "sampled_df_without_replacement_100 = df.sample(withReplacement=False, fraction=1.0, seed=1011)\n",
        "print(\"Sampled DataFrame without Replacement (100%):\")\n",
        "sampled_df_without_replacement_100.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "427500b8"
      },
      "source": [
        "# More Examples for sampleBy()\n",
        "\n",
        "# Sample different fractions based on 'age' groups\n",
        "# For simplicity, let's create age groups\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_with_age_group = df.withColumn(\"age_group\",\n",
        "    when(col(\"age\") < 30, \"young\")\n",
        "    .when((col(\"age\") >= 30) & (col(\"age\") < 40), \"middle_aged\")\n",
        "    .otherwise(\"senior\")\n",
        ")\n",
        "\n",
        "print(\"DataFrame with Age Group:\")\n",
        "df_with_age_group.show()\n",
        "\n",
        "# Define fractions for sampling by 'age_group'\n",
        "age_group_fractions = {\"young\": 0.7, \"middle_aged\": 0.4, \"senior\": 1.0}\n",
        "\n",
        "# Perform stratified sampling by 'age_group'\n",
        "sampled_df_by_age_group = df_with_age_group.sampleBy(\"age_group\", age_group_fractions, seed=1213)\n",
        "\n",
        "print(\"Sampled DataFrame by Age Group:\")\n",
        "sampled_df_by_age_group.show()\n",
        "\n",
        "# Another example: sample by 'gender' with different seeds\n",
        "gender_fractions_2 = {\"Male\": 0.6, \"Female\": 0.9}\n",
        "sampled_df_by_gender_2 = df.sampleBy(\"gender\", gender_fractions_2, seed=1415)\n",
        "\n",
        "print(\"Sampled DataFrame by Gender (different seed):\")\n",
        "sampled_df_by_gender_2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot() in PySpark"
      ],
      "metadata": {
        "id": "mjAeacZVKlz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3623815f"
      },
      "source": [
        "## `split()` in PySpark\n",
        "\n",
        "The `split()` function in PySpark is used to split a string column into an array of strings based on a specified delimiter. It's a function available in `pyspark.sql.functions`.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5caac601"
      },
      "source": [
        "# Example 1: Basic split()\n",
        "\n",
        "from pyspark.sql.functions import split, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SplitExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"apple,banana,orange\",), (\"grape;kiwi\",), (\"mango\",)]\n",
        "columns = [\"fruits\"]\n",
        "df_fruits = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_fruits.show(truncate=False)\n",
        "\n",
        "# Split the 'fruits' column by comma\n",
        "df_split_comma = df_fruits.withColumn(\"fruit_list_comma\", split(col(\"fruits\"), \",\"))\n",
        "\n",
        "print(\"DataFrame after splitting by comma:\")\n",
        "df_split_comma.show(truncate=False)\n",
        "\n",
        "# Split the 'fruits' column by semicolon\n",
        "df_split_semicolon = df_fruits.withColumn(\"fruit_list_semicolon\", split(col(\"fruits\"), \";\"))\n",
        "\n",
        "print(\"DataFrame after splitting by semicolon:\")\n",
        "df_split_semicolon.show(truncate=False)\n",
        "\n",
        "# Split by both comma and semicolon using regex\n",
        "df_split_regex = df_fruits.withColumn(\"fruit_list_regex\", split(col(\"fruits\"), \"[,;]\"))\n",
        "\n",
        "print(\"DataFrame after splitting by comma or semicolon (regex):\")\n",
        "df_split_regex.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dfb33eb"
      },
      "source": [
        "# Example 2: Using the limit parameter\n",
        "\n",
        "from pyspark.sql.functions import split, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"a_b_c_d_e\",), (\"x_y\",), (\"z\",)]\n",
        "columns = [\"text\"]\n",
        "df_limit = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_limit.show()\n",
        "\n",
        "# Split with limit = 2\n",
        "df_split_limit_2 = df_limit.withColumn(\"split_limit_2\", split(col(\"text\"), \"_\", 2))\n",
        "\n",
        "print(\"DataFrame after splitting with limit = 2:\")\n",
        "df_split_limit_2.show(truncate=False)\n",
        "\n",
        "# Split with limit = 0 (same as -1)\n",
        "df_split_limit_0 = df_limit.withColumn(\"split_limit_0\", split(col(\"text\"), \"_\", 0))\n",
        "\n",
        "print(\"DataFrame after splitting with limit = 0:\")\n",
        "df_split_limit_0.show(truncate=False)\n",
        "\n",
        "# Split with limit = -1 (default)\n",
        "df_split_limit_neg1 = df_limit.withColumn(\"split_limit_neg1\", split(col(\"text\"), \"_\", -1))\n",
        "\n",
        "print(\"DataFrame after splitting with limit = -1:\")\n",
        "df_split_limit_neg1.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f63d471"
      },
      "source": [
        "## `concat_ws()` in PySpark\n",
        "\n",
        "The `concat_ws()` function (concatenate with separator) is used to concatenate multiple string columns together into a single string column, with a specified separator placed between each concatenated value. It's a function available in `pyspark.sql.functions`.\n",
        "\n",
        "This function is useful for combining information from different columns into a more readable format or preparing data for output.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07f1531d"
      },
      "source": [
        "# Example 1: Basic concat_ws()\n",
        "\n",
        "from pyspark.sql.functions import concat_ws, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ConcatWSExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"John\", \"Doe\", \"USA\"),\n",
        "    (\"Jane\", \"Smith\", \"Canada\"),\n",
        "    (\"Peter\", \"Jones\", \"UK\"),\n",
        "    (None, \"Brown\", \"Germany\"), # Example with a null value\n",
        "    (\"Alice\", None, \"France\")  # Example with a null value\n",
        "]\n",
        "columns = [\"first_name\", \"last_name\", \"country\"]\n",
        "df_names = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_names.show()\n",
        "\n",
        "# Concatenate first_name and last_name with a space\n",
        "df_full_name = df_names.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
        "\n",
        "print(\"DataFrame with full_name:\")\n",
        "df_full_name.show()\n",
        "\n",
        "# Concatenate first_name, last_name, and country with a comma and space\n",
        "df_full_info = df_names.withColumn(\"full_info\", concat_ws(\", \", col(\"first_name\"), col(\"last_name\"), col(\"country\")))\n",
        "\n",
        "print(\"DataFrame with full_info:\")\n",
        "df_full_info.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a447677"
      },
      "source": [
        "**Handling NULLs:**\n",
        "\n",
        "`concat_ws()` gracefully handles NULL values. If a column value is NULL, it is simply skipped, and the separator is not added for that specific value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "320f2c24"
      },
      "source": [
        "# Example 2: concat_ws() with array column\n",
        "\n",
        "from pyspark.sql.functions import concat_ws, col, array, lit\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Sample DataFrame with an array column\n",
        "data = [\n",
        "    (\"apple\", [\"red\", \"green\"]),\n",
        "    (\"banana\", [\"yellow\"]),\n",
        "    (\"orange\", [\"orange\", \"sweet\", \"citrus\"]),\n",
        "    (\"grape\", []), # Empty array\n",
        "    (\"kiwi\", None) # Null array\n",
        "]\n",
        "columns = [\"fruit\", \"properties\"]\n",
        "df_fruits_props = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_fruits_props.show(truncate=False)\n",
        "\n",
        "# Concatenate elements of the 'properties' array with a hyphen\n",
        "df_props_string = df_fruits_props.withColumn(\"properties_string\", concat_ws(\"-\", col(\"properties\")))\n",
        "\n",
        "print(\"DataFrame with properties_string (concatenated array):\")\n",
        "df_props_string.show(truncate=False)\n",
        "\n",
        "# Concatenate fruit name and properties array elements\n",
        "df_combined = df_fruits_props.withColumn(\"fruit_and_props\", concat_ws(\":\", col(\"fruit\"), concat_ws(\",\", col(\"properties\"))))\n",
        "\n",
        "print(\"DataFrame with fruit_and_props:\")\n",
        "df_combined.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1: Basic concat_ws() (cell 07f1531d)\n",
        "\n",
        "This example demonstrates the basic usage of concat_ws() to combine string columns with a specified separator.\n",
        "\n",
        "Original DataFrame: This shows the initial data with first_name, last_name, and country columns, including some rows with NULL values.\n",
        "Concatenate first_name and last_name with a space:\n",
        "df_names.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
        "This line adds a new column named full_name.\n",
        "concat_ws(\" \", ...) is used to concatenate the columns. The first argument \" \" is the separator (a space).\n",
        "col(\"first_name\"), col(\"last_name\") are the columns to be concatenated.\n",
        "The output DataFrame with full_name shows the combined first_name and last_name. Notice how the row with NULL in first_name just shows the last_name (\"Brown\"), and the row with NULL in last_name just shows the first_name (\"Alice\"). concat_ws skips the NULL values and doesn't add the separator for them.\n",
        "Concatenate first_name, last_name, and country with a comma and space:\n",
        "df_names.withColumn(\"full_info\", concat_ws(\", \", col(\"first_name\"), col(\"last_name\"), col(\"country\")))\n",
        "This line adds another new column named full_info.\n",
        "concat_ws(\", \", ...) uses \", \" as the separator.\n",
        "col(\"first_name\"), col(\"last_name\"), col(\"country\") are the columns to concatenate.\n",
        "The output DataFrame with full_info shows the combined information. Again, observe how NULL values are handled – for the row with NULL first name, the output is \"Brown, Germany\", skipping the NULL first name and the separator before it.\n",
        "Example 2: concat_ws() with array column (cell 320f2c24)\n",
        "\n",
        "This example shows how concat_ws() can be used with an array column.\n",
        "\n",
        "Original DataFrame: This DataFrame has a fruit column and a properties column, which is an array of strings. It includes examples with multiple elements, a single element, an empty array, and a NULL array.\n",
        "Concatenate elements of the 'properties' array with a hyphen:\n",
        "df_fruits_props.withColumn(\"properties_string\", concat_ws(\"-\", col(\"properties\")))\n",
        "This adds a new column properties_string.\n",
        "concat_ws(\"-\", col(\"properties\")) concatenates the elements within the properties array using a hyphen - as the separator.\n",
        "The output DataFrame with properties_string (concatenated array) shows the array elements joined by hyphens. Note that the empty array results in an empty string, and the NULL array also results in a NULL value in the new column.\n",
        "Concatenate fruit name and properties array elements:\n",
        "df_combined = df_fruits_props.withColumn(\"fruit_and_props\", concat_ws(\":\", col(\"fruit\"), concat_ws(\",\", col(\"properties\"))))\n",
        "This is a more complex example where concat_ws() is nested.\n",
        "The outer concat_ws(\":\", ...) concatenates the fruit column and the result of the inner concat_ws(). The separator for the outer concatenation is :.\n",
        "The inner concat_ws(\",\", col(\"properties\")) concatenates the elements within the properties array using a comma , as the separator (as seen in the previous step).\n",
        "The output DataFrame with fruit_and_props shows the fruit name, followed by a colon, and then the properties from the array joined by commas. Rows with empty or NULL arrays for properties still have the fruit name and the colon, followed by nothing.\n",
        "In summary, concat_ws() is a versatile function for combining string columns or elements within a string array, providing control over the separator and handling NULL values gracefully.\n",
        "\n",
        "Let me know if you'd like a deeper dive into any specific part of these examples!\n",
        "\n"
      ],
      "metadata": {
        "id": "vwTx368CM7f6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aafeda44"
      },
      "source": [
        "## Regular Expression Methods in PySpark\n",
        "\n",
        "PySpark provides functions in `pyspark.sql.functions` for working with regular expressions on string columns. Two common ones are `regexp_extract()` and `regexp_replace()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55b36bad"
      },
      "source": [
        "## `translate()` in PySpark\n",
        "\n",
        "The `translate()` function in PySpark is used to replace a sequence of characters in a string column with another sequence of characters. It performs a character-by-character replacement.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7893e8a4"
      },
      "source": [
        "# Example 1: Basic translate()\n",
        "\n",
        "from pyspark.sql.functions import translate, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TranslateExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"abcdefg\",),\n",
        "    (\"12345\",),\n",
        "    (\"hello world\",),\n",
        "    (\"PySpark\",),\n",
        "    (None,) # Example with a null value\n",
        "]\n",
        "columns = [\"text\"]\n",
        "df_text = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Replace 'abc' with 'xyz'\n",
        "# 'a' is replaced by 'x', 'b' by 'y', 'c' by 'z'\n",
        "df_translated_basic = df_text.withColumn(\"translated_text\", translate(col(\"text\"), \"abc\", \"xyz\"))\n",
        "\n",
        "print(\"DataFrame after translate(col('text'), 'abc', 'xyz'):\")\n",
        "df_translated_basic.show()\n",
        "\n",
        "# Replace digits with asterisks\n",
        "df_translated_digits = df_text.withColumn(\"translated_digits\", translate(col(\"text\"), \"0123456789\", \"**********\"))\n",
        "\n",
        "print(\"DataFrame after translate(col('text'), '0123456789', '**********'):\")\n",
        "df_translated_digits.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb0a226d"
      },
      "source": [
        "# Example 2: Unequal lengths of 'from' and 'to' characters\n",
        "\n",
        "from pyspark.sql.functions import translate, col\n",
        "\n",
        "# Assume df_text is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Replace 'aeiou' with '123'\n",
        "# 'a' -> '1', 'e' -> '2', 'i' -> '3'. 'o' and 'u' are removed.\n",
        "df_translated_unequal = df_text.withColumn(\"translated_unequal\", translate(col(\"text\"), \"aeiou\", \"123\"))\n",
        "\n",
        "print(\"DataFrame after translate(col('text'), 'aeiou', '123'):\")\n",
        "df_translated_unequal.show()\n",
        "\n",
        "# Replace 'xyz' with '12345'\n",
        "# 'x' -> '1', 'y' -> '2', 'z' -> '3'. No characters in 'to' for '4' and '5'.\n",
        "df_translated_unequal_2 = df_text.withColumn(\"translated_unequal_2\", translate(col(\"text\"), \"xyz\", \"12345\"))\n",
        "\n",
        "print(\"DataFrame after translate(col('text'), 'xyz', '12345'):\")\n",
        "df_translated_unequal_2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe982f86"
      },
      "source": [
        "# Example 3: Removing characters\n",
        "\n",
        "from pyspark.sql.functions import translate, col\n",
        "\n",
        "# Assume df_text is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Remove all vowels\n",
        "# 'from' contains vowels, 'to' is an empty string\n",
        "df_translated_remove_vowels = df_text.withColumn(\"no_vowels\", translate(col(\"text\"), \"aeiouAEIOU\", \"\"))\n",
        "\n",
        "print(\"DataFrame after removing vowels:\")\n",
        "df_translated_remove_vowels.show()\n",
        "\n",
        "# Remove spaces and commas\n",
        "df_translated_remove_chars = df_text.withColumn(\"no_spaces_commas\", translate(col(\"text\"), \" ,\", \"\"))\n",
        "\n",
        "print(\"DataFrame after removing spaces and commas:\")\n",
        "df_translated_remove_chars.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark's translate() function, when the length of the from string and the to string are unequal, the translation is still done character by character based on the position in the strings.\n",
        "\n",
        "If the from string is longer than the to string, the characters in the from string that do not have a corresponding character at the same position in the to string are removed from the input string.\n",
        "If the to string is longer than the from string, the extra characters in the to string are ignored.\n",
        "You can see this in Example 2 (cell fb0a226d). When translating 'aeiou' to '123', 'a' becomes '1', 'e' becomes '2', 'i' becomes '3', but 'o' and 'u' are removed because there are no 4th and 5th characters in the '123' string."
      ],
      "metadata": {
        "id": "phSVi7rkNrcB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1965b2c6"
      },
      "source": [
        "## `substring()` in PySpark\n",
        "\n",
        "The `substring()` function in PySpark is used to extract a substring from a string column. It takes the starting position and the length of the substring to extract.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "423a5148"
      },
      "source": [
        "display(df_substring_basic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ac1bc69"
      },
      "source": [
        "display(df_translated_basic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74984f70"
      },
      "source": [
        "# Example 1: Basic substring()\n",
        "\n",
        "from pyspark.sql.functions import substring, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SubstringExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"abcdefg\",),\n",
        "    (\"PySpark\",),\n",
        "    (\"Data Science\",),\n",
        "    (None,) # Example with a null value\n",
        "]\n",
        "columns = [\"text\"]\n",
        "df_text = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Extract substring starting from position 3 with length 4\n",
        "df_substring_basic = df_text.withColumn(\"substring_example\", substring(col(\"text\"), 3, 4))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), 3, 4):\")\n",
        "df_substring_basic.show()\n",
        "\n",
        "# Extract substring from the beginning (position 1) with length 3\n",
        "df_substring_start = df_text.withColumn(\"substring_from_start\", substring(col(\"text\"), 1, 3))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), 1, 3):\")\n",
        "df_substring_start.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72a8b52b"
      },
      "source": [
        "# Example 2: Using negative position\n",
        "\n",
        "from pyspark.sql.functions import substring, col\n",
        "\n",
        "# Assume df_text is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Extract substring starting from 3 characters from the end with length 3\n",
        "df_substring_negative_pos = df_text.withColumn(\"substring_negative\", substring(col(\"text\"), -3, 3))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), -3, 3):\")\n",
        "df_substring_negative_pos.show()\n",
        "\n",
        "# Extract substring starting from 5 characters from the end with length 2\n",
        "df_substring_negative_pos_2 = df_text.withColumn(\"substring_negative_2\", substring(col(\"text\"), -5, 2))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), -5, 2):\")\n",
        "df_substring_negative_pos_2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1743b8c4"
      },
      "source": [
        "# Example 3: Handling lengths longer than the remaining string\n",
        "\n",
        "from pyspark.sql.functions import substring, col\n",
        "\n",
        "# Assume df_text is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Extract substring starting from position 5 with a length longer than remaining\n",
        "df_substring_long_len = df_text.withColumn(\"substring_long\", substring(col(\"text\"), 5, 10))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), 5, 10):\")\n",
        "df_substring_long_len.show()\n",
        "\n",
        "# Extract substring starting from a position beyond the string length\n",
        "df_substring_invalid_pos = df_text.withColumn(\"substring_invalid\", substring(col(\"text\"), 10, 3))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), 10, 3):\")\n",
        "df_substring_invalid_pos.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156ddecf"
      },
      "source": [
        "## `regexp_extract()`\n",
        "\n",
        "`regexp_extract()` is used to extract a specific part of a string that matches a regular expression pattern.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76398ef7"
      },
      "source": [
        "# Example 1: regexp_extract()\n",
        "\n",
        "from pyspark.sql.functions import regexp_extract, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RegexExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"user_123_abc\",),\n",
        "    (\"another_user_456_xyz\",),\n",
        "    (\"id_789\",),\n",
        "    (\"no_match\",)\n",
        "]\n",
        "columns = [\"text\"]\n",
        "df_regex = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_regex.show()\n",
        "\n",
        "# Extract the numbers after \"user_\"\n",
        "# Pattern: \"user_\" followed by one or more digits (\\d+)\n",
        "# Group 1: the digits captured by (\\d+)\n",
        "df_extracted = df_regex.withColumn(\"extracted_number\", regexp_extract(col(\"text\"), r\"user_(\\d+)\", 1))\n",
        "\n",
        "print(\"DataFrame after extracting numbers:\")\n",
        "df_extracted.show()\n",
        "\n",
        "# Extract text after \"user_\"\n",
        "# Pattern: \"user_\" followed by anything (.*)\n",
        "# Group 1: the text captured by (.*)\n",
        "df_extracted_text = df_regex.withColumn(\"extracted_text\", regexp_extract(col(\"text\"), r\"user_(.*)\", 1))\n",
        "\n",
        "print(\"DataFrame after extracting text:\")\n",
        "df_extracted_text.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89548c93"
      },
      "source": [
        "## `regexp_replace()`\n",
        "\n",
        "`regexp_replace()` is used to replace all occurrences of a substring that matches a regular expression pattern with another string.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b6e88b7"
      },
      "source": [
        "# Example 2: regexp_replace()\n",
        "\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "# Assume df_regex is already created from the previous example\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_regex.show()\n",
        "\n",
        "# Replace all digits with 'X'\n",
        "df_replaced_digits = df_regex.withColumn(\"replaced_digits\", regexp_replace(col(\"text\"), r\"\\d+\", \"X\"))\n",
        "\n",
        "print(\"DataFrame after replacing digits:\")\n",
        "df_replaced_digits.show()\n",
        "\n",
        "# Replace \"user_\" with \"id_\"\n",
        "df_replaced_user = df_regex.withColumn(\"replaced_user\", regexp_replace(col(\"text\"), \"user_\", \"id_\"))\n",
        "\n",
        "print(\"DataFrame after replacing 'user_':\")\n",
        "df_replaced_user.show()\n",
        "\n",
        "# Remove anything after \"_\"\n",
        "df_removed_after_underscore = df_regex.withColumn(\"removed_after_underscore\", regexp_replace(col(\"text\"), r\"\\_.*\", \"\"))\n",
        "\n",
        "print(\"DataFrame after removing text after underscore:\")\n",
        "df_removed_after_underscore.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90965fa0"
      },
      "source": [
        "# Example 3: Accessing elements of the resulting array\n",
        "\n",
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Assume df_fruits is already created from Example 1\n",
        "\n",
        "df_split_comma = df_fruits.withColumn(\"fruit_list_comma\", split(col(\"fruits\"), \",\"))\n",
        "\n",
        "# Access the first element (index 0)\n",
        "df_first_fruit = df_split_comma.withColumn(\"first_fruit\", col(\"fruit_list_comma\")[0])\n",
        "\n",
        "print(\"DataFrame with the first fruit:\")\n",
        "df_first_fruit.show(truncate=False)\n",
        "\n",
        "# Access the second element (index 1)\n",
        "df_second_fruit = df_split_comma.withColumn(\"second_fruit\", col(\"fruit_list_comma\")[1])\n",
        "\n",
        "print(\"DataFrame with the second fruit:\")\n",
        "df_second_fruit.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac4c863"
      },
      "source": [
        "## `pivot()` in PySpark\n",
        "\n",
        "`pivot()` is a transformation used to rotate a table-valued expression by turning the unique values from one column into multiple columns. It's commonly used for data aggregation and reshaping, similar to a pivot table in spreadsheet software.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8d7d073"
      },
      "source": [
        "# Example 1: Basic pivot()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"USA\", \"ProductA\", 100),\n",
        "    (\"USA\", \"ProductB\", 150),\n",
        "    (\"Canada\", \"ProductA\", 120),\n",
        "    (\"Canada\", \"ProductC\", 200),\n",
        "    (\"USA\", \"ProductB\", 180),\n",
        "    (\"Canada\", \"ProductA\", 130)\n",
        "]\n",
        "\n",
        "columns = [\"country\", \"product\", \"amount\"]\n",
        "\n",
        "df_sales = spark.createDataFrame(data, columns)\n",
        "print(\"Original DataFrame:\")\n",
        "df_sales.show()\n",
        "\n",
        "# Pivot the data to show total amount by country and product\n",
        "pivot_df = df_sales.groupBy(\"country\").pivot(\"product\").sum(\"amount\")\n",
        "\n",
        "print(\"Pivoted DataFrame:\")\n",
        "pivot_df.show()\n",
        "\n",
        "# Note: NULL values appear where a combination of grouping column and pivot column value does not exist in the original data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43bcd1d4"
      },
      "source": [
        "# Example 2: pivot() with specified values\n",
        "\n",
        "# It's generally recommended to provide a list of values to the pivot function\n",
        "# to avoid collecting all unique values from a large dataset.\n",
        "\n",
        "product_values = [\"ProductA\", \"ProductB\", \"ProductC\"]\n",
        "\n",
        "pivot_df_specified = df_sales.groupBy(\"country\").pivot(\"product\", product_values).sum(\"amount\")\n",
        "\n",
        "print(\"Pivoted DataFrame with specified values:\")\n",
        "pivot_df_specified.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca8fc1af"
      },
      "source": [
        "# Example 3: pivot() with multiple aggregations\n",
        "\n",
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "pivot_df_multi_agg = df_sales.groupBy(\"country\").pivot(\"product\", product_values).agg(sum(\"amount\").alias(\"total_amount\"), count(\"amount\").alias(\"count\"))\n",
        "\n",
        "print(\"Pivoted DataFrame with multiple aggregations:\")\n",
        "pivot_df_multi_agg.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "214ca9ce"
      },
      "source": [
        "# Example 4: pivot() on a different dataset (using the employee df from earlier)\n",
        "\n",
        "# Pivot the employee data to show average salary by department and gender\n",
        "pivot_employee_df = df.groupBy(\"department_id\").pivot(\"gender\").agg(avg(\"salary\"))\n",
        "\n",
        "print(\"Pivoted Employee DataFrame (Average Salary by Department and Gender):\")\n",
        "pivot_employee_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}