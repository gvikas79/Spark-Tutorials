{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**What is collect() in PySpark?**\n",
        "\n",
        "*   collect() is an action in PySpark (not a transformation).\n",
        "*   It retrieves all rows of a DataFrame (or RDD) to the driver node as a list of Row objects.\n",
        "*   Since Spark works in a distributed environment, data is spread across executors.\n",
        "*  collect() brings everything back to your local Python process.\n",
        "\n",
        "\n",
        "Warning: Donâ€™t use collect() on very large datasets (it can cause OutOfMemoryError). Use it only for small results, testing, or debugging.\n",
        "\n",
        "Syntax\n",
        "\n",
        "DataFrame.collect()\n",
        "\n",
        "Returns:\n",
        "A list of Row objects, where each row represents a record from the DataFrame.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2_ebj63J7WrJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X27pjT2A6z6W",
        "outputId": "253d6509-6b55-4701-c3cf-8a74eedb7982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   name|age|salary|\n",
            "+-------+---+------+\n",
            "|  Alice| 25|  3000|\n",
            "|    Bob| 30|  4000|\n",
            "|Charlie| 28|  5000|\n",
            "+-------+---+------+\n",
            "\n",
            "[Row(name='Alice', age=25, salary=3000), Row(name='Bob', age=30, salary=4000), Row(name='Charlie', age=28, salary=5000)]\n"
          ]
        }
      ],
      "source": [
        "#Example 1: Basic Usage\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CollectExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25, 3000), (\"Bob\", 30, 4000), (\"Charlie\", 28, 5000)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Collect all rows\n",
        "result = df.collect()\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2: Iterating Over Collected Data\n",
        "rows = df.collect()\n",
        "\n",
        "for row in rows:\n",
        "    print(row[\"name\"], row[\"age\"], row[\"salary\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C53I-n4g8Az8",
        "outputId": "715b4044-d78f-483d-915a-b613f08e920b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice 25 3000\n",
            "Bob 30 4000\n",
            "Charlie 28 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Convert to Pandas\n",
        "#Often, after collecting, people convert to Pandas for local analysis:\n",
        "\n",
        "pandas_df = df.toPandas()\n",
        "print(pandas_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA_kae_v8Fuj",
        "outputId": "d9e3b17d-45b4-401b-8f91-dae71635dd63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      name  age  salary\n",
            "0    Alice   25    3000\n",
            "1      Bob   30    4000\n",
            "2  Charlie   28    5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 4: Collect with Filtering\n",
        "\n",
        "result = df.filter(df[\"age\"] > 26).collect()\n",
        "for row in result:\n",
        "    print(row.name, row.salary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY7QxtHP8Ssv",
        "outputId": "11323105-ca98-4156-fc09-9153f835da2c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bob 4000\n",
            "Charlie 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use collect()?\n",
        "\n",
        "Use collect() when:\n",
        "\n",
        "â€¢\tDataset is small enough to fit in memory.\n",
        "\n",
        "â€¢\tYou want to debug, print, or inspect results locally.\n",
        "\n",
        "â€¢\tYouâ€™re passing results to an external Python library (like Pandas, NumPy, Matplotlib).\n",
        "\n",
        "Avoid collect() when:\n",
        "â€¢\tDataset is large (millions of rows, GBs of data).\n",
        "â€¢\tIt can cause driver out of memory issues.\n",
        "\n",
        "Tip: For safer alternatives use:\n",
        "\n",
        "â€¢\tshow(n) â†’ prints first n rows nicely.\n",
        "\n",
        "â€¢\ttake(n) â†’ returns first n rows as list.\n",
        "\n",
        "â€¢\tlimit(n).collect() â†’ collects only a subset.\n",
        "\n"
      ],
      "metadata": {
        "id": "no-x3Xxe8g5x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XPC_R9nCadx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is transform() in PySpark?\n",
        "\n",
        "â€¢\ttransform() is available on DataFrame objects.\n",
        "\n",
        "â€¢\tIt allows you to apply a function (transformation) to a DataFrame in a clean, reusable, and chainable way.\n",
        "\n",
        "â€¢\tInstead of writing complex transformations inline, you can wrap them in functions and pass them to transform().\n",
        "\n",
        "â€¢\tIt improves readability and reusability of your PySpark code.\n",
        "________________________________________\n",
        "Syntax\n",
        "DataFrame.transform(func)\n",
        "\n",
        "â€¢\tfunc â†’ a Python function that takes a DataFrame as input and returns a DataFrame.\n",
        "\n",
        "â€¢\tReturns â†’ the transformed DataFrame.\n"
      ],
      "metadata": {
        "id": "JWusN--j84OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1: Basic Usage\n",
        "from pyspark.sql.functions import col\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25, 3000), (\"Bob\", 30, 4000), (\"Charlie\", 28, 5000)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoupuWMJ8WkD",
        "outputId": "c822f10e-d1d2-4a44-f070-2aaf6ed270be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   name|age|salary|\n",
            "+-------+---+------+\n",
            "|  Alice| 25|  3000|\n",
            "|    Bob| 30|  4000|\n",
            "|Charlie| 28|  5000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to add a 10% bonus to salary\n",
        "\n",
        "def add_bonus(dataframe):\n",
        "    return dataframe.withColumn(\"salary_with_bonus\", col(\"salary\") * 1.1)\n",
        "\n",
        "# Apply using transform\n",
        "df_transformed = df.transform(add_bonus)\n",
        "df_transformed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhgA2tJm9N0L",
        "outputId": "e62735f7-360e-40fd-95c1-770d528f7955"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+------------------+\n",
            "|   name|age|salary| salary_with_bonus|\n",
            "+-------+---+------+------------------+\n",
            "|  Alice| 25|  3000|3300.0000000000005|\n",
            "|    Bob| 30|  4000|            4400.0|\n",
            "|Charlie| 28|  5000|            5500.0|\n",
            "+-------+---+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2: Chaining Multiple transform() Calls\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "# Function to uppercase name\n",
        "def uppercase_name(df):\n",
        "    return df.withColumn(\"name_upper\", upper(col(\"name\")))\n",
        "\n",
        "# Function to categorize salary\n",
        "def categorize_salary(df):\n",
        "    return df.withColumn(\"salary_level\",\n",
        "                         (col(\"salary\") > 4000).cast(\"string\"))\n",
        "\n",
        "# Apply multiple transformations\n",
        "df_chain = df.transform(uppercase_name).transform(categorize_salary)\n",
        "df_chain.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5S7lU1l9Sjz",
        "outputId": "1b80f2a9-6b5b-4819-a54a-0b42e764ee23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+----------+------------+\n",
            "|   name|age|salary|name_upper|salary_level|\n",
            "+-------+---+------+----------+------------+\n",
            "|  Alice| 25|  3000|     ALICE|       false|\n",
            "|    Bob| 30|  4000|       BOB|       false|\n",
            "|Charlie| 28|  5000|   CHARLIE|        true|\n",
            "+-------+---+------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Passing Parameters with lambda\n",
        "#You can also use lambda functions directly:\n",
        "\n",
        "df_lambda = df.transform(lambda d: d.withColumn(\"age_plus_5\", col(\"age\") + 5))\n",
        "df_lambda.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqvzlDSB9dbW",
        "outputId": "415d2375-49b7-44e5-f82d-e5690f5c4488"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+----------+\n",
            "|   name|age|salary|age_plus_5|\n",
            "+-------+---+------+----------+\n",
            "|  Alice| 25|  3000|        30|\n",
            "|    Bob| 30|  4000|        35|\n",
            "|Charlie| 28|  5000|        33|\n",
            "+-------+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 4: Reusable Pipeline with transform()\n",
        "\n",
        "def pipeline(df):\n",
        "    return (df\n",
        "            .transform(add_bonus)         # Step 1: Add bonus\n",
        "            .transform(uppercase_name)    # Step 2: Uppercase name\n",
        "            .transform(lambda d: d.filter(col(\"age\") > 26))  # Step 3: Filter\n",
        "           )\n",
        "\n",
        "df_pipeline = df.transform(pipeline)\n",
        "df_pipeline.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wnqhbe09wVl",
        "outputId": "e6814988-4bb1-43f2-a993-8ecb769d6350"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+-----------------+----------+\n",
            "|   name|age|salary|salary_with_bonus|name_upper|\n",
            "+-------+---+------+-----------------+----------+\n",
            "|    Bob| 30|  4000|           4400.0|       BOB|\n",
            "|Charlie| 28|  5000|           5500.0|   CHARLIE|\n",
            "+-------+---+------+-----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Why Use transform()?\n",
        "\n",
        "Makes code cleaner & modular (define transformations once, reuse many times).\n",
        "\n",
        "Useful when building pipelines of transformations.\n",
        "\n",
        "Works well with functional programming style in PySpark.\n"
      ],
      "metadata": {
        "id": "C4kFoz4395Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Direct Method Chaining\n",
        "#You can chain transformations directly on the DataFrame:\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TransformVsChaining\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 25, 3000), (\"Bob\", 30, 4000), (\"Charlie\", 28, 5000)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])\n",
        "\n",
        "# Direct chaining\n",
        "df_chain = (\n",
        "    df.withColumn(\"salary_with_bonus\", col(\"salary\") * 1.1)\n",
        "      .withColumn(\"name_upper\", upper(col(\"name\")))\n",
        "      .filter(col(\"age\") > 26)\n",
        ")\n",
        "\n",
        "df_chain.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOcmVXVj90XQ",
        "outputId": "97de5c1a-e2a5-4785-93c9-aed5825f1112"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+-----------------+----------+\n",
            "|   name|age|salary|salary_with_bonus|name_upper|\n",
            "+-------+---+------+-----------------+----------+\n",
            "|    Bob| 30|  4000|           4400.0|       BOB|\n",
            "|Charlie| 28|  5000|           5500.0|   CHARLIE|\n",
            "+-------+---+------+-----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using transform()\n",
        "#Instead of repeating logic, you define reusable functions:\n",
        "# Define reusable transformations\n",
        "\n",
        "def add_bonus(df):\n",
        "    return df.withColumn(\"salary_with_bonus\", col(\"salary\") * 1.1)\n",
        "\n",
        "def uppercase_name(df):\n",
        "    return df.withColumn(\"name_upper\", upper(col(\"name\")))\n",
        "\n",
        "def filter_age(df):\n",
        "    return df.filter(col(\"age\") > 26)\n",
        "\n",
        "# Apply with transform\n",
        "df_transformed = (\n",
        "    df.transform(add_bonus)\n",
        "      .transform(uppercase_name)\n",
        "      .transform(filter_age)\n",
        ")\n",
        "\n",
        "df_transformed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRH-_aNP-P9z",
        "outputId": "824460ef-deee-46e1-c831-beb658ecf45b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+-----------------+----------+\n",
            "|   name|age|salary|salary_with_bonus|name_upper|\n",
            "+-------+---+------+-----------------+----------+\n",
            "|    Bob| 30|  4000|           4400.0|       BOB|\n",
            "|Charlie| 28|  5000|           5500.0|   CHARLIE|\n",
            "+-------+---+------+-----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output is the same, but advantages:\n",
        "\n",
        "â€¢\tModular: Each transformation is a function.\n",
        "\n",
        "â€¢\tReusable: You can apply add_bonus() or filter_age() to other DataFrames easily.\n",
        "\n",
        "â€¢\tReadable: Clearly separates logical steps.\n"
      ],
      "metadata": {
        "id": "n5WcCmOY-XMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mixing lambda with transform()\n",
        "\n",
        "#For quick one-off transformations:\n",
        "df_lambda = (\n",
        "    df.transform(add_bonus)\n",
        "      .transform(lambda d: d.withColumn(\"age_plus_5\", col(\"age\") + 5))\n",
        ")\n",
        "df_lambda.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLcjSVvN-bCR",
        "outputId": "c6812312-ee2d-4904-e418-52433a841811"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+------------------+----------+\n",
            "|   name|age|salary| salary_with_bonus|age_plus_5|\n",
            "+-------+---+------+------------------+----------+\n",
            "|  Alice| 25|  3000|3300.0000000000005|        30|\n",
            "|    Bob| 30|  4000|            4400.0|        35|\n",
            "|Charlie| 28|  5000|            5500.0|        33|\n",
            "+-------+---+------+------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use What?\n",
        "\n",
        "Approach\tBest for\n",
        "Method chaining\tQuick scripts, small transformations, throwaway code\n",
        "transform()\n",
        "Reusable pipelines, production code, when the same transformations must be applied to multiple DataFrames\n",
        "________________________________________\n",
        "In real-world projects, transform() shines when:\n",
        "\n",
        "â€¢\tYou build data pipelines.\n",
        "\n",
        "â€¢\tYou want clean, testable, reusable code.\n",
        "\n",
        "â€¢\tYouâ€™re working in a team (easier to understand functions like add_bonus than inline chains).\n"
      ],
      "metadata": {
        "id": "pDdeS5SJ-nSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike Pandas, PySpark DataFrame does not have an apply() method.\n",
        "Instead, there are different contexts where apply() exists in PySpark:\n",
        "\n",
        "1.\tapply() in Pandas UDFs (with pandas_udf)\n",
        "\n",
        "2.\tapplyInPandas() on DataFrames\n",
        "\n",
        "3.\tapply() in grouped operations (GroupedData)\n",
        "\n",
        "4.\tRDD map() / mapPartitions() (the lower-level equivalent to apply logic)\n",
        "\n",
        "Letâ€™s go through them one by one with code examples:\n"
      ],
      "metadata": {
        "id": "TNdhgPq3-2Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apply() in Pandas UDFs\n",
        "#PySpark integrates with Pandas via vectorized UDFs.\n",
        "#Here, apply() is used inside Pandas UDF functions.\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 28)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Define a Pandas UDF that applies a custom transformation\n",
        "@pandas_udf(\"int\")\n",
        "def add_five(age_series: pd.Series) -> pd.Series:\n",
        "    return age_series.apply(lambda x: x + 5)\n",
        "\n",
        "# Use it in a DataFrame\n",
        "df_with_new = df.withColumn(\"age_plus_5\", add_five(df[\"age\"]))\n",
        "df_with_new.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX62V3ji-e02",
        "outputId": "01afdcf7-f4e8-4c65-9018-5e3221b87e1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+\n",
            "|   name|age|age_plus_5|\n",
            "+-------+---+----------+\n",
            "|  Alice| 25|        30|\n",
            "|    Bob| 30|        35|\n",
            "|Charlie| 28|        33|\n",
            "+-------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#applyInPandas()\n",
        "#This is a DataFrame-level method that allows you to apply a function on grouped Pandas DataFrames.\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Schema for output\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age_plus_10\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Function to apply on each Pandas DataFrame\n",
        "def add_ten(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    pdf[\"age_plus_10\"] = pdf[\"age\"] + 10\n",
        "    return pdf[[\"name\", \"age_plus_10\"]]\n",
        "\n",
        "# Use applyInPandas\n",
        "df_applied = df.groupBy(\"name\").applyInPandas(add_ten, schema=schema)\n",
        "df_applied.show()\n",
        "\n",
        "#applyInPandas() is grouped: Spark splits data into groups â†’ converts each group to Pandas â†’ applies the function â†’ merges results back."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQRP4znk_RDa",
        "outputId": "273503bd-f9a6-41eb-ddb3-6c82e1c3144b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|   name|age_plus_10|\n",
            "+-------+-----------+\n",
            "|  Alice|         35|\n",
            "|    Bob|         40|\n",
            "|Charlie|         38|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Equivalent of apply() on RDDs\n",
        "#If you want Pandas-style apply() for row-wise operations, you can use map() on RDDs:\n",
        "\n",
        "# Convert DataFrame to RDD\n",
        "rdd = df.rdd\n",
        "\n",
        "# Apply transformation (similar to row-wise apply)\n",
        "rdd_applied = rdd.map(lambda row: (row[\"name\"], row[\"age\"] + 2))\n",
        "print(rdd_applied.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH6JFEN1_VEh",
        "outputId": "c85a4dab-da60-4098-99a0-c60553455033"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', 27), ('Bob', 32), ('Charlie', 30)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "â€¢\tNo direct apply() on PySpark DataFrames like Pandas.\n",
        "\n",
        "â€¢\tYou use:\n",
        "\n",
        "  o\tpandas_udf with .apply() â†’ for row/column ops inside Pandas.\n",
        "\n",
        "  o\tapplyInPandas() â†’ for grouped transformations.\n",
        "\n",
        "  o\tGroupedData + Pandas UDF â†’ for custom aggregations.\n",
        "\n",
        "  o\tRDD .map() â†’ as a lower-level apply.\n",
        "\n",
        "\n",
        "Great one  â€” letâ€™s carefully unpack map() and flatMap() in PySpark.\n",
        "\n",
        "These two are RDD (Resilient Distributed Dataset) methods, not DataFrame methods. They're used for low-level transformations in Spark.\n"
      ],
      "metadata": {
        "id": "6dEj28_I_4pO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LOysyFX_0Ri"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "map() in PySpark\n",
        "\n",
        "  â€¢\tApplies a function to each element of the RDD.\n",
        "\n",
        "  â€¢\tReturns a new RDD where each input element produces exactly one output element.\n",
        "\n",
        "  â€¢\tOutput count = Input count (1 â†’ 1 mapping).\n",
        "\n",
        "\n",
        "\n",
        "Good for element-wise transformations.\n"
      ],
      "metadata": {
        "id": "isgHm2WOAKs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example: map()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MapFlatMapExample\").getOrCreate()\n",
        "\n",
        "# Create an RDD\n",
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Multiply each element by 2\n",
        "mapped_rdd = rdd.map(lambda x: x * 2)\n",
        "\n",
        "print(mapped_rdd.collect())\n",
        "\n",
        "#Each element produced one output.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3bL9WBKAaD-",
        "outputId": "0b15bda0-433f-4457-df4c-8f15afaaea3c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "flatMap() in PySpark\n",
        "\n",
        "â€¢\tSimilar to map(), but flattens the results.\n",
        "\n",
        "â€¢\tEach input element can produce zero, one, or many output elements.\n",
        "\n",
        "â€¢\tOutput count â‰  Input count.\n",
        "\n",
        "\n",
        "Good for splitting, expanding, or filtering data.\n"
      ],
      "metadata": {
        "id": "_-KOGkKtAfYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example: flatMap()\n",
        "\n",
        "# RDD of sentences\n",
        "rdd2 = spark.sparkContext.parallelize([\"hello world\", \"spark map flatmap\", \"pyspark example\"])\n",
        "\n",
        "# Split each sentence into words\n",
        "flatmapped_rdd = rdd2.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "print(flatmapped_rdd.collect())\n",
        "\n",
        "#Each sentence produced multiple words, and flatMap() flattened them into a single list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6QiHJ5NAnRs",
        "outputId": "47967889-80e4-4033-f9f1-a8cadcd3ba92"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'spark', 'map', 'flatmap', 'pyspark', 'example']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparison Between map() and flatMap()\n",
        "\n",
        "#Using map() for word splitting:\n",
        "\n",
        "mapped_words = rdd2.map(lambda line: line.split(\" \"))\n",
        "print(mapped_words.collect())\n",
        "\n",
        "#Result is a list of lists (not flattened).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dUtzBB2Aq1L",
        "outputId": "ac388456-f8b9-4253-d7e0-1108ad36440e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['hello', 'world'], ['spark', 'map', 'flatmap'], ['pyspark', 'example']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using flatMap() for word splitting:\n",
        "\n",
        "flatmapped_words = rdd2.flatMap(lambda line: line.split(\" \"))\n",
        "print(flatmapped_words.collect())\n",
        "\n",
        "#Result is a flat list of words.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kr2Uz7vA8VB",
        "outputId": "82a1dc30-4f70-4935-ccf9-62413f2bef1e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'spark', 'map', 'flatmap', 'pyspark', 'example']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Another Example: Filtering with flatMap()\n",
        "\n",
        "# RDD with numbers\n",
        "nums = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# flatMap returns [] for odd numbers, [x] for even numbers\n",
        "evens = nums.flatMap(lambda x: [x] if x % 2 == 0 else [])\n",
        "\n",
        "print(evens.collect())\n",
        "\n",
        "#Unlike map() which always returns one value, flatMap() can return zero elements (filtering effect).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm28dsGyA8HO",
        "outputId": "076f3de6-56c9-44ad-a7d1-9f69d0c03c39"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Syntax\n",
        "\n",
        "rdd.foreach(f)\n",
        "\n",
        "f â†’ a function to be executed on each element\n"
      ],
      "metadata": {
        "id": "degM7ua4Xc7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1: Simple Print\n",
        "\n",
        "# Create an RDD\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Apply foreach\n",
        "def print_element(x):\n",
        "    print(f\"Value: {x}\")\n",
        "\n",
        "rdd.foreach(print_element)\n",
        "#Note: You might not always see output in the driver logs because the printing happens on worker nodes.\n"
      ],
      "metadata": {
        "id": "5lt45LjYDwQ-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing to an External File\n",
        "\n",
        "import os\n",
        "\n",
        "def write_to_file(x):\n",
        "    with open(\"output.txt\", \"a\") as f:\n",
        "        f.write(str(x) + \"\\n\")\n",
        "\n",
        "rdd.foreach(write_to_file)\n",
        "\n",
        "#Each worker writes locally on its machine, not to the driver.\n",
        "#So this is useful only in distributed storage or external databases\n"
      ],
      "metadata": {
        "id": "-mUpTIFoXwYW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Using foreach for Database Insert\n",
        "\n",
        "def insert_to_db(x):\n",
        "    # Example: mock DB insert\n",
        "    print(f\"Inserting {x} into database...\")\n",
        "\n",
        "rdd.foreach(insert_to_db)\n"
      ],
      "metadata": {
        "id": "oI-lGBSlXwVW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ Difference Between foreach() and map()\n",
        "\n",
        "map() â†’ transformation, returns a new RDD.\n",
        "\n",
        "foreach() â†’ action, returns nothing.\n"
      ],
      "metadata": {
        "id": "yK6ezSZsX78U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using map\n",
        "\n",
        "mapped = rdd.map(lambda x: x*2)\n",
        "print(mapped.collect())  # [2,4,6,8,10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJeRe6uKXwQ6",
        "outputId": "2cd254f4-8761-4276-f1f3-d7da5bbafda4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using foreach\n",
        "\n",
        "rdd.foreach(lambda x: print(x*2))  # Prints values but no return"
      ],
      "metadata": {
        "id": "Rg3FyHjuXwPA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways:\n",
        "\n",
        "foreach() is an action.\n",
        "\n",
        "Executes function on each element of RDD.\n",
        "\n",
        "Typically used for side effects (DB updates, external API calls, logging).\n",
        "\n",
        "Doesnâ€™t return an RDD or DataFrame"
      ],
      "metadata": {
        "id": "oq7N9dKrYRHU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXmFi4FRYVPd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is partitionBy()?\n",
        "\n",
        "partitionBy() is used when writing data (especially in formats like Parquet, ORC, Avro, CSV) to organize the output files into separate folders based on one or more columns.\n",
        "\n",
        "This helps with:\n",
        "\n",
        "Efficient querying (only scanning partitions needed).\n",
        "\n",
        "Reducing data size when reading.\n",
        "\n",
        "Better performance with tools like Spark SQL, Hive, Presto, etc.\n",
        "\n",
        "\n",
        "Syntax\n",
        "DataFrameWriter.partitionBy(col1, col2, ...).format(\"...\").save(path)\n",
        "\n",
        "\n",
        "col1, col2 â†’ columns to partition the data by.\n",
        "\n",
        "format(\"parquet\") (or CSV, JSON, etc.).\n",
        "\n",
        "save(path) â†’ location in HDFS/local/S3.\n"
      ],
      "metadata": {
        "id": "VTE4hdaGYWXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1: Partition by one column\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Alice\", \"HR\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Charlie\", \"IT\", 4500),\n",
        "    (4, \"David\", \"Finance\", 3500),\n",
        "    (5, \"Eve\", \"HR\", 3200)\n",
        "]\n",
        "\n",
        "columns = [\"id\", \"name\", \"dept\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Write data partitioned by 'dept'\n",
        "df.write.partitionBy(\"dept\").mode(\"overwrite\").parquet(\"output/employee_partitioned\")\n",
        "\n",
        "#This creates a folder structure like:\n"
      ],
      "metadata": {
        "id": "XwkMtbkeYhsd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2: Partition by multiple columns\n",
        "# Write data partitioned by 'dept' and 'salary'\n",
        "\n",
        "df.write.partitionBy(\"dept\", \"salary\").mode(\"overwrite\").parquet(\"output/employee_multi_partitioned\")\n",
        "\n",
        "\n",
        "#Folder structure will look like in the output"
      ],
      "metadata": {
        "id": "YUQhSURXYtEW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Reading partitioned data\n",
        "# Read the partitioned Parquet back\n",
        "\n",
        "df_read = spark.read.parquet(\"output/employee_partitioned\")\n",
        "\n",
        "df_read.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcLaC_s5Y2cR",
        "outputId": "d28a82be-dda2-4fca-92ef-1dbe8c842864"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-------+\n",
            "| id|   name|salary|   dept|\n",
            "+---+-------+------+-------+\n",
            "|  3|Charlie|  4500|     IT|\n",
            "|  4|  David|  3500|Finance|\n",
            "|  1|  Alice|  3000|     HR|\n",
            "|  2|    Bob|  4000|     IT|\n",
            "|  5|    Eve|  3200|     HR|\n",
            "+---+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "sTJmhB77aWiY",
        "outputId": "3072dc73-81d7-4986-d815-1a8388a9e3f5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Notes:\n",
        "\n",
        "partitionBy() does not change data inside files, only the directory structure.\n",
        "\n",
        "Itâ€™s mainly useful for big data optimization.\n",
        "\n",
        "Works best with formats like Parquet and ORC (not efficient with CSV/JSON).\n"
      ],
      "metadata": {
        "id": "ax621Vp7ZB27"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AklCHkXaZDSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is MapType?\n",
        "\n",
        "MapType is a Spark SQL data type that stores key-value pairs (like a Python dictionary).\n",
        "\n",
        "Both keys and values have fixed data types (e.g., StringType for keys and IntegerType for values).\n",
        "\n",
        "Keys are always non-null, but values can be nullable depending on schema definition.\n",
        "\n",
        "Syntax\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType\n",
        "\n",
        "MapType(keyType, valueType, valueContainsNull=True)\n",
        "keyType â†’ Data type of keys (e.g., StringType(), IntegerType()).\n",
        "\n",
        "valueType â†’ Data type of values.\n",
        "\n",
        "valueContainsNull â†’ Boolean (default = True). Whether map values can contain null.\n"
      ],
      "metadata": {
        "id": "6ALA_js6ZIxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1: Creating a MapType Column\n",
        "\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField\n",
        "\n",
        "data = [\n",
        "    (1, {\"math\": 80, \"english\": 90}),\n",
        "    (2, {\"math\": 85, \"science\": 95}),\n",
        "    (3, None)  # null map\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"scores\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlhURCnqZN5G",
        "outputId": "e963d980-c7c1-4bb1-9b18-fcb595ff28f9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------+\n",
            "|id |scores                     |\n",
            "+---+---------------------------+\n",
            "|1  |{english -> 90, math -> 80}|\n",
            "|2  |{science -> 95, math -> 85}|\n",
            "|3  |NULL                       |\n",
            "+---+---------------------------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- scores: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2: Accessing Map Values\n",
        "\n",
        "#You can use col[\"mapField\"][\"key\"] syntax.\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.select(\n",
        "    col(\"id\"),\n",
        "    col(\"scores\")[\"math\"].alias(\"math_score\"),\n",
        "    col(\"scores\")[\"english\"].alias(\"english_score\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpyG0HZkZVtZ",
        "outputId": "beb8c561-ef10-4309-efdd-bff05b87215e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------------+\n",
            "| id|math_score|english_score|\n",
            "+---+----------+-------------+\n",
            "|  1|        80|           90|\n",
            "|  2|        85|         NULL|\n",
            "|  3|      NULL|         NULL|\n",
            "+---+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Creating Map Column Dynamically\n",
        "#You can create a map column using create_map():\n",
        "\n",
        "from pyspark.sql.functions import create_map, lit\n",
        "\n",
        "df2 = df.withColumn(\"extra\", create_map(lit(\"physics\"), lit(88), lit(\"chemistry\"), lit(77)))\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "id": "u1hn-ARLZiza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways\n",
        "\n",
        "MapType is like a dictionary in PySpark DataFrames.\n",
        "\n",
        "Keys are always non-null, values can be null.\n",
        "\n",
        "Use col[\"mapField\"][\"key\"] to extract values.\n",
        "\n",
        "Use create_map() to create new map columns.\n"
      ],
      "metadata": {
        "id": "VubYMWnRZufz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pm9LYd0UDdhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"emp.csv\")"
      ],
      "metadata": {
        "id": "rELLni6o-rg2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK8o0SVh-rKw",
        "outputId": "45ab5ef6-e01b-4703-f788-3d485dda0311"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bonus(salary):\n",
        "  return int(salary) * 0.1"
      ],
      "metadata": {
        "id": "zRhNtT2O-q0B"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "bonus_udf = udf(bonus)\n",
        "spark.udf.register(\"bonus_sql_udf\", bonus, \"double\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "2hvmqa5JA9Rq",
        "outputId": "b2177838-2ae1-4f10-cd69-a03940def9a0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.bonus(salary)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>bonus</b><br/>def bonus(salary)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-2677044005.py</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"bonus\", bonus_udf(col(\"salary\"))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhCIC_QtBNZT",
        "outputId": "8cfb22ff-1541-41e3-ad54-4ab39a995029"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date| bonus|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|5000.0|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|4500.0|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|5500.0|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|4800.0|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|6000.0|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|5200.0|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|7000.0|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|5100.0|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|5800.0|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|4700.0|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|6500.0|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|5400.0|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|7500.0|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|4600.0|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|6300.0|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|4900.0|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|5700.0|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|5000.0|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|6200.0|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|5300.0|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"bonus\", col(\"salary\")*1.1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roxaqV4iBWo_",
        "outputId": "5cbb4f3f-30e7-49ca-aa5a-48ae501ac473"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+-----------------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|            bonus|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|55000.00000000001|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|49500.00000000001|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|60500.00000000001|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|52800.00000000001|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|          66000.0|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|57200.00000000001|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|          77000.0|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|56100.00000000001|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|63800.00000000001|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|51700.00000000001|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|          71500.0|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|59400.00000000001|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|          82500.0|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|50600.00000000001|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|          69300.0|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|53900.00000000001|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|62700.00000000001|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|55000.00000000001|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|          68200.0|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|58300.00000000001|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using a UDF with transform()\n",
        "\n",
        "# Define a function that uses the UDF\n",
        "def add_bonus_udf_transform(dataframe):\n",
        "    return dataframe.withColumn(\"bonus_from_udf\", bonus_udf(col(\"salary\")))\n",
        "\n",
        "# Apply the transformation using transform()\n",
        "df_with_udf_transform = df.transform(add_bonus_udf_transform)\n",
        "\n",
        "df_with_udf_transform.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS-pV1NfCkWL",
        "outputId": "a4062141-ca66-4439-9ee7-2daa79ff1551"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+--------------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|bonus_from_udf|\n",
            "+-----------+-------------+-------------+---+------+------+----------+--------------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|        5000.0|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|        4500.0|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|        5500.0|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|        4800.0|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|        6000.0|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|        5200.0|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|        7000.0|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|        5100.0|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|        5800.0|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|        4700.0|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|        6500.0|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|        5400.0|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|        7500.0|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|        4600.0|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|        6300.0|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|        4900.0|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|        5700.0|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|        5000.0|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|        6200.0|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|        5300.0|\n",
            "+-----------+-------------+-------------+---+------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a93ed198"
      },
      "source": [
        "Advantages of using UDFs over simple Python functions in PySpark:\n",
        "\n",
        "*   **Integration with Spark SQL:** Registered UDFs can be directly used in Spark SQL queries, making your logic accessible from both DataFrame API and SQL.\n",
        "*   **Serialization and Distribution:** Spark handles the serialization and distribution of UDFs to worker nodes, allowing your custom logic to be executed in a distributed manner.\n",
        "*   **Performance (Pandas UDFs):** Pandas UDFs (vectorized UDFs) can offer significant performance improvements for certain operations by leveraging Apache Arrow and processing data in batches within Pandas DataFrames.\n",
        "*   **Reusability:** Once defined and registered, a UDF can be easily reused across different parts of your Spark application.\n",
        "*   **Handling Complex Logic:** UDFs are useful when the required transformation logic is too complex to express using built-in Spark functions."
      ]
    }
  ]
}