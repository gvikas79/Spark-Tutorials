{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSW3cR2HPW6pN/d7xo7SMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvikas79/Spark-Tutorials/blob/main/spark_class3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is collect() in PySpark?**\n",
        "\n",
        "*   collect() is an action in PySpark (not a transformation).\n",
        "*   It retrieves all rows of a DataFrame (or RDD) to the driver node as a list of Row objects.\n",
        "*   Since Spark works in a distributed environment, data is spread across executors.\n",
        "*  collect() brings everything back to your local Python process.\n",
        "\n",
        "\n",
        "Warning: Donâ€™t use collect() on very large datasets (it can cause OutOfMemoryError). Use it only for small results, testing, or debugging.\n",
        "\n",
        "Syntax\n",
        "\n",
        "DataFrame.collect()\n",
        "\n",
        "Returns:\n",
        "A list of Row objects, where each row represents a record from the DataFrame.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2_ebj63J7WrJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X27pjT2A6z6W",
        "outputId": "01cb7d38-306b-4022-bc7b-3ce7759ef823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   name|age|salary|\n",
            "+-------+---+------+\n",
            "|  Alice| 25|  3000|\n",
            "|    Bob| 30|  4000|\n",
            "|Charlie| 28|  5000|\n",
            "+-------+---+------+\n",
            "\n",
            "[Row(name='Alice', age=25, salary=3000), Row(name='Bob', age=30, salary=4000), Row(name='Charlie', age=28, salary=5000)]\n"
          ]
        }
      ],
      "source": [
        "#Example 1: Basic Usage\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CollectExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25, 3000), (\"Bob\", 30, 4000), (\"Charlie\", 28, 5000)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Collect all rows\n",
        "result = df.collect()\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2: Iterating Over Collected Data\n",
        "rows = df.collect()\n",
        "\n",
        "for row in rows:\n",
        "    print(row[\"name\"], row[\"age\"], row[\"salary\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C53I-n4g8Az8",
        "outputId": "3cfd0f00-d0cc-4944-8b2b-fea70b110bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice 25 3000\n",
            "Bob 30 4000\n",
            "Charlie 28 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Convert to Pandas\n",
        "#Often, after collecting, people convert to Pandas for local analysis:\n",
        "\n",
        "pandas_df = df.toPandas()\n",
        "print(pandas_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA_kae_v8Fuj",
        "outputId": "38be4272-9e54-4f75-8bb6-86271b1cb81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      name  age  salary\n",
            "0    Alice   25    3000\n",
            "1      Bob   30    4000\n",
            "2  Charlie   28    5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 4: Collect with Filtering\n",
        "\n",
        "result = df.filter(df[\"age\"] > 26).collect()\n",
        "for row in result:\n",
        "    print(row.name, row.salary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY7QxtHP8Ssv",
        "outputId": "85c4296f-734b-4987-d4fc-e40a9b038fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bob 4000\n",
            "Charlie 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use collect()?\n",
        "\n",
        "Use collect() when:\n",
        "\n",
        "â€¢\tDataset is small enough to fit in memory.\n",
        "\n",
        "â€¢\tYou want to debug, print, or inspect results locally.\n",
        "\n",
        "â€¢\tYouâ€™re passing results to an external Python library (like Pandas, NumPy, Matplotlib).\n",
        "\n",
        "Avoid collect() when:\n",
        "â€¢\tDataset is large (millions of rows, GBs of data).\n",
        "â€¢\tIt can cause driver out of memory issues.\n",
        "\n",
        "Tip: For safer alternatives use:\n",
        "\n",
        "â€¢\tshow(n) â†’ prints first n rows nicely.\n",
        "\n",
        "â€¢\ttake(n) â†’ returns first n rows as list.\n",
        "\n",
        "â€¢\tlimit(n).collect() â†’ collects only a subset.\n",
        "\n"
      ],
      "metadata": {
        "id": "no-x3Xxe8g5x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XPC_R9nCadx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is transform() in PySpark?\n",
        "\n",
        "â€¢\ttransform() is available on DataFrame objects.\n",
        "\n",
        "â€¢\tIt allows you to apply a function (transformation) to a DataFrame in a clean, reusable, and chainable way.\n",
        "\n",
        "â€¢\tInstead of writing complex transformations inline, you can wrap them in functions and pass them to transform().\n",
        "\n",
        "â€¢\tIt improves readability and reusability of your PySpark code.\n",
        "________________________________________\n",
        "Syntax\n",
        "DataFrame.transform(func)\n",
        "\n",
        "â€¢\tfunc â†’ a Python function that takes a DataFrame as input and returns a DataFrame.\n",
        "\n",
        "â€¢\tReturns â†’ the transformed DataFrame.\n"
      ],
      "metadata": {
        "id": "JWusN--j84OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1: Basic Usage\n",
        "from pyspark.sql.functions import col\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25, 3000), (\"Bob\", 30, 4000), (\"Charlie\", 28, 5000)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoupuWMJ8WkD",
        "outputId": "71c3d873-f200-4ea2-9ce7-8a5696d15344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   name|age|salary|\n",
            "+-------+---+------+\n",
            "|  Alice| 25|  3000|\n",
            "|    Bob| 30|  4000|\n",
            "|Charlie| 28|  5000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to add a 10% bonus to salary\n",
        "\n",
        "def add_bonus(dataframe):\n",
        "    return dataframe.withColumn(\"salary_with_bonus\", col(\"salary\") * 1.1)\n",
        "\n",
        "# Apply using transform\n",
        "df_transformed = df.transform(add_bonus)\n",
        "df_transformed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhgA2tJm9N0L",
        "outputId": "5ace540a-c706-45f5-e776-59b7276e4ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+------------------+\n",
            "|   name|age|salary| salary_with_bonus|\n",
            "+-------+---+------+------------------+\n",
            "|  Alice| 25|  3000|3300.0000000000005|\n",
            "|    Bob| 30|  4000|            4400.0|\n",
            "|Charlie| 28|  5000|            5500.0|\n",
            "+-------+---+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2: Chaining Multiple transform() Calls\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "# Function to uppercase name\n",
        "def uppercase_name(df):\n",
        "    return df.withColumn(\"name_upper\", upper(col(\"name\")))\n",
        "\n",
        "# Function to categorize salary\n",
        "def categorize_salary(df):\n",
        "    return df.withColumn(\"salary_level\",\n",
        "                         (col(\"salary\") > 4000).cast(\"string\"))\n",
        "\n",
        "# Apply multiple transformations\n",
        "df_chain = df.transform(uppercase_name).transform(categorize_salary)\n",
        "df_chain.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5S7lU1l9Sjz",
        "outputId": "96bd14fe-f4e3-4476-a91f-44b2447d6b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+----------+------------+\n",
            "|   name|age|salary|name_upper|salary_level|\n",
            "+-------+---+------+----------+------------+\n",
            "|  Alice| 25|  3000|     ALICE|       false|\n",
            "|    Bob| 30|  4000|       BOB|       false|\n",
            "|Charlie| 28|  5000|   CHARLIE|        true|\n",
            "+-------+---+------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Passing Parameters with lambda\n",
        "#You can also use lambda functions directly:\n",
        "\n",
        "df_lambda = df.transform(lambda d: d.withColumn(\"age_plus_5\", col(\"age\") + 5))\n",
        "df_lambda.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqvzlDSB9dbW",
        "outputId": "d6437351-1d42-4911-b50d-6056853feff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+----------+\n",
            "|   name|age|salary|age_plus_5|\n",
            "+-------+---+------+----------+\n",
            "|  Alice| 25|  3000|        30|\n",
            "|    Bob| 30|  4000|        35|\n",
            "|Charlie| 28|  5000|        33|\n",
            "+-------+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 4: Reusable Pipeline with transform()\n",
        "\n",
        "def pipeline(df):\n",
        "    return (df\n",
        "            .transform(add_bonus)         # Step 1: Add bonus\n",
        "            .transform(uppercase_name)    # Step 2: Uppercase name\n",
        "            .transform(lambda d: d.filter(col(\"age\") > 26))  # Step 3: Filter\n",
        "           )\n",
        "\n",
        "df_pipeline = df.transform(pipeline)\n",
        "df_pipeline.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wnqhbe09wVl",
        "outputId": "ad948eb4-269b-4d78-ed83-71388051a9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+-----------------+----------+\n",
            "|   name|age|salary|salary_with_bonus|name_upper|\n",
            "+-------+---+------+-----------------+----------+\n",
            "|    Bob| 30|  4000|           4400.0|       BOB|\n",
            "|Charlie| 28|  5000|           5500.0|   CHARLIE|\n",
            "+-------+---+------+-----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Why Use transform()?\n",
        "\n",
        "Makes code cleaner & modular (define transformations once, reuse many times).\n",
        "\n",
        "Useful when building pipelines of transformations.\n",
        "\n",
        "Works well with functional programming style in PySpark.\n"
      ],
      "metadata": {
        "id": "C4kFoz4395Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Direct Method Chaining\n",
        "#You can chain transformations directly on the DataFrame:\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TransformVsChaining\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 25, 3000), (\"Bob\", 30, 4000), (\"Charlie\", 28, 5000)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])\n",
        "\n",
        "# Direct chaining\n",
        "df_chain = (\n",
        "    df.withColumn(\"salary_with_bonus\", col(\"salary\") * 1.1)\n",
        "      .withColumn(\"name_upper\", upper(col(\"name\")))\n",
        "      .filter(col(\"age\") > 26)\n",
        ")\n",
        "\n",
        "df_chain.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOcmVXVj90XQ",
        "outputId": "70638664-c6e7-4051-bbce-e00fa42fafe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+-----------------+----------+\n",
            "|   name|age|salary|salary_with_bonus|name_upper|\n",
            "+-------+---+------+-----------------+----------+\n",
            "|    Bob| 30|  4000|           4400.0|       BOB|\n",
            "|Charlie| 28|  5000|           5500.0|   CHARLIE|\n",
            "+-------+---+------+-----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using transform()\n",
        "#Instead of repeating logic, you define reusable functions:\n",
        "# Define reusable transformations\n",
        "\n",
        "def add_bonus(df):\n",
        "    return df.withColumn(\"salary_with_bonus\", col(\"salary\") * 1.1)\n",
        "\n",
        "def uppercase_name(df):\n",
        "    return df.withColumn(\"name_upper\", upper(col(\"name\")))\n",
        "\n",
        "def filter_age(df):\n",
        "    return df.filter(col(\"age\") > 26)\n",
        "\n",
        "# Apply with transform\n",
        "df_transformed = (\n",
        "    df.transform(add_bonus)\n",
        "      .transform(uppercase_name)\n",
        "      .transform(filter_age)\n",
        ")\n",
        "\n",
        "df_transformed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRH-_aNP-P9z",
        "outputId": "65b4fc8f-e78c-49d3-fb2e-f90010aa2595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+-----------------+----------+\n",
            "|   name|age|salary|salary_with_bonus|name_upper|\n",
            "+-------+---+------+-----------------+----------+\n",
            "|    Bob| 30|  4000|           4400.0|       BOB|\n",
            "|Charlie| 28|  5000|           5500.0|   CHARLIE|\n",
            "+-------+---+------+-----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output is the same, but advantages:\n",
        "\n",
        "â€¢\tModular: Each transformation is a function.\n",
        "\n",
        "â€¢\tReusable: You can apply add_bonus() or filter_age() to other DataFrames easily.\n",
        "\n",
        "â€¢\tReadable: Clearly separates logical steps.\n"
      ],
      "metadata": {
        "id": "n5WcCmOY-XMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mixing lambda with transform()\n",
        "\n",
        "#For quick one-off transformations:\n",
        "df_lambda = (\n",
        "    df.transform(add_bonus)\n",
        "      .transform(lambda d: d.withColumn(\"age_plus_5\", col(\"age\") + 5))\n",
        ")\n",
        "df_lambda.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLcjSVvN-bCR",
        "outputId": "4d0b65ae-b7a7-42c0-e8a3-97950a905a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+------------------+----------+\n",
            "|   name|age|salary| salary_with_bonus|age_plus_5|\n",
            "+-------+---+------+------------------+----------+\n",
            "|  Alice| 25|  3000|3300.0000000000005|        30|\n",
            "|    Bob| 30|  4000|            4400.0|        35|\n",
            "|Charlie| 28|  5000|            5500.0|        33|\n",
            "+-------+---+------+------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use What?\n",
        "\n",
        "Approach\tBest for\n",
        "Method chaining\tQuick scripts, small transformations, throwaway code\n",
        "transform()\n",
        "Reusable pipelines, production code, when the same transformations must be applied to multiple DataFrames\n",
        "________________________________________\n",
        "In real-world projects, transform() shines when:\n",
        "\n",
        "â€¢\tYou build data pipelines.\n",
        "\n",
        "â€¢\tYou want clean, testable, reusable code.\n",
        "\n",
        "â€¢\tYouâ€™re working in a team (easier to understand functions like add_bonus than inline chains).\n"
      ],
      "metadata": {
        "id": "pDdeS5SJ-nSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike Pandas, PySpark DataFrame does not have an apply() method.\n",
        "Instead, there are different contexts where apply() exists in PySpark:\n",
        "\n",
        "1.\tapply() in Pandas UDFs (with pandas_udf)\n",
        "\n",
        "2.\tapplyInPandas() on DataFrames\n",
        "\n",
        "3.\tapply() in grouped operations (GroupedData)\n",
        "\n",
        "4.\tRDD map() / mapPartitions() (the lower-level equivalent to apply logic)\n",
        "\n",
        "Letâ€™s go through them one by one with code examples:\n"
      ],
      "metadata": {
        "id": "TNdhgPq3-2Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apply() in Pandas UDFs\n",
        "#PySpark integrates with Pandas via vectorized UDFs.\n",
        "#Here, apply() is used inside Pandas UDF functions.\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 28)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Define a Pandas UDF that applies a custom transformation\n",
        "@pandas_udf(\"int\")\n",
        "def add_five(age_series: pd.Series) -> pd.Series:\n",
        "    return age_series.apply(lambda x: x + 5)\n",
        "\n",
        "# Use it in a DataFrame\n",
        "df_with_new = df.withColumn(\"age_plus_5\", add_five(df[\"age\"]))\n",
        "df_with_new.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX62V3ji-e02",
        "outputId": "b8a4fd7a-203a-4d9d-e398-eed8aef03093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+\n",
            "|   name|age|age_plus_5|\n",
            "+-------+---+----------+\n",
            "|  Alice| 25|        30|\n",
            "|    Bob| 30|        35|\n",
            "|Charlie| 28|        33|\n",
            "+-------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#applyInPandas()\n",
        "#This is a DataFrame-level method that allows you to apply a function on grouped Pandas DataFrames.\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Schema for output\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age_plus_10\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Function to apply on each Pandas DataFrame\n",
        "def add_ten(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    pdf[\"age_plus_10\"] = pdf[\"age\"] + 10\n",
        "    return pdf[[\"name\", \"age_plus_10\"]]\n",
        "\n",
        "# Use applyInPandas\n",
        "df_applied = df.groupBy(\"name\").applyInPandas(add_ten, schema=schema)\n",
        "df_applied.show()\n",
        "\n",
        "#applyInPandas() is grouped: Spark splits data into groups â†’ converts each group to Pandas â†’ applies the function â†’ merges results back."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQRP4znk_RDa",
        "outputId": "ee04d50e-12a3-4c8b-9ade-795a0a2cfcc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|   name|age_plus_10|\n",
            "+-------+-----------+\n",
            "|  Alice|         35|\n",
            "|    Bob|         40|\n",
            "|Charlie|         38|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Equivalent of apply() on RDDs\n",
        "#If you want Pandas-style apply() for row-wise operations, you can use map() on RDDs:\n",
        "\n",
        "# Convert DataFrame to RDD\n",
        "rdd = df.rdd\n",
        "\n",
        "# Apply transformation (similar to row-wise apply)\n",
        "rdd_applied = rdd.map(lambda row: (row[\"name\"], row[\"age\"] + 2))\n",
        "print(rdd_applied.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH6JFEN1_VEh",
        "outputId": "31b355ac-0495-49ab-8cc8-1be067835546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', 27), ('Bob', 32), ('Charlie', 30)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "â€¢\tNo direct apply() on PySpark DataFrames like Pandas.\n",
        "\n",
        "â€¢\tYou use:\n",
        "\n",
        "  o\tpandas_udf with .apply() â†’ for row/column ops inside Pandas.\n",
        "\n",
        "  o\tapplyInPandas() â†’ for grouped transformations.\n",
        "\n",
        "  o\tGroupedData + Pandas UDF â†’ for custom aggregations.\n",
        "\n",
        "  o\tRDD .map() â†’ as a lower-level apply.\n",
        "\n",
        "\n",
        "Great one  â€” letâ€™s carefully unpack map() and flatMap() in PySpark.\n",
        "\n",
        "These two are RDD (Resilient Distributed Dataset) methods, not DataFrame methods. They're used for low-level transformations in Spark.\n"
      ],
      "metadata": {
        "id": "6dEj28_I_4pO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LOysyFX_0Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "map() in PySpark\n",
        "\n",
        "  â€¢\tApplies a function to each element of the RDD.\n",
        "\n",
        "  â€¢\tReturns a new RDD where each input element produces exactly one output element.\n",
        "\n",
        "  â€¢\tOutput count = Input count (1 â†’ 1 mapping).\n",
        "\n",
        "\n",
        "\n",
        "Good for element-wise transformations.\n"
      ],
      "metadata": {
        "id": "isgHm2WOAKs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example: map()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MapFlatMapExample\").getOrCreate()\n",
        "\n",
        "# Create an RDD\n",
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Multiply each element by 2\n",
        "mapped_rdd = rdd.map(lambda x: x * 2)\n",
        "\n",
        "print(mapped_rdd.collect())\n",
        "\n",
        "#Each element produced one output.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3bL9WBKAaD-",
        "outputId": "eb358975-c20d-45b8-d4ff-aea9b23393ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "flatMap() in PySpark\n",
        "\n",
        "â€¢\tSimilar to map(), but flattens the results.\n",
        "\n",
        "â€¢\tEach input element can produce zero, one, or many output elements.\n",
        "\n",
        "â€¢\tOutput count â‰  Input count.\n",
        "\n",
        "\n",
        "Good for splitting, expanding, or filtering data.\n"
      ],
      "metadata": {
        "id": "_-KOGkKtAfYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example: flatMap()\n",
        "\n",
        "# RDD of sentences\n",
        "rdd2 = spark.sparkContext.parallelize([\"hello world\", \"spark map flatmap\", \"pyspark example\"])\n",
        "\n",
        "# Split each sentence into words\n",
        "flatmapped_rdd = rdd2.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "print(flatmapped_rdd.collect())\n",
        "\n",
        "#Each sentence produced multiple words, and flatMap() flattened them into a single list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6QiHJ5NAnRs",
        "outputId": "b7a00d40-0a2e-4b2e-ca80-ff613f27b34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'spark', 'map', 'flatmap', 'pyspark', 'example']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparison Between map() and flatMap()\n",
        "\n",
        "#Using map() for word splitting:\n",
        "\n",
        "mapped_words = rdd2.map(lambda line: line.split(\" \"))\n",
        "print(mapped_words.collect())\n",
        "\n",
        "#Result is a list of lists (not flattened).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dUtzBB2Aq1L",
        "outputId": "c502953c-bdc6-4c76-8c71-d22dad4add96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['hello', 'world'], ['spark', 'map', 'flatmap'], ['pyspark', 'example']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using flatMap() for word splitting:\n",
        "\n",
        "flatmapped_words = rdd2.flatMap(lambda line: line.split(\" \"))\n",
        "print(flatmapped_words.collect())\n",
        "\n",
        "#Result is a flat list of words.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kr2Uz7vA8VB",
        "outputId": "e46b9593-79ab-4fd7-8149-6e110fea3a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'spark', 'map', 'flatmap', 'pyspark', 'example']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Another Example: Filtering with flatMap()\n",
        "\n",
        "# RDD with numbers\n",
        "nums = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# flatMap returns [] for odd numbers, [x] for even numbers\n",
        "evens = nums.flatMap(lambda x: [x] if x % 2 == 0 else [])\n",
        "\n",
        "print(evens.collect())\n",
        "\n",
        "#Unlike map() which always returns one value, flatMap() can return zero elements (filtering effect).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm28dsGyA8HO",
        "outputId": "1876b49a-c5a3-440b-ffdb-c06ad3ef9a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Syntax\n",
        "\n",
        "rdd.foreach(f)\n",
        "\n",
        "f â†’ a function to be executed on each element\n"
      ],
      "metadata": {
        "id": "degM7ua4Xc7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1: Simple Print\n",
        "\n",
        "# Create an RDD\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Apply foreach\n",
        "def print_element(x):\n",
        "    print(f\"Value: {x}\")\n",
        "\n",
        "rdd.foreach(print_element)\n",
        "#Note: You might not always see output in the driver logs because the printing happens on worker nodes.\n"
      ],
      "metadata": {
        "id": "5lt45LjYDwQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing to an External File\n",
        "\n",
        "import os\n",
        "\n",
        "def write_to_file(x):\n",
        "    with open(\"output.txt\", \"a\") as f:\n",
        "        f.write(str(x) + \"\\n\")\n",
        "\n",
        "rdd.foreach(write_to_file)\n",
        "\n",
        "#Each worker writes locally on its machine, not to the driver.\n",
        "#So this is useful only in distributed storage or external databases\n"
      ],
      "metadata": {
        "id": "-mUpTIFoXwYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Using foreach for Database Insert\n",
        "\n",
        "def insert_to_db(x):\n",
        "    # Example: mock DB insert\n",
        "    print(f\"Inserting {x} into database...\")\n",
        "\n",
        "rdd.foreach(insert_to_db)\n"
      ],
      "metadata": {
        "id": "oI-lGBSlXwVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ Difference Between foreach() and map()\n",
        "\n",
        "map() â†’ transformation, returns a new RDD.\n",
        "\n",
        "foreach() â†’ action, returns nothing.\n"
      ],
      "metadata": {
        "id": "yK6ezSZsX78U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using map\n",
        "\n",
        "mapped = rdd.map(lambda x: x*2)\n",
        "print(mapped.collect())  # [2,4,6,8,10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJeRe6uKXwQ6",
        "outputId": "ebfde4ae-ae8e-4b34-b937-f1b2d66b0bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using foreach\n",
        "\n",
        "rdd.foreach(lambda x: print(x*2))  # Prints values but no return"
      ],
      "metadata": {
        "id": "Rg3FyHjuXwPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways:\n",
        "\n",
        "foreach() is an action.\n",
        "\n",
        "Executes function on each element of RDD.\n",
        "\n",
        "Typically used for side effects (DB updates, external API calls, logging).\n",
        "\n",
        "Doesnâ€™t return an RDD or DataFrame"
      ],
      "metadata": {
        "id": "oq7N9dKrYRHU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXmFi4FRYVPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is partitionBy()?\n",
        "\n",
        "partitionBy() is used when writing data (especially in formats like Parquet, ORC, Avro, CSV) to organize the output files into separate folders based on one or more columns.\n",
        "\n",
        "This helps with:\n",
        "\n",
        "Efficient querying (only scanning partitions needed).\n",
        "\n",
        "Reducing data size when reading.\n",
        "\n",
        "Better performance with tools like Spark SQL, Hive, Presto, etc.\n",
        "\n",
        "\n",
        "Syntax\n",
        "DataFrameWriter.partitionBy(col1, col2, ...).format(\"...\").save(path)\n",
        "\n",
        "\n",
        "col1, col2 â†’ columns to partition the data by.\n",
        "\n",
        "format(\"parquet\") (or CSV, JSON, etc.).\n",
        "\n",
        "save(path) â†’ location in HDFS/local/S3.\n"
      ],
      "metadata": {
        "id": "VTE4hdaGYWXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1: Partition by one column\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Alice\", \"HR\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Charlie\", \"IT\", 4500),\n",
        "    (4, \"David\", \"Finance\", 3500),\n",
        "    (5, \"Eve\", \"HR\", 3200)\n",
        "]\n",
        "\n",
        "columns = [\"id\", \"name\", \"dept\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Write data partitioned by 'dept'\n",
        "df.write.partitionBy(\"dept\").mode(\"overwrite\").parquet(\"output/employee_partitioned\")\n",
        "\n",
        "#This creates a folder structure like:\n"
      ],
      "metadata": {
        "id": "XwkMtbkeYhsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2: Partition by multiple columns\n",
        "# Write data partitioned by 'dept' and 'salary'\n",
        "\n",
        "df.write.partitionBy(\"dept\", \"salary\").mode(\"overwrite\").parquet(\"output/employee_multi_partitioned\")\n",
        "\n",
        "\n",
        "#Folder structure will look like in the output"
      ],
      "metadata": {
        "id": "YUQhSURXYtEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Reading partitioned data\n",
        "# Read the partitioned Parquet back\n",
        "\n",
        "df_read = spark.read.parquet(\"output/employee_partitioned\")\n",
        "\n",
        "df_read.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcLaC_s5Y2cR",
        "outputId": "8807fe64-db72-4f22-806c-7331c2b4fe52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-------+\n",
            "| id|   name|salary|   dept|\n",
            "+---+-------+------+-------+\n",
            "|  3|Charlie|  4500|     IT|\n",
            "|  1|  Alice|  3000|     HR|\n",
            "|  4|  David|  3500|Finance|\n",
            "|  2|    Bob|  4000|     IT|\n",
            "|  5|    Eve|  3200|     HR|\n",
            "+---+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Notes:\n",
        "\n",
        "partitionBy() does not change data inside files, only the directory structure.\n",
        "\n",
        "Itâ€™s mainly useful for big data optimization.\n",
        "\n",
        "Works best with formats like Parquet and ORC (not efficient with CSV/JSON).\n"
      ],
      "metadata": {
        "id": "ax621Vp7ZB27"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AklCHkXaZDSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is MapType?\n",
        "\n",
        "MapType is a Spark SQL data type that stores key-value pairs (like a Python dictionary).\n",
        "\n",
        "Both keys and values have fixed data types (e.g., StringType for keys and IntegerType for values).\n",
        "\n",
        "Keys are always non-null, but values can be nullable depending on schema definition.\n",
        "\n",
        "Syntax\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType\n",
        "\n",
        "MapType(keyType, valueType, valueContainsNull=True)\n",
        "keyType â†’ Data type of keys (e.g., StringType(), IntegerType()).\n",
        "\n",
        "valueType â†’ Data type of values.\n",
        "\n",
        "valueContainsNull â†’ Boolean (default = True). Whether map values can contain null.\n"
      ],
      "metadata": {
        "id": "6ALA_js6ZIxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1: Creating a MapType Column\n",
        "\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField\n",
        "\n",
        "data = [\n",
        "    (1, {\"math\": 80, \"english\": 90}),\n",
        "    (2, {\"math\": 85, \"science\": 95}),\n",
        "    (3, None)  # null map\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"scores\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlhURCnqZN5G",
        "outputId": "27e1c835-6fbb-497d-d1ad-5d294ac2a2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------+\n",
            "|id |scores                     |\n",
            "+---+---------------------------+\n",
            "|1  |{english -> 90, math -> 80}|\n",
            "|2  |{science -> 95, math -> 85}|\n",
            "|3  |NULL                       |\n",
            "+---+---------------------------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- scores: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2: Accessing Map Values\n",
        "\n",
        "#You can use col[\"mapField\"][\"key\"] syntax.\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.select(\n",
        "    col(\"id\"),\n",
        "    col(\"scores\")[\"math\"].alias(\"math_score\"),\n",
        "    col(\"scores\")[\"english\"].alias(\"english_score\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpyG0HZkZVtZ",
        "outputId": "ab2ca123-08b0-4e76-bdb6-87bcb8ddeecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------------+\n",
            "| id|math_score|english_score|\n",
            "+---+----------+-------------+\n",
            "|  1|        80|           90|\n",
            "|  2|        85|         NULL|\n",
            "|  3|      NULL|         NULL|\n",
            "+---+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3: Creating Map Column Dynamically\n",
        "#You can create a map column using create_map():\n",
        "\n",
        "from pyspark.sql.functions import create_map, lit\n",
        "\n",
        "df2 = df.withColumn(\"extra\", create_map(lit(\"physics\"), lit(88), lit(\"chemistry\"), lit(77)))\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "id": "u1hn-ARLZiza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d93e40-b40c-4682-caba-89368048c241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------+--------------------------------+\n",
            "|id |scores                     |extra                           |\n",
            "+---+---------------------------+--------------------------------+\n",
            "|1  |{english -> 90, math -> 80}|{physics -> 88, chemistry -> 77}|\n",
            "|2  |{science -> 95, math -> 85}|{physics -> 88, chemistry -> 77}|\n",
            "|3  |NULL                       |{physics -> 88, chemistry -> 77}|\n",
            "+---+---------------------------+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways\n",
        "\n",
        "MapType is like a dictionary in PySpark DataFrames.\n",
        "\n",
        "Keys are always non-null, values can be null.\n",
        "\n",
        "Use col[\"mapField\"][\"key\"] to extract values.\n",
        "\n",
        "Use create_map() to create new map columns.\n"
      ],
      "metadata": {
        "id": "VubYMWnRZufz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pm9LYd0UDdhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"emp.csv\")"
      ],
      "metadata": {
        "id": "rELLni6o-rg2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "MK8o0SVh-rKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5402686-b2ba-4abe-c4f4-1751e1411852"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bonus(salary):\n",
        "  return int(salary) * 0.1"
      ],
      "metadata": {
        "id": "zRhNtT2O-q0B"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "bonus_udf = udf(bonus)\n",
        "spark.udf.register(\"bonus_sql_udf\", bonus, \"double\")"
      ],
      "metadata": {
        "id": "2hvmqa5JA9Rq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "c35b2de0-b984-4dac-cdcf-297bce847e3e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.bonus(salary)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>bonus</b><br/>def bonus(salary)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-2677044005.py</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"bonus\", bonus_udf(col(\"salary\"))).show()"
      ],
      "metadata": {
        "id": "yhCIC_QtBNZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027c1525-ccd9-4451-8251-e3f68b63af3b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date| bonus|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|5000.0|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|4500.0|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|5500.0|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|4800.0|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|6000.0|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|5200.0|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|7000.0|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|5100.0|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|5800.0|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|4700.0|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|6500.0|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|5400.0|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|7500.0|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|4600.0|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|6300.0|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|4900.0|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|5700.0|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|5000.0|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|6200.0|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|5300.0|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"bonus\", col(\"salary\")*1.1).show()"
      ],
      "metadata": {
        "id": "roxaqV4iBWo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140343d1-ffb4-4fd7-80c6-629cca615c3b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+-----------------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|            bonus|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|55000.00000000001|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|49500.00000000001|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|60500.00000000001|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|52800.00000000001|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|          66000.0|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|57200.00000000001|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|          77000.0|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|56100.00000000001|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|63800.00000000001|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|51700.00000000001|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|          71500.0|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|59400.00000000001|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|          82500.0|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|50600.00000000001|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|          69300.0|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|53900.00000000001|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|62700.00000000001|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|55000.00000000001|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|          68200.0|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|58300.00000000001|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using a UDF with transform()\n",
        "\n",
        "# Define a function that uses the UDF\n",
        "def add_bonus_udf_transform(dataframe):\n",
        "    return dataframe.withColumn(\"bonus_from_udf\", bonus_udf(col(\"salary\")))\n",
        "\n",
        "# Apply the transformation using transform()\n",
        "df_with_udf_transform = df.transform(add_bonus_udf_transform)\n",
        "\n",
        "df_with_udf_transform.show()"
      ],
      "metadata": {
        "id": "fS-pV1NfCkWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab7705a-8264-4d78-82db-c1f96a78e6a3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+--------------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|bonus_from_udf|\n",
            "+-----------+-------------+-------------+---+------+------+----------+--------------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|        5000.0|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|        4500.0|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|        5500.0|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|        4800.0|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|        6000.0|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|        5200.0|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|        7000.0|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|        5100.0|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|        5800.0|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|        4700.0|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|        6500.0|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|        5400.0|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|        7500.0|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|        4600.0|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|        6300.0|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|        4900.0|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|        5700.0|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|        5000.0|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|        6200.0|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|        5300.0|\n",
            "+-----------+-------------+-------------+---+------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a93ed198"
      },
      "source": [
        "Advantages of using UDFs over simple Python functions in PySpark:\n",
        "\n",
        "*   **Integration with Spark SQL:** Registered UDFs can be directly used in Spark SQL queries, making your logic accessible from both DataFrame API and SQL.\n",
        "*   **Serialization and Distribution:** Spark handles the serialization and distribution of UDFs to worker nodes, allowing your custom logic to be executed in a distributed manner.\n",
        "*   **Performance (Pandas UDFs):** Pandas UDFs (vectorized UDFs) can offer significant performance improvements for certain operations by leveraging Apache Arrow and processing data in batches within Pandas DataFrames.\n",
        "*   **Reusability:** Once defined and registered, a UDF can be easily reused across different parts of your Spark application.\n",
        "*   **Handling Complex Logic:** UDFs are useful when the required transformation logic is too complex to express using built-in Spark functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0JnMYbROUth"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "002f68e7"
      },
      "source": [
        "## `explode()` in PySpark\n",
        "\n",
        "The `explode()` function is used to create a new row for each element in an array or map column. It essentially transforms a single row with an array/map into multiple rows, with each new row containing one element from the original array/map.\n",
        "\n",
        "This is particularly useful when you have nested data structures (arrays or maps) in your DataFrame and you want to flatten them for further processing or analysis.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f20e6a1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88dc0e9e-5f02-4e6f-9a32-e8f1f6762f00"
      },
      "source": [
        "# Example 1: explode() with an array column\n",
        "\n",
        "from pyspark.sql.functions import explode, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame with an array column\n",
        "data = [\n",
        "    (\"Alice\", [\"Math\", \"Science\"]),\n",
        "    (\"Bob\", [\"History\"]),\n",
        "    (\"Charlie\", []), # Empty array\n",
        "    (\"David\", None) # Null array\n",
        "]\n",
        "columns = [\"name\", \"subjects\"]\n",
        "df_array = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_array.show(truncate=False)\n",
        "\n",
        "# Use explode() on the 'subjects' array column\n",
        "df_exploded_array = df_array.select(col(\"name\"), explode(col(\"subjects\")).alias(\"subject\"))\n",
        "\n",
        "print(\"DataFrame after explode() on array column:\")\n",
        "df_exploded_array.show(truncate=False)\n",
        "\n",
        "# Note: Rows with empty or null arrays are dropped by default."
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---------------+\n",
            "|name   |subjects       |\n",
            "+-------+---------------+\n",
            "|Alice  |[Math, Science]|\n",
            "|Bob    |[History]      |\n",
            "|Charlie|[]             |\n",
            "|David  |NULL           |\n",
            "+-------+---------------+\n",
            "\n",
            "DataFrame after explode() on array column:\n",
            "+-----+-------+\n",
            "|name |subject|\n",
            "+-----+-------+\n",
            "|Alice|Math   |\n",
            "|Alice|Science|\n",
            "|Bob  |History|\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d603b8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac811bcd-4751-4ba5-83fc-17c70b96b279"
      },
      "source": [
        "# Example 2: explode() with a map column\n",
        "\n",
        "from pyspark.sql.functions import explode, col, create_map, lit\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField\n",
        "\n",
        "# Sample DataFrame with a map column\n",
        "data = [\n",
        "    (\"Alice\", {\"Math\": 90, \"Science\": 85}),\n",
        "    (\"Bob\", {\"History\": 75}),\n",
        "    (\"Charlie\", {}), # Empty map\n",
        "    (\"David\", None) # Null map\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"scores\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "df_map = spark.createDataFrame(data, schema)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_map.show(truncate=False)\n",
        "\n",
        "# Use explode() on the 'scores' map column\n",
        "# explode() on a map results in two columns: 'key' and 'value'\n",
        "df_exploded_map = df_map.select(col(\"name\"), explode(col(\"scores\")))\n",
        "\n",
        "print(\"DataFrame after explode() on map column:\")\n",
        "df_exploded_map.show(truncate=False)\n",
        "\n",
        "# You can rename the resulting columns\n",
        "df_exploded_map_renamed = df_map.select(col(\"name\"), explode(col(\"scores\")).alias(\"course\", \"score\"))\n",
        "\n",
        "print(\"DataFrame after explode() on map column (renamed columns):\")\n",
        "df_exploded_map_renamed.show(truncate=False)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---------------------------+\n",
            "|name   |scores                     |\n",
            "+-------+---------------------------+\n",
            "|Alice  |{Science -> 85, Math -> 90}|\n",
            "|Bob    |{History -> 75}            |\n",
            "|Charlie|{}                         |\n",
            "|David  |NULL                       |\n",
            "+-------+---------------------------+\n",
            "\n",
            "DataFrame after explode() on map column:\n",
            "+-----+-------+-----+\n",
            "|name |key    |value|\n",
            "+-----+-------+-----+\n",
            "|Alice|Science|85   |\n",
            "|Alice|Math   |90   |\n",
            "|Bob  |History|75   |\n",
            "+-----+-------+-----+\n",
            "\n",
            "DataFrame after explode() on map column (renamed columns):\n",
            "+-----+-------+-----+\n",
            "|name |course |score|\n",
            "+-----+-------+-----+\n",
            "|Alice|Science|85   |\n",
            "|Alice|Math   |90   |\n",
            "|Bob  |History|75   |\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8edb1491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbddbc8f-5fda-4da9-9520-4f9a906bb945"
      },
      "source": [
        "# Example 3: explode_outer()\n",
        "\n",
        "from pyspark.sql.functions import explode_outer, col\n",
        "\n",
        "# Assume df_array is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_array.show(truncate=False)\n",
        "\n",
        "# Use explode_outer() on the 'subjects' array column\n",
        "df_exploded_outer_array = df_array.select(col(\"name\"), explode_outer(col(\"subjects\")).alias(\"subject\"))\n",
        "\n",
        "print(\"DataFrame after explode_outer() on array column:\")\n",
        "df_exploded_outer_array.show(truncate=False)\n",
        "\n",
        "# Assume df_map is already created from Example 2\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_map.show(truncate=False)\n",
        "\n",
        "# Use explode_outer() on the 'scores' map column\n",
        "df_exploded_outer_map = df_map.select(col(\"name\"), explode_outer(col(\"scores\")).alias(\"course\", \"score\"))\n",
        "\n",
        "print(\"DataFrame after explode_outer() on map column:\")\n",
        "df_exploded_outer_map.show(truncate=False)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---------------+\n",
            "|name   |subjects       |\n",
            "+-------+---------------+\n",
            "|Alice  |[Math, Science]|\n",
            "|Bob    |[History]      |\n",
            "|Charlie|[]             |\n",
            "|David  |NULL           |\n",
            "+-------+---------------+\n",
            "\n",
            "DataFrame after explode_outer() on array column:\n",
            "+-------+-------+\n",
            "|name   |subject|\n",
            "+-------+-------+\n",
            "|Alice  |Math   |\n",
            "|Alice  |Science|\n",
            "|Bob    |History|\n",
            "|Charlie|NULL   |\n",
            "|David  |NULL   |\n",
            "+-------+-------+\n",
            "\n",
            "Original DataFrame:\n",
            "+-------+---------------------------+\n",
            "|name   |scores                     |\n",
            "+-------+---------------------------+\n",
            "|Alice  |{Science -> 85, Math -> 90}|\n",
            "|Bob    |{History -> 75}            |\n",
            "|Charlie|{}                         |\n",
            "|David  |NULL                       |\n",
            "+-------+---------------------------+\n",
            "\n",
            "DataFrame after explode_outer() on map column:\n",
            "+-------+-------+-----+\n",
            "|name   |course |score|\n",
            "+-------+-------+-----+\n",
            "|Alice  |Science|85   |\n",
            "|Alice  |Math   |90   |\n",
            "|Bob    |History|75   |\n",
            "|Charlie|NULL   |NULL |\n",
            "|David  |NULL   |NULL |\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab01de83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc95ca9-1782-4353-eadd-2a2fc397a22b"
      },
      "source": [
        "df_exploded_array.printSchema()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- subject: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d2c522"
      },
      "source": [
        "**Handling Nulls and Empty Arrays/Maps:**\n",
        "\n",
        "By default, `explode()` drops rows where the array or map column is null or empty.\n",
        "\n",
        "If you want to keep these rows and have nulls in the exploded columns, you can use `explode_outer()`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The explode_outer() function is used on the original DataFrame containing the array (or map) column, not on a DataFrame that has already been exploded.\n",
        "\n",
        "You use explode_outer() in the same way you would use explode(), but it will include rows where the array or map is NULL or empty, resulting in NULL values in the new exploded column(s).\n",
        "\n",
        "I demonstrated this in Example 3 (cell 8edb1491), where explode_outer(col(\"subjects\")) was applied to the original df_array DataFrame."
      ],
      "metadata": {
        "id": "3h805UC5PJc8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4894c9d9"
      },
      "source": [
        "## `create_map()` in PySpark\n",
        "\n",
        "The `create_map()` function in PySpark is used to create a new map column (key-value pairs) from existing columns or literal values. It's a function available in `pyspark.sql.functions`.\n",
        "\n",
        "This function is useful for structuring data into a map format, which can then be used for various operations, including working with `MapType` columns or preparing data for nested structures.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create_map(lit(key1), lit(value1), lit(key2), lit(value2))"
      ],
      "metadata": {
        "id": "0Fa77DecQSFD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5663cba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e019c625-346c-4340-9449-eebc86478d3b"
      },
      "source": [
        "# Example 1: create_map() from existing columns\n",
        "\n",
        "from pyspark.sql.functions import create_map, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CreateMapExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"Alice\", \"Math\", 90, \"Science\", 85),\n",
        "    (\"Bob\", \"History\", 75, \"Art\", 88),\n",
        "    (\"Charlie\", \"Physics\", 92, \"Chemistry\", 80)\n",
        "]\n",
        "columns = [\"name\", \"subject1_name\", \"subject1_score\", \"subject2_name\", \"subject2_score\"]\n",
        "df_subjects = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_subjects.show()\n",
        "\n",
        "# Create a map column from subject name and score pairs\n",
        "df_with_map = df_subjects.withColumn(\"scores_map\",\n",
        "                                     create_map(\n",
        "                                         col(\"subject1_name\"), col(\"subject1_score\"),\n",
        "                                         col(\"subject2_name\"), col(\"subject2_score\")\n",
        "                                     ))\n",
        "\n",
        "print(\"DataFrame after creating a map column:\")\n",
        "df_with_map.show(truncate=False)\n",
        "\n",
        "df_with_map.printSchema()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+-------------+--------------+-------------+--------------+\n",
            "|   name|subject1_name|subject1_score|subject2_name|subject2_score|\n",
            "+-------+-------------+--------------+-------------+--------------+\n",
            "|  Alice|         Math|            90|      Science|            85|\n",
            "|    Bob|      History|            75|          Art|            88|\n",
            "|Charlie|      Physics|            92|    Chemistry|            80|\n",
            "+-------+-------------+--------------+-------------+--------------+\n",
            "\n",
            "DataFrame after creating a map column:\n",
            "+-------+-------------+--------------+-------------+--------------+--------------------------------+\n",
            "|name   |subject1_name|subject1_score|subject2_name|subject2_score|scores_map                      |\n",
            "+-------+-------------+--------------+-------------+--------------+--------------------------------+\n",
            "|Alice  |Math         |90            |Science      |85            |{Math -> 90, Science -> 85}     |\n",
            "|Bob    |History      |75            |Art          |88            |{History -> 75, Art -> 88}      |\n",
            "|Charlie|Physics      |92            |Chemistry    |80            |{Physics -> 92, Chemistry -> 80}|\n",
            "+-------+-------------+--------------+-------------+--------------+--------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- subject1_name: string (nullable = true)\n",
            " |-- subject1_score: long (nullable = true)\n",
            " |-- subject2_name: string (nullable = true)\n",
            " |-- subject2_score: long (nullable = true)\n",
            " |-- scores_map: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: long (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d67d7fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46efbeb2-249b-4e21-f72c-fdd880b46ed0"
      },
      "source": [
        "# Example 2: create_map() from literal values\n",
        "\n",
        "from pyspark.sql.functions import create_map, lit\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Assume spark is already created\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "columns = [\"name\", \"age\"]\n",
        "df_lit = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_lit.show()\n",
        "\n",
        "# Add a map column with fixed literal values\n",
        "df_with_literal_map = df_lit.withColumn(\"info\", create_map(\n",
        "    lit(\"city\"), lit(\"New York\"),\n",
        "    lit(\"country\"), lit(\"USA\")\n",
        "))\n",
        "\n",
        "print(\"DataFrame after adding a map column with literal values:\")\n",
        "df_with_literal_map.show(truncate=False)\n",
        "\n",
        "df_with_literal_map.printSchema()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "+-----+---+\n",
            "\n",
            "DataFrame after adding a map column with literal values:\n",
            "+-----+---+----------------------------------+\n",
            "|name |age|info                              |\n",
            "+-----+---+----------------------------------+\n",
            "|Alice|25 |{city -> New York, country -> USA}|\n",
            "|Bob  |30 |{city -> New York, country -> USA}|\n",
            "+-----+---+----------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- info: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478fc3ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093d92a4-1c09-49b0-f117-c4c0c64b4bd3"
      },
      "source": [
        "# Example 3: create_map() with mixed columns and literals\n",
        "\n",
        "from pyspark.sql.functions import create_map, col, lit\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Assume spark is already created\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 25, \"Engineer\"), (\"Bob\", 30, \"Doctor\")]\n",
        "columns = [\"name\", \"age\", \"occupation\"]\n",
        "df_mixed = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_mixed.show()\n",
        "\n",
        "# Create a map column using a mix of columns and literals\n",
        "df_with_mixed_map = df_mixed.withColumn(\"details\", create_map(\n",
        "    lit(\"age\"), col(\"age\").cast(\"string\"), # Cast age to string for consistency\n",
        "    lit(\"occupation\"), col(\"occupation\"),\n",
        "    lit(\"status\"), lit(\"active\")\n",
        "))\n",
        "\n",
        "print(\"DataFrame after creating a map column with mixed types:\")\n",
        "df_with_mixed_map.show(truncate=False)\n",
        "\n",
        "df_with_mixed_map.printSchema()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-----+---+----------+\n",
            "| name|age|occupation|\n",
            "+-----+---+----------+\n",
            "|Alice| 25|  Engineer|\n",
            "|  Bob| 30|    Doctor|\n",
            "+-----+---+----------+\n",
            "\n",
            "DataFrame after creating a map column with mixed types:\n",
            "+-----+---+----------+-----------------------------------------------------+\n",
            "|name |age|occupation|details                                              |\n",
            "+-----+---+----------+-----------------------------------------------------+\n",
            "|Alice|25 |Engineer  |{age -> 25, occupation -> Engineer, status -> active}|\n",
            "|Bob  |30 |Doctor    |{age -> 30, occupation -> Doctor, status -> active}  |\n",
            "+-----+---+----------+-----------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- occupation: string (nullable = true)\n",
            " |-- details: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "paMl3SvyQXFJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42d38ef"
      },
      "source": [
        "## `map_keys()` and `map_values()` in PySpark\n",
        "\n",
        "`map_keys()` and `map_values()` are PySpark SQL functions used to extract the keys and values, respectively, from a MapType column in a DataFrame.\n",
        "\n",
        "*   **`map_keys(col)`**: Returns an array containing all the keys in the MapType column. The order of keys in the array is not guaranteed.\n",
        "*   **`map_values(col)`**: Returns an array containing all the values in the MapType column. The order of values in the array corresponds to the order of keys returned by `map_keys()`.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8de9351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d2557b-8ac4-4360-c5ae-f90d7eec8356"
      },
      "source": [
        "from pyspark.sql.functions import map_keys, map_values, col\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MapKeysValuesExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame with a MapType column (using the df_map from a previous example)\n",
        "data = [\n",
        "    (\"Alice\", {\"Math\": 90, \"Science\": 85}),\n",
        "    (\"Bob\", {\"History\": 75}),\n",
        "    (\"Charlie\", {}), # Empty map\n",
        "    (\"David\", None) # Null map\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"scores\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "df_map = spark.createDataFrame(data, schema)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_map.show(truncate=False)\n",
        "df_map.printSchema()\n",
        "\n",
        "# Example 1: Using map_keys()\n",
        "df_keys = df_map.select(col(\"name\"), map_keys(col(\"scores\")).alias(\"score_keys\"))\n",
        "\n",
        "print(\"DataFrame with score keys:\")\n",
        "df_keys.show(truncate=False)\n",
        "df_keys.printSchema()\n",
        "\n",
        "# Example 2: Using map_values()\n",
        "df_values = df_map.select(col(\"name\"), map_values(col(\"scores\")).alias(\"score_values\"))\n",
        "\n",
        "print(\"DataFrame with score values:\")\n",
        "df_values.show(truncate=False)\n",
        "df_values.printSchema()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---------------------------+\n",
            "|name   |scores                     |\n",
            "+-------+---------------------------+\n",
            "|Alice  |{Science -> 85, Math -> 90}|\n",
            "|Bob    |{History -> 75}            |\n",
            "|Charlie|{}                         |\n",
            "|David  |NULL                       |\n",
            "+-------+---------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- scores: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n",
            "DataFrame with score keys:\n",
            "+-------+---------------+\n",
            "|name   |score_keys     |\n",
            "+-------+---------------+\n",
            "|Alice  |[Science, Math]|\n",
            "|Bob    |[History]      |\n",
            "|Charlie|[]             |\n",
            "|David  |NULL           |\n",
            "+-------+---------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- score_keys: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "DataFrame with score values:\n",
            "+-------+------------+\n",
            "|name   |score_values|\n",
            "+-------+------------+\n",
            "|Alice  |[85, 90]    |\n",
            "|Bob    |[75]        |\n",
            "|Charlie|[]          |\n",
            "|David  |NULL        |\n",
            "+-------+------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- score_values: array (nullable = true)\n",
            " |    |-- element: integer (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BCgEZIlMQ0f3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65a0d01b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b9cdb6-2d1a-48c9-d309-80fc7940571e"
      },
      "source": [
        "from pyspark.sql.functions import map_keys, col\n",
        "\n",
        "# Assume df_with_literal_map is already created (from the create_map examples)\n",
        "\n",
        "# Use map_keys() on the 'info' column\n",
        "df_literal_map_keys = df_with_literal_map.select(col(\"name\"), map_keys(col(\"info\")).alias(\"info_keys\"))\n",
        "\n",
        "print(\"DataFrame with keys from the literal map:\")\n",
        "df_literal_map_keys.show(truncate=False)\n",
        "df_literal_map_keys.printSchema()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with keys from the literal map:\n",
            "+-----+---------------+\n",
            "|name |info_keys      |\n",
            "+-----+---------------+\n",
            "|Alice|[city, country]|\n",
            "|Bob  |[city, country]|\n",
            "+-----+---------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- info_keys: array (nullable = false)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36bbadb5"
      },
      "source": [
        "## `collect_list()` and `collect_set()` in PySpark\n",
        "\n",
        "`collect_list()` and `collect_set()` are aggregation functions in PySpark that are used to gather elements from a column into a list or a set, respectively, within each group. They are often used after a `groupBy()` operation.\n",
        "\n",
        "*   **`collect_list(col)`**: Aggregates the elements of the specified column into a `list`. It includes duplicate values and the order of elements in the list is not guaranteed.\n",
        "*   **`collect_set(col)`**: Aggregates the elements of the specified column into a `set`. It only includes unique values and the order of elements in the set is not guaranteed (as sets are unordered collections).\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce207c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c7387b-1e69-46cc-9a1d-fddd34e1cffd"
      },
      "source": [
        "# Example 1: Basic Usage with groupBy()\n",
        "\n",
        "from pyspark.sql.functions import collect_list, collect_set, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CollectListSetExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"A\", 1),\n",
        "    (\"B\", 2),\n",
        "    (\"A\", 3),\n",
        "    (\"C\", 4),\n",
        "    (\"B\", 2),\n",
        "    (\"A\", 1)\n",
        "]\n",
        "columns = [\"category\", \"value\"]\n",
        "df_agg = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_agg.show()\n",
        "\n",
        "# Group by 'category' and collect values into a list\n",
        "df_list = df_agg.groupBy(\"category\").agg(collect_list(\"value\").alias(\"list_of_values\"))\n",
        "\n",
        "print(\"DataFrame after groupBy() and collect_list():\")\n",
        "df_list.show()\n",
        "\n",
        "# Group by 'category' and collect unique values into a set\n",
        "df_set = df_agg.groupBy(\"category\").agg(collect_set(\"value\").alias(\"set_of_values\"))\n",
        "\n",
        "print(\"DataFrame after groupBy() and collect_set():\")\n",
        "df_set.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+--------+-----+\n",
            "|category|value|\n",
            "+--------+-----+\n",
            "|       A|    1|\n",
            "|       B|    2|\n",
            "|       A|    3|\n",
            "|       C|    4|\n",
            "|       B|    2|\n",
            "|       A|    1|\n",
            "+--------+-----+\n",
            "\n",
            "DataFrame after groupBy() and collect_list():\n",
            "+--------+--------------+\n",
            "|category|list_of_values|\n",
            "+--------+--------------+\n",
            "|       B|        [2, 2]|\n",
            "|       A|     [1, 3, 1]|\n",
            "|       C|           [4]|\n",
            "+--------+--------------+\n",
            "\n",
            "DataFrame after groupBy() and collect_set():\n",
            "+--------+-------------+\n",
            "|category|set_of_values|\n",
            "+--------+-------------+\n",
            "|       B|          [2]|\n",
            "|       A|       [1, 3]|\n",
            "|       C|          [4]|\n",
            "+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7146508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8952a0-9d63-4d31-8d1c-30698be05802"
      },
      "source": [
        "# Example 2: Using collect_list() and collect_set() without groupBy()\n",
        "\n",
        "# When used without groupBy(), these functions will collect all values from the entire DataFrame into a single list or set.\n",
        "df_all_list = df_agg.agg(collect_list(\"value\").alias(\"all_values_list\"))\n",
        "print(\"DataFrame after collect_list() on entire DataFrame:\")\n",
        "df_all_list.show(truncate=False)\n",
        "\n",
        "df_all_set = df_agg.agg(collect_set(\"value\").alias(\"all_values_set\"))\n",
        "print(\"DataFrame after collect_set() on entire DataFrame:\")\n",
        "df_all_set.show(truncate=False)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after collect_list() on entire DataFrame:\n",
            "+------------------+\n",
            "|all_values_list   |\n",
            "+------------------+\n",
            "|[1, 2, 3, 4, 2, 1]|\n",
            "+------------------+\n",
            "\n",
            "DataFrame after collect_set() on entire DataFrame:\n",
            "+--------------+\n",
            "|all_values_set|\n",
            "+--------------+\n",
            "|[1, 2, 3, 4]  |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ab8cc78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16050b7-7966-4ad6-f1d3-e32b07a4e96f"
      },
      "source": [
        "# Example 3: Collecting multiple columns or complex types\n",
        "\n",
        "from pyspark.sql.functions import struct\n",
        "\n",
        "# Collect 'category' and 'value' as structs into a list\n",
        "df_struct_list = df_agg.groupBy(\"category\").agg(collect_list(struct(\"category\", \"value\")).alias(\"list_of_structs\"))\n",
        "print(\"DataFrame after collecting structs:\")\n",
        "df_struct_list.show(truncate=False)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after collecting structs:\n",
            "+--------+------------------------+\n",
            "|category|list_of_structs         |\n",
            "+--------+------------------------+\n",
            "|B       |[{B, 2}, {B, 2}]        |\n",
            "|A       |[{A, 1}, {A, 3}, {A, 1}]|\n",
            "|C       |[{C, 4}]                |\n",
            "+--------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample() and sampleBy() in PySpark"
      ],
      "metadata": {
        "id": "CfTjtay0JiPk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47ceb17"
      },
      "source": [
        "## `sample()` in PySpark\n",
        "\n",
        "`sample()` is used for simple random sampling. It allows you to randomly select a fraction of rows from your DataFrame.\n",
        "\n",
        "You can perform sampling with or without replacement.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e57f454a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1296cbb3-52a4-4746-c744-9b3950c40481"
      },
      "source": [
        "# Example: sample()\n",
        "\n",
        "# Assume df is already created from the previous examples (e.g., the employee dataframe)\n",
        "\n",
        "# Simple random sampling with replacement (sample 30% of data)\n",
        "sampled_df_with_replacement = df.sample(withReplacement=True, fraction=0.3, seed=123)\n",
        "\n",
        "print(\"Sampled DataFrame with Replacement:\")\n",
        "sampled_df_with_replacement.show()\n",
        "\n",
        "# Simple random sampling without replacement (sample 30% of data)\n",
        "sampled_df_without_replacement = df.sample(withReplacement=False, fraction=0.3, seed=123)\n",
        "\n",
        "print(\"Sampled DataFrame without Replacement:\")\n",
        "sampled_df_without_replacement.show()\n",
        "\n",
        "# Note: The exact number of rows in the sampled DataFrame might vary slightly\n",
        "# from fraction * total_rows due to the probabilistic nature of sampling."
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled DataFrame with Replacement:\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "|employee_id|department_id|       name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "|          1|          101|   John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          6|          103|  Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|         10|          104|   Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         12|          105| Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         15|          106|Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         15|          106|Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         17|          105|George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|  Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "\n",
            "Sampled DataFrame without Replacement:\n",
            "+-----------+-------------+---------+---+------+------+----------+\n",
            "|employee_id|department_id|     name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+---------+---+------+------+----------+\n",
            "|          1|          101| John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          5|          103|Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|         18|          104|Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         20|          102|Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+---------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f21e6af"
      },
      "source": [
        "## `sampleBy()` in PySpark\n",
        "\n",
        "`sampleBy()` allows you to perform stratified sampling. This means you can sample different fractions of data from different categories (strata) within a column.\n",
        "\n",
        "It's useful when you have an imbalanced dataset and want to ensure that each category is represented in your sample according to a specified proportion.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8776326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a053a1-0a9b-4dc8-a955-7d0a315fa810"
      },
      "source": [
        "# Example: sampleBy()\n",
        "\n",
        "# Assume df is already created from the previous examples (e.g., the employee dataframe)\n",
        "\n",
        "# Define fractions for stratified sampling by 'gender'\n",
        "# Sample 50% of 'Male' and 100% of 'Female'\n",
        "gender_fractions = {\"Male\": 0.5, \"Female\": 1.0}\n",
        "\n",
        "# Perform stratified sampling\n",
        "sampled_df_by_gender = df.sampleBy(\"gender\", gender_fractions, seed=42)\n",
        "\n",
        "print(\"Sampled DataFrame by Gender:\")\n",
        "sampled_df_by_gender.show()\n",
        "\n",
        "# Example: sampleBy() by 'department_id'\n",
        "# Sample 80% from department 101, 50% from 102, and 100% from 103\n",
        "dept_fractions = {101: 0.8, 102: 0.5, 103: 1.0}\n",
        "\n",
        "# Perform stratified sampling\n",
        "sampled_df_by_dept = df.sampleBy(\"department_id\", dept_fractions, seed=42)\n",
        "\n",
        "print(\"Sampled DataFrame by Department ID:\")\n",
        "sampled_df_by_dept.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled DataFrame by Gender:\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "|employee_id|department_id|       name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "|          2|          101| Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          4|          102|  Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          6|          103|  Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          8|          102|   Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|         10|          104|   Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104| David Park| 38|  Male| 65000|2015-11-01|\n",
            "|         12|          105| Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         13|          106|  Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|         14|          107|  Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         16|          107|Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|  Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|  Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "\n",
            "Sampled DataFrame by Department ID:\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "|employee_id|department_id|       name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "|          1|          101|   John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101| Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          4|          102|  Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|  Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|  Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          8|          102|   Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|    Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         19|          103|Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a70076b"
      },
      "source": [
        "Here are some additional examples to further illustrate `sample()` and `sampleBy()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b216da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de5c7e5-5138-4e79-9d0c-1fe1d9519599"
      },
      "source": [
        "# More Examples for sample()\n",
        "\n",
        "# Sample 50% of data with replacement\n",
        "sampled_df_with_replacement_50 = df.sample(withReplacement=True, fraction=0.5, seed=456)\n",
        "print(\"Sampled DataFrame with Replacement (50%):\")\n",
        "sampled_df_with_replacement_50.show()\n",
        "\n",
        "# Sample 20% of data without replacement\n",
        "sampled_df_without_replacement_20 = df.sample(withReplacement=False, fraction=0.2, seed=789)\n",
        "print(\"Sampled DataFrame without Replacement (20%):\")\n",
        "sampled_df_without_replacement_20.show()\n",
        "\n",
        "# Sample 100% of data without replacement (should return the original DataFrame approximately)\n",
        "sampled_df_without_replacement_100 = df.sample(withReplacement=False, fraction=1.0, seed=1011)\n",
        "print(\"Sampled DataFrame without Replacement (100%):\")\n",
        "sampled_df_without_replacement_100.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled DataFrame with Replacement (50%):\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "|employee_id|department_id|       name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "|          2|          101| Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          2|          101| Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|  Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          6|          103|  Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          6|          103|  Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          9|          103|    Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         12|          105| Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         14|          107|  Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         16|          107|Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         16|          107|Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         16|          107|Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         19|          103|Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "+-----------+-------------+-----------+---+------+------+----------+\n",
            "\n",
            "Sampled DataFrame without Replacement (20%):\n",
            "+-----------+-------------+----------+---+------+------+----------+\n",
            "|employee_id|department_id|      name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+----------+---+------+------+----------+\n",
            "|          2|          101|Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|         10|          104|  Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         13|          106| Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "+-----------+-------------+----------+---+------+------+----------+\n",
            "\n",
            "Sampled DataFrame without Replacement (100%):\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "427500b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b0bc46-cfb2-4f8f-f48d-63e58e4fee8e"
      },
      "source": [
        "# More Examples for sampleBy()\n",
        "\n",
        "# Sample different fractions based on 'age' groups\n",
        "# For simplicity, let's create age groups\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_with_age_group = df.withColumn(\"age_group\",\n",
        "    when(col(\"age\") < 30, \"young\")\n",
        "    .when((col(\"age\") >= 30) & (col(\"age\") < 40), \"middle_aged\")\n",
        "    .otherwise(\"senior\")\n",
        ")\n",
        "\n",
        "print(\"DataFrame with Age Group:\")\n",
        "df_with_age_group.show()\n",
        "\n",
        "# Define fractions for sampling by 'age_group'\n",
        "age_group_fractions = {\"young\": 0.7, \"middle_aged\": 0.4, \"senior\": 1.0}\n",
        "\n",
        "# Perform stratified sampling by 'age_group'\n",
        "sampled_df_by_age_group = df_with_age_group.sampleBy(\"age_group\", age_group_fractions, seed=1213)\n",
        "\n",
        "print(\"Sampled DataFrame by Age Group:\")\n",
        "sampled_df_by_age_group.show()\n",
        "\n",
        "# Another example: sample by 'gender' with different seeds\n",
        "gender_fractions_2 = {\"Male\": 0.6, \"Female\": 0.9}\n",
        "sampled_df_by_gender_2 = df.sampleBy(\"gender\", gender_fractions_2, seed=1415)\n",
        "\n",
        "print(\"Sampled DataFrame by Gender (different seed):\")\n",
        "sampled_df_by_gender_2.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with Age Group:\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|  age_group|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|middle_aged|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|      young|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|middle_aged|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|      young|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|     senior|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|middle_aged|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|     senior|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|      young|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|middle_aged|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|      young|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|middle_aged|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|middle_aged|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|     senior|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|      young|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|middle_aged|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|middle_aged|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|middle_aged|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|      young|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|middle_aged|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|middle_aged|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------+\n",
            "\n",
            "Sampled DataFrame by Age Group:\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|  age_group|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------+\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|      young|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|      young|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|     senior|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|     senior|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|      young|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|middle_aged|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|middle_aged|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|     senior|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|      young|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|middle_aged|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|middle_aged|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|      young|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|middle_aged|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|middle_aged|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-----------+\n",
            "\n",
            "Sampled DataFrame by Gender (different seed):\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot() in PySpark"
      ],
      "metadata": {
        "id": "mjAeacZVKlz0"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3623815f"
      },
      "source": [
        "## `split()` in PySpark\n",
        "\n",
        "The `split()` function in PySpark is used to split a string column into an array of strings based on a specified delimiter. It's a function available in `pyspark.sql.functions`.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5caac601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f74113-9b7b-4a4a-b064-d1760973b8e8"
      },
      "source": [
        "# Example 1: Basic split()\n",
        "\n",
        "from pyspark.sql.functions import split, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SplitExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"apple,banana,orange\",), (\"grape;kiwi\",), (\"mango\",)]\n",
        "columns = [\"fruits\"]\n",
        "df_fruits = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_fruits.show(truncate=False)\n",
        "\n",
        "# Split the 'fruits' column by comma\n",
        "df_split_comma = df_fruits.withColumn(\"fruit_list_comma\", split(col(\"fruits\"), \",\"))\n",
        "\n",
        "print(\"DataFrame after splitting by comma:\")\n",
        "df_split_comma.show(truncate=False)\n",
        "\n",
        "# Split the 'fruits' column by semicolon\n",
        "df_split_semicolon = df_fruits.withColumn(\"fruit_list_semicolon\", split(col(\"fruits\"), \";\"))\n",
        "\n",
        "print(\"DataFrame after splitting by semicolon:\")\n",
        "df_split_semicolon.show(truncate=False)\n",
        "\n",
        "# Split by both comma and semicolon using regex\n",
        "df_split_regex = df_fruits.withColumn(\"fruit_list_regex\", split(col(\"fruits\"), \"[,;]\"))\n",
        "\n",
        "print(\"DataFrame after splitting by comma or semicolon (regex):\")\n",
        "df_split_regex.show(truncate=False)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------------------+\n",
            "|fruits             |\n",
            "+-------------------+\n",
            "|apple,banana,orange|\n",
            "|grape;kiwi         |\n",
            "|mango              |\n",
            "+-------------------+\n",
            "\n",
            "DataFrame after splitting by comma:\n",
            "+-------------------+-----------------------+\n",
            "|fruits             |fruit_list_comma       |\n",
            "+-------------------+-----------------------+\n",
            "|apple,banana,orange|[apple, banana, orange]|\n",
            "|grape;kiwi         |[grape;kiwi]           |\n",
            "|mango              |[mango]                |\n",
            "+-------------------+-----------------------+\n",
            "\n",
            "DataFrame after splitting by semicolon:\n",
            "+-------------------+---------------------+\n",
            "|fruits             |fruit_list_semicolon |\n",
            "+-------------------+---------------------+\n",
            "|apple,banana,orange|[apple,banana,orange]|\n",
            "|grape;kiwi         |[grape, kiwi]        |\n",
            "|mango              |[mango]              |\n",
            "+-------------------+---------------------+\n",
            "\n",
            "DataFrame after splitting by comma or semicolon (regex):\n",
            "+-------------------+-----------------------+\n",
            "|fruits             |fruit_list_regex       |\n",
            "+-------------------+-----------------------+\n",
            "|apple,banana,orange|[apple, banana, orange]|\n",
            "|grape;kiwi         |[grape, kiwi]          |\n",
            "|mango              |[mango]                |\n",
            "+-------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dfb33eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04400db2-dbcb-45f8-9d90-5a3ee17f0870"
      },
      "source": [
        "# Example 2: Using the limit parameter\n",
        "\n",
        "from pyspark.sql.functions import split, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"a_b_c_d_e\",), (\"x_y\",), (\"z\",)]\n",
        "columns = [\"text\"]\n",
        "df_limit = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_limit.show()\n",
        "\n",
        "# Split with limit = 2\n",
        "df_split_limit_2 = df_limit.withColumn(\"split_limit_2\", split(col(\"text\"), \"_\", 2))\n",
        "\n",
        "print(\"DataFrame after splitting with limit = 2:\")\n",
        "df_split_limit_2.show(truncate=False)\n",
        "\n",
        "# Split with limit = 0 (same as -1)\n",
        "df_split_limit_0 = df_limit.withColumn(\"split_limit_0\", split(col(\"text\"), \"_\", 0))\n",
        "\n",
        "print(\"DataFrame after splitting with limit = 0:\")\n",
        "df_split_limit_0.show(truncate=False)\n",
        "\n",
        "# Split with limit = -1 (default)\n",
        "df_split_limit_neg1 = df_limit.withColumn(\"split_limit_neg1\", split(col(\"text\"), \"_\", -1))\n",
        "\n",
        "print(\"DataFrame after splitting with limit = -1:\")\n",
        "df_split_limit_neg1.show(truncate=False)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---------+\n",
            "|     text|\n",
            "+---------+\n",
            "|a_b_c_d_e|\n",
            "|      x_y|\n",
            "|        z|\n",
            "+---------+\n",
            "\n",
            "DataFrame after splitting with limit = 2:\n",
            "+---------+-------------+\n",
            "|text     |split_limit_2|\n",
            "+---------+-------------+\n",
            "|a_b_c_d_e|[a, b_c_d_e] |\n",
            "|x_y      |[x, y]       |\n",
            "|z        |[z]          |\n",
            "+---------+-------------+\n",
            "\n",
            "DataFrame after splitting with limit = 0:\n",
            "+---------+---------------+\n",
            "|text     |split_limit_0  |\n",
            "+---------+---------------+\n",
            "|a_b_c_d_e|[a, b, c, d, e]|\n",
            "|x_y      |[x, y]         |\n",
            "|z        |[z]            |\n",
            "+---------+---------------+\n",
            "\n",
            "DataFrame after splitting with limit = -1:\n",
            "+---------+----------------+\n",
            "|text     |split_limit_neg1|\n",
            "+---------+----------------+\n",
            "|a_b_c_d_e|[a, b, c, d, e] |\n",
            "|x_y      |[x, y]          |\n",
            "|z        |[z]             |\n",
            "+---------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f63d471"
      },
      "source": [
        "## `concat_ws()` in PySpark\n",
        "\n",
        "The `concat_ws()` function (concatenate with separator) is used to concatenate multiple string columns together into a single string column, with a specified separator placed between each concatenated value. It's a function available in `pyspark.sql.functions`.\n",
        "\n",
        "This function is useful for combining information from different columns into a more readable format or preparing data for output.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07f1531d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0aa3af-6ebd-4da8-bb7d-9bb436ab94b1"
      },
      "source": [
        "# Example 1: Basic concat_ws()\n",
        "\n",
        "from pyspark.sql.functions import concat_ws, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ConcatWSExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"John\", \"Doe\", \"USA\"),\n",
        "    (\"Jane\", \"Smith\", \"Canada\"),\n",
        "    (\"Peter\", \"Jones\", \"UK\"),\n",
        "    (None, \"Brown\", \"Germany\"), # Example with a null value\n",
        "    (\"Alice\", None, \"France\")  # Example with a null value\n",
        "]\n",
        "columns = [\"first_name\", \"last_name\", \"country\"]\n",
        "df_names = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_names.show()\n",
        "\n",
        "# Concatenate first_name and last_name with a space\n",
        "df_full_name = df_names.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
        "\n",
        "print(\"DataFrame with full_name:\")\n",
        "df_full_name.show()\n",
        "\n",
        "# Concatenate first_name, last_name, and country with a comma and space\n",
        "df_full_info = df_names.withColumn(\"full_info\", concat_ws(\", \", col(\"first_name\"), col(\"last_name\"), col(\"country\")))\n",
        "\n",
        "print(\"DataFrame with full_info:\")\n",
        "df_full_info.show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+----------+---------+-------+\n",
            "|first_name|last_name|country|\n",
            "+----------+---------+-------+\n",
            "|      John|      Doe|    USA|\n",
            "|      Jane|    Smith| Canada|\n",
            "|     Peter|    Jones|     UK|\n",
            "|      NULL|    Brown|Germany|\n",
            "|     Alice|     NULL| France|\n",
            "+----------+---------+-------+\n",
            "\n",
            "DataFrame with full_name:\n",
            "+----------+---------+-------+-----------+\n",
            "|first_name|last_name|country|  full_name|\n",
            "+----------+---------+-------+-----------+\n",
            "|      John|      Doe|    USA|   John Doe|\n",
            "|      Jane|    Smith| Canada| Jane Smith|\n",
            "|     Peter|    Jones|     UK|Peter Jones|\n",
            "|      NULL|    Brown|Germany|      Brown|\n",
            "|     Alice|     NULL| France|      Alice|\n",
            "+----------+---------+-------+-----------+\n",
            "\n",
            "DataFrame with full_info:\n",
            "+----------+---------+-------+-------------------+\n",
            "|first_name|last_name|country|          full_info|\n",
            "+----------+---------+-------+-------------------+\n",
            "|      John|      Doe|    USA|     John, Doe, USA|\n",
            "|      Jane|    Smith| Canada|Jane, Smith, Canada|\n",
            "|     Peter|    Jones|     UK|   Peter, Jones, UK|\n",
            "|      NULL|    Brown|Germany|     Brown, Germany|\n",
            "|     Alice|     NULL| France|      Alice, France|\n",
            "+----------+---------+-------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a447677"
      },
      "source": [
        "**Handling NULLs:**\n",
        "\n",
        "`concat_ws()` gracefully handles NULL values. If a column value is NULL, it is simply skipped, and the separator is not added for that specific value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "320f2c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba16238-7f54-476e-fe14-7dce53c7f80b"
      },
      "source": [
        "# Example 2: concat_ws() with array column\n",
        "\n",
        "from pyspark.sql.functions import concat_ws, col, array, lit\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Sample DataFrame with an array column\n",
        "data = [\n",
        "    (\"apple\", [\"red\", \"green\"]),\n",
        "    (\"banana\", [\"yellow\"]),\n",
        "    (\"orange\", [\"orange\", \"sweet\", \"citrus\"]),\n",
        "    (\"grape\", []), # Empty array\n",
        "    (\"kiwi\", None) # Null array\n",
        "]\n",
        "columns = [\"fruit\", \"properties\"]\n",
        "df_fruits_props = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_fruits_props.show(truncate=False)\n",
        "\n",
        "# Concatenate elements of the 'properties' array with a hyphen\n",
        "df_props_string = df_fruits_props.withColumn(\"properties_string\", concat_ws(\"-\", col(\"properties\")))\n",
        "\n",
        "print(\"DataFrame with properties_string (concatenated array):\")\n",
        "df_props_string.show(truncate=False)\n",
        "\n",
        "# Concatenate fruit name and properties array elements\n",
        "df_combined = df_fruits_props.withColumn(\"fruit_and_props\", concat_ws(\":\", col(\"fruit\"), concat_ws(\",\", col(\"properties\"))))\n",
        "\n",
        "print(\"DataFrame with fruit_and_props:\")\n",
        "df_combined.show(truncate=False)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+------+-----------------------+\n",
            "|fruit |properties             |\n",
            "+------+-----------------------+\n",
            "|apple |[red, green]           |\n",
            "|banana|[yellow]               |\n",
            "|orange|[orange, sweet, citrus]|\n",
            "|grape |[]                     |\n",
            "|kiwi  |NULL                   |\n",
            "+------+-----------------------+\n",
            "\n",
            "DataFrame with properties_string (concatenated array):\n",
            "+------+-----------------------+-------------------+\n",
            "|fruit |properties             |properties_string  |\n",
            "+------+-----------------------+-------------------+\n",
            "|apple |[red, green]           |red-green          |\n",
            "|banana|[yellow]               |yellow             |\n",
            "|orange|[orange, sweet, citrus]|orange-sweet-citrus|\n",
            "|grape |[]                     |                   |\n",
            "|kiwi  |NULL                   |                   |\n",
            "+------+-----------------------+-------------------+\n",
            "\n",
            "DataFrame with fruit_and_props:\n",
            "+------+-----------------------+--------------------------+\n",
            "|fruit |properties             |fruit_and_props           |\n",
            "+------+-----------------------+--------------------------+\n",
            "|apple |[red, green]           |apple:red,green           |\n",
            "|banana|[yellow]               |banana:yellow             |\n",
            "|orange|[orange, sweet, citrus]|orange:orange,sweet,citrus|\n",
            "|grape |[]                     |grape:                    |\n",
            "|kiwi  |NULL                   |kiwi:                     |\n",
            "+------+-----------------------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1: Basic concat_ws() (cell 07f1531d)\n",
        "\n",
        "This example demonstrates the basic usage of concat_ws() to combine string columns with a specified separator.\n",
        "\n",
        "Original DataFrame: This shows the initial data with first_name, last_name, and country columns, including some rows with NULL values.\n",
        "Concatenate first_name and last_name with a space:\n",
        "df_names.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
        "This line adds a new column named full_name.\n",
        "concat_ws(\" \", ...) is used to concatenate the columns. The first argument \" \" is the separator (a space).\n",
        "col(\"first_name\"), col(\"last_name\") are the columns to be concatenated.\n",
        "The output DataFrame with full_name shows the combined first_name and last_name. Notice how the row with NULL in first_name just shows the last_name (\"Brown\"), and the row with NULL in last_name just shows the first_name (\"Alice\"). concat_ws skips the NULL values and doesn't add the separator for them.\n",
        "Concatenate first_name, last_name, and country with a comma and space:\n",
        "df_names.withColumn(\"full_info\", concat_ws(\", \", col(\"first_name\"), col(\"last_name\"), col(\"country\")))\n",
        "This line adds another new column named full_info.\n",
        "concat_ws(\", \", ...) uses \", \" as the separator.\n",
        "col(\"first_name\"), col(\"last_name\"), col(\"country\") are the columns to concatenate.\n",
        "The output DataFrame with full_info shows the combined information. Again, observe how NULL values are handled â€“ for the row with NULL first name, the output is \"Brown, Germany\", skipping the NULL first name and the separator before it.\n",
        "Example 2: concat_ws() with array column (cell 320f2c24)\n",
        "\n",
        "This example shows how concat_ws() can be used with an array column.\n",
        "\n",
        "Original DataFrame: This DataFrame has a fruit column and a properties column, which is an array of strings. It includes examples with multiple elements, a single element, an empty array, and a NULL array.\n",
        "Concatenate elements of the 'properties' array with a hyphen:\n",
        "df_fruits_props.withColumn(\"properties_string\", concat_ws(\"-\", col(\"properties\")))\n",
        "This adds a new column properties_string.\n",
        "concat_ws(\"-\", col(\"properties\")) concatenates the elements within the properties array using a hyphen - as the separator.\n",
        "The output DataFrame with properties_string (concatenated array) shows the array elements joined by hyphens. Note that the empty array results in an empty string, and the NULL array also results in a NULL value in the new column.\n",
        "Concatenate fruit name and properties array elements:\n",
        "df_combined = df_fruits_props.withColumn(\"fruit_and_props\", concat_ws(\":\", col(\"fruit\"), concat_ws(\",\", col(\"properties\"))))\n",
        "This is a more complex example where concat_ws() is nested.\n",
        "The outer concat_ws(\":\", ...) concatenates the fruit column and the result of the inner concat_ws(). The separator for the outer concatenation is :.\n",
        "The inner concat_ws(\",\", col(\"properties\")) concatenates the elements within the properties array using a comma , as the separator (as seen in the previous step).\n",
        "The output DataFrame with fruit_and_props shows the fruit name, followed by a colon, and then the properties from the array joined by commas. Rows with empty or NULL arrays for properties still have the fruit name and the colon, followed by nothing.\n",
        "In summary, concat_ws() is a versatile function for combining string columns or elements within a string array, providing control over the separator and handling NULL values gracefully.\n",
        "\n",
        "Let me know if you'd like a deeper dive into any specific part of these examples!\n",
        "\n"
      ],
      "metadata": {
        "id": "vwTx368CM7f6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55b36bad"
      },
      "source": [
        "## `translate()` in PySpark\n",
        "\n",
        "The `translate()` function in PySpark is used to replace a sequence of characters in a string column with another sequence of characters. It performs a character-by-character replacement.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7893e8a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5168d942-4d12-4338-adb5-db0f5adde085"
      },
      "source": [
        "# Example 1: Basic translate()\n",
        "\n",
        "from pyspark.sql.functions import translate, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TranslateExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"abcdefg\",),\n",
        "    (\"12345\",),\n",
        "    (\"hello world\",),\n",
        "    (\"PySpark\",),\n",
        "    (None,) # Example with a null value\n",
        "]\n",
        "columns = [\"text\"]\n",
        "df_text = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Replace 'abc' with 'xyz'\n",
        "# 'a' is replaced by 'x', 'b' by 'y', 'c' by 'z'\n",
        "df_translated_basic = df_text.withColumn(\"translated_text\", translate(col(\"text\"), \"abc\", \"xyz\"))\n",
        "\n",
        "print(\"DataFrame after translate(col('text'), 'abc', 'xyz'):\")\n",
        "df_translated_basic.show()\n",
        "\n",
        "# Replace digits with asterisks\n",
        "df_translated_digits = df_text.withColumn(\"translated_digits\", translate(col(\"text\"), \"0123456789\", \"**********\"))\n",
        "\n",
        "print(\"DataFrame after translate(col('text'), '0123456789', '**********'):\")\n",
        "df_translated_digits.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-----------+\n",
            "|       text|\n",
            "+-----------+\n",
            "|    abcdefg|\n",
            "|      12345|\n",
            "|hello world|\n",
            "|    PySpark|\n",
            "|       NULL|\n",
            "+-----------+\n",
            "\n",
            "DataFrame after translate(col('text'), 'abc', 'xyz'):\n",
            "+-----------+---------------+\n",
            "|       text|translated_text|\n",
            "+-----------+---------------+\n",
            "|    abcdefg|        xyzdefg|\n",
            "|      12345|          12345|\n",
            "|hello world|    hello world|\n",
            "|    PySpark|        PySpxrk|\n",
            "|       NULL|           NULL|\n",
            "+-----------+---------------+\n",
            "\n",
            "DataFrame after translate(col('text'), '0123456789', '**********'):\n",
            "+-----------+-----------------+\n",
            "|       text|translated_digits|\n",
            "+-----------+-----------------+\n",
            "|    abcdefg|          abcdefg|\n",
            "|      12345|            *****|\n",
            "|hello world|      hello world|\n",
            "|    PySpark|          PySpark|\n",
            "|       NULL|             NULL|\n",
            "+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb0a226d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f87f638-c30a-4be3-e7f9-4c64e0511629"
      },
      "source": [
        "# Example 2: Unequal lengths of 'from' and 'to' characters\n",
        "\n",
        "from pyspark.sql.functions import translate, col\n",
        "\n",
        "# Assume df_text is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Replace 'aeiou' with '123'\n",
        "# 'a' -> '1', 'e' -> '2', 'i' -> '3'. 'o' and 'u' are removed.\n",
        "df_translated_unequal = df_text.withColumn(\"translated_unequal\", translate(col(\"text\"), \"aeiou\", \"123\"))\n",
        "\n",
        "print(\"DataFrame after translate(col('text'), 'aeiou', '123'):\")\n",
        "df_translated_unequal.show()\n",
        "\n",
        "# Replace 'xyz' with '12345'\n",
        "# 'x' -> '1', 'y' -> '2', 'z' -> '3'. No characters in 'to' for '4' and '5'.\n",
        "df_translated_unequal_2 = df_text.withColumn(\"translated_unequal_2\", translate(col(\"text\"), \"xyz\", \"12345\"))\n",
        "\n",
        "print(\"DataFrame after translate(col('text'), 'xyz', '12345'):\")\n",
        "df_translated_unequal_2.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-----------+\n",
            "|       text|\n",
            "+-----------+\n",
            "|    abcdefg|\n",
            "|      12345|\n",
            "|hello world|\n",
            "|    PySpark|\n",
            "|       NULL|\n",
            "+-----------+\n",
            "\n",
            "DataFrame after translate(col('text'), 'aeiou', '123'):\n",
            "+-----------+------------------+\n",
            "|       text|translated_unequal|\n",
            "+-----------+------------------+\n",
            "|    abcdefg|           1bcd2fg|\n",
            "|      12345|             12345|\n",
            "|hello world|         h2ll wrld|\n",
            "|    PySpark|           PySp1rk|\n",
            "|       NULL|              NULL|\n",
            "+-----------+------------------+\n",
            "\n",
            "DataFrame after translate(col('text'), 'xyz', '12345'):\n",
            "+-----------+--------------------+\n",
            "|       text|translated_unequal_2|\n",
            "+-----------+--------------------+\n",
            "|    abcdefg|             abcdefg|\n",
            "|      12345|               12345|\n",
            "|hello world|         hello world|\n",
            "|    PySpark|             P2Spark|\n",
            "|       NULL|                NULL|\n",
            "+-----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe982f86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fe56df-b98e-4a06-8e8e-c4a7a346e712"
      },
      "source": [
        "# Example 3: Removing characters\n",
        "\n",
        "from pyspark.sql.functions import translate, col\n",
        "\n",
        "# Assume df_text is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Remove all vowels\n",
        "# 'from' contains vowels, 'to' is an empty string\n",
        "df_translated_remove_vowels = df_text.withColumn(\"no_vowels\", translate(col(\"text\"), \"aeiouAEIOU\", \"\"))\n",
        "\n",
        "print(\"DataFrame after removing vowels:\")\n",
        "df_translated_remove_vowels.show()\n",
        "\n",
        "# Remove spaces and commas\n",
        "df_translated_remove_chars = df_text.withColumn(\"no_spaces_commas\", translate(col(\"text\"), \" ,\", \"\"))\n",
        "\n",
        "print(\"DataFrame after removing spaces and commas:\")\n",
        "df_translated_remove_chars.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-----------+\n",
            "|       text|\n",
            "+-----------+\n",
            "|    abcdefg|\n",
            "|      12345|\n",
            "|hello world|\n",
            "|    PySpark|\n",
            "|       NULL|\n",
            "+-----------+\n",
            "\n",
            "DataFrame after removing vowels:\n",
            "+-----------+---------+\n",
            "|       text|no_vowels|\n",
            "+-----------+---------+\n",
            "|    abcdefg|    bcdfg|\n",
            "|      12345|    12345|\n",
            "|hello world| hll wrld|\n",
            "|    PySpark|   PySprk|\n",
            "|       NULL|     NULL|\n",
            "+-----------+---------+\n",
            "\n",
            "DataFrame after removing spaces and commas:\n",
            "+-----------+----------------+\n",
            "|       text|no_spaces_commas|\n",
            "+-----------+----------------+\n",
            "|    abcdefg|         abcdefg|\n",
            "|      12345|           12345|\n",
            "|hello world|      helloworld|\n",
            "|    PySpark|         PySpark|\n",
            "|       NULL|            NULL|\n",
            "+-----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark's translate() function, when the length of the from string and the to string are unequal, the translation is still done character by character based on the position in the strings.\n",
        "\n",
        "If the from string is longer than the to string, the characters in the from string that do not have a corresponding character at the same position in the to string are removed from the input string.\n",
        "If the to string is longer than the from string, the extra characters in the to string are ignored.\n",
        "You can see this in Example 2 (cell fb0a226d). When translating 'aeiou' to '123', 'a' becomes '1', 'e' becomes '2', 'i' becomes '3', but 'o' and 'u' are removed because there are no 4th and 5th characters in the '123' string."
      ],
      "metadata": {
        "id": "phSVi7rkNrcB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1965b2c6"
      },
      "source": [
        "## `substring()` in PySpark\n",
        "\n",
        "The `substring()` function in PySpark is used to extract a substring from a string column. It takes the starting position and the length of the substring to extract.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "423a5148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00e660d6-4f66-45a5-d679-4518c5b54f3c"
      },
      "source": [
        "display(df_substring_basic)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[text: string, substring_example: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ac1bc69"
      },
      "source": [
        "display(df_translated_basic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74984f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24aca8cb-d648-4da2-c622-6832e3364aae"
      },
      "source": [
        "# Example 1: Basic substring()\n",
        "\n",
        "from pyspark.sql.functions import substring, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SubstringExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"abcdefg\",),\n",
        "    (\"PySpark\",),\n",
        "    (\"Data Science\",),\n",
        "    (None,) # Example with a null value\n",
        "]\n",
        "columns = [\"text\"]\n",
        "df_text = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Extract substring starting from position 3 with length 4\n",
        "df_substring_basic = df_text.withColumn(\"substring_example\", substring(col(\"text\"), 3, 4))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), 3, 4):\")\n",
        "df_substring_basic.show()\n",
        "\n",
        "# Extract substring from the beginning (position 1) with length 3\n",
        "df_substring_start = df_text.withColumn(\"substring_from_start\", substring(col(\"text\"), 1, 3))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), 1, 3):\")\n",
        "df_substring_start.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+------------+\n",
            "|        text|\n",
            "+------------+\n",
            "|     abcdefg|\n",
            "|     PySpark|\n",
            "|Data Science|\n",
            "|        NULL|\n",
            "+------------+\n",
            "\n",
            "DataFrame after substring(col('text'), 3, 4):\n",
            "+------------+-----------------+\n",
            "|        text|substring_example|\n",
            "+------------+-----------------+\n",
            "|     abcdefg|             cdef|\n",
            "|     PySpark|             Spar|\n",
            "|Data Science|             ta S|\n",
            "|        NULL|             NULL|\n",
            "+------------+-----------------+\n",
            "\n",
            "DataFrame after substring(col('text'), 1, 3):\n",
            "+------------+--------------------+\n",
            "|        text|substring_from_start|\n",
            "+------------+--------------------+\n",
            "|     abcdefg|                 abc|\n",
            "|     PySpark|                 PyS|\n",
            "|Data Science|                 Dat|\n",
            "|        NULL|                NULL|\n",
            "+------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72a8b52b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6baae0ed-ff89-4bbc-dbac-6cd120f4e7e0"
      },
      "source": [
        "# Example 2: Using negative position\n",
        "\n",
        "from pyspark.sql.functions import substring, col\n",
        "\n",
        "# Assume df_text is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Extract substring starting from 3 characters from the end with length 3\n",
        "df_substring_negative_pos = df_text.withColumn(\"substring_negative\", substring(col(\"text\"), -3, 3))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), -3, 3):\")\n",
        "df_substring_negative_pos.show()\n",
        "\n",
        "# Extract substring starting from 5 characters from the end with length 2\n",
        "df_substring_negative_pos_2 = df_text.withColumn(\"substring_negative_2\", substring(col(\"text\"), -5, 2))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), -5, 2):\")\n",
        "df_substring_negative_pos_2.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+------------+\n",
            "|        text|\n",
            "+------------+\n",
            "|     abcdefg|\n",
            "|     PySpark|\n",
            "|Data Science|\n",
            "|        NULL|\n",
            "+------------+\n",
            "\n",
            "DataFrame after substring(col('text'), -3, 3):\n",
            "+------------+------------------+\n",
            "|        text|substring_negative|\n",
            "+------------+------------------+\n",
            "|     abcdefg|               efg|\n",
            "|     PySpark|               ark|\n",
            "|Data Science|               nce|\n",
            "|        NULL|              NULL|\n",
            "+------------+------------------+\n",
            "\n",
            "DataFrame after substring(col('text'), -5, 2):\n",
            "+------------+--------------------+\n",
            "|        text|substring_negative_2|\n",
            "+------------+--------------------+\n",
            "|     abcdefg|                  cd|\n",
            "|     PySpark|                  Sp|\n",
            "|Data Science|                  ie|\n",
            "|        NULL|                NULL|\n",
            "+------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1743b8c4"
      },
      "source": [
        "# Example 3: Handling lengths longer than the remaining string\n",
        "\n",
        "from pyspark.sql.functions import substring, col\n",
        "\n",
        "# Assume df_text is already created from Example 1\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_text.show()\n",
        "\n",
        "# Extract substring starting from position 5 with a length longer than remaining\n",
        "df_substring_long_len = df_text.withColumn(\"substring_long\", substring(col(\"text\"), 5, 10))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), 5, 10):\")\n",
        "df_substring_long_len.show()\n",
        "\n",
        "# Extract substring starting from a position beyond the string length\n",
        "df_substring_invalid_pos = df_text.withColumn(\"substring_invalid\", substring(col(\"text\"), 10, 3))\n",
        "\n",
        "print(\"DataFrame after substring(col('text'), 10, 3):\")\n",
        "df_substring_invalid_pos.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aafeda44"
      },
      "source": [
        "## Regular Expression Methods in PySpark\n",
        "\n",
        "PySpark provides functions in `pyspark.sql.functions` for working with regular expressions on string columns. Two common ones are `regexp_extract()` and `regexp_replace()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156ddecf"
      },
      "source": [
        "## `regexp_extract()`\n",
        "\n",
        "`regexp_extract()` is used to extract a specific part of a string that matches a regular expression pattern.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76398ef7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1596e187-e6f4-488e-af66-9cd71bfb2ce3"
      },
      "source": [
        "# Example 1: regexp_extract()\n",
        "\n",
        "from pyspark.sql.functions import regexp_extract, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RegexExample\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [\n",
        "    (\"user_123_abc\",),\n",
        "    (\"another_user_456_xyz\",),\n",
        "    (\"id_789\",),\n",
        "    (\"no_match\",)\n",
        "]\n",
        "columns = [\"text\"]\n",
        "df_regex = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_regex.show()\n",
        "\n",
        "# Extract the numbers after \"user_\"\n",
        "# Pattern: \"user_\" followed by one or more digits (\\d+)\n",
        "# Group 1: the digits captured by (\\d+)\n",
        "df_extracted = df_regex.withColumn(\"extracted_number\", regexp_extract(col(\"text\"), r\"user_(\\d+)\", 1))\n",
        "\n",
        "print(\"DataFrame after extracting numbers:\")\n",
        "df_extracted.show()\n",
        "\n",
        "# Extract text after \"user_\"\n",
        "# Pattern: \"user_\" followed by anything (.*)\n",
        "# Group 1: the text captured by (.*)\n",
        "df_extracted_text = df_regex.withColumn(\"extracted_text\", regexp_extract(col(\"text\"), r\"user_(.*)\", 1))\n",
        "\n",
        "print(\"DataFrame after extracting text:\")\n",
        "df_extracted_text.show()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|        user_123_abc|\n",
            "|another_user_456_xyz|\n",
            "|              id_789|\n",
            "|            no_match|\n",
            "+--------------------+\n",
            "\n",
            "DataFrame after extracting numbers:\n",
            "+--------------------+----------------+\n",
            "|                text|extracted_number|\n",
            "+--------------------+----------------+\n",
            "|        user_123_abc|             123|\n",
            "|another_user_456_xyz|             456|\n",
            "|              id_789|                |\n",
            "|            no_match|                |\n",
            "+--------------------+----------------+\n",
            "\n",
            "DataFrame after extracting text:\n",
            "+--------------------+--------------+\n",
            "|                text|extracted_text|\n",
            "+--------------------+--------------+\n",
            "|        user_123_abc|       123_abc|\n",
            "|another_user_456_xyz|       456_xyz|\n",
            "|              id_789|              |\n",
            "|            no_match|              |\n",
            "+--------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89548c93"
      },
      "source": [
        "## `regexp_replace()`\n",
        "\n",
        "`regexp_replace()` is used to replace all occurrences of a substring that matches a regular expression pattern with another string.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b6e88b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171438ba-a6f2-4ce1-a4bf-628a211104ea"
      },
      "source": [
        "# Example 2: regexp_replace()\n",
        "\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "# Assume df_regex is already created from the previous example\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_regex.show()\n",
        "\n",
        "# Replace all digits with 'X'\n",
        "df_replaced_digits = df_regex.withColumn(\"replaced_digits\", regexp_replace(col(\"text\"), r\"\\d+\", \"X\"))\n",
        "\n",
        "print(\"DataFrame after replacing digits:\")\n",
        "df_replaced_digits.show()\n",
        "\n",
        "# Replace \"user_\" with \"id_\"\n",
        "df_replaced_user = df_regex.withColumn(\"replaced_user\", regexp_replace(col(\"text\"), \"user_\", \"id_\"))\n",
        "\n",
        "print(\"DataFrame after replacing 'user_':\")\n",
        "df_replaced_user.show()\n",
        "\n",
        "# Remove anything after \"_\"\n",
        "df_removed_after_underscore = df_regex.withColumn(\"removed_after_underscore\", regexp_replace(col(\"text\"), r\"\\_.*\", \"\"))\n",
        "\n",
        "print(\"DataFrame after removing text after underscore:\")\n",
        "df_removed_after_underscore.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|        user_123_abc|\n",
            "|another_user_456_xyz|\n",
            "|              id_789|\n",
            "|            no_match|\n",
            "+--------------------+\n",
            "\n",
            "DataFrame after replacing digits:\n",
            "+--------------------+------------------+\n",
            "|                text|   replaced_digits|\n",
            "+--------------------+------------------+\n",
            "|        user_123_abc|        user_X_abc|\n",
            "|another_user_456_xyz|another_user_X_xyz|\n",
            "|              id_789|              id_X|\n",
            "|            no_match|          no_match|\n",
            "+--------------------+------------------+\n",
            "\n",
            "DataFrame after replacing 'user_':\n",
            "+--------------------+------------------+\n",
            "|                text|     replaced_user|\n",
            "+--------------------+------------------+\n",
            "|        user_123_abc|        id_123_abc|\n",
            "|another_user_456_xyz|another_id_456_xyz|\n",
            "|              id_789|            id_789|\n",
            "|            no_match|          no_match|\n",
            "+--------------------+------------------+\n",
            "\n",
            "DataFrame after removing text after underscore:\n",
            "+--------------------+------------------------+\n",
            "|                text|removed_after_underscore|\n",
            "+--------------------+------------------------+\n",
            "|        user_123_abc|                    user|\n",
            "|another_user_456_xyz|                 another|\n",
            "|              id_789|                      id|\n",
            "|            no_match|                      no|\n",
            "+--------------------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90965fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb89955-1efc-44a0-81fa-895f740b937e"
      },
      "source": [
        "# Example 3: Accessing elements of the resulting array\n",
        "\n",
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Assume df_fruits is already created from Example 1\n",
        "\n",
        "df_split_comma = df_fruits.withColumn(\"fruit_list_comma\", split(col(\"fruits\"), \",\"))\n",
        "\n",
        "# Access the first element (index 0)\n",
        "df_first_fruit = df_split_comma.withColumn(\"first_fruit\", col(\"fruit_list_comma\")[0])\n",
        "\n",
        "print(\"DataFrame with the first fruit:\")\n",
        "df_first_fruit.show(truncate=False)\n",
        "\n",
        "# Access the second element (index 1)\n",
        "df_second_fruit = df_split_comma.withColumn(\"second_fruit\", col(\"fruit_list_comma\")[1])\n",
        "\n",
        "print(\"DataFrame with the second fruit:\")\n",
        "df_second_fruit.show(truncate=False)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with the first fruit:\n",
            "+-------------------+-----------------------+-----------+\n",
            "|fruits             |fruit_list_comma       |first_fruit|\n",
            "+-------------------+-----------------------+-----------+\n",
            "|apple,banana,orange|[apple, banana, orange]|apple      |\n",
            "|grape;kiwi         |[grape;kiwi]           |grape;kiwi |\n",
            "|mango              |[mango]                |mango      |\n",
            "+-------------------+-----------------------+-----------+\n",
            "\n",
            "DataFrame with the second fruit:\n",
            "+-------------------+-----------------------+------------+\n",
            "|fruits             |fruit_list_comma       |second_fruit|\n",
            "+-------------------+-----------------------+------------+\n",
            "|apple,banana,orange|[apple, banana, orange]|banana      |\n",
            "|grape;kiwi         |[grape;kiwi]           |NULL        |\n",
            "|mango              |[mango]                |NULL        |\n",
            "+-------------------+-----------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac4c863"
      },
      "source": [
        "## `pivot()` in PySpark\n",
        "\n",
        "`pivot()` is a transformation used to rotate a table-valued expression by turning the unique values from one column into multiple columns. It's commonly used for data aggregation and reshaping, similar to a pivot table in spreadsheet software.\n",
        "\n",
        "**Syntax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8d7d073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe79877-b8d3-438f-bf52-91e0906fa8ce"
      },
      "source": [
        "# Example 1: Basic pivot()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"USA\", \"ProductA\", 100),\n",
        "    (\"USA\", \"ProductB\", 150),\n",
        "    (\"Canada\", \"ProductA\", 120),\n",
        "    (\"Canada\", \"ProductC\", 200),\n",
        "    (\"USA\", \"ProductB\", 180),\n",
        "    (\"Canada\", \"ProductA\", 130)\n",
        "]\n",
        "\n",
        "columns = [\"country\", \"product\", \"amount\"]\n",
        "\n",
        "df_sales = spark.createDataFrame(data, columns)\n",
        "print(\"Original DataFrame:\")\n",
        "df_sales.show()\n",
        "\n",
        "# Pivot the data to show total amount by country and product\n",
        "pivot_df = df_sales.groupBy(\"country\").pivot(\"product\").sum(\"amount\")\n",
        "\n",
        "print(\"Pivoted DataFrame:\")\n",
        "pivot_df.show()\n",
        "\n",
        "# Note: NULL values appear where a combination of grouping column and pivot column value does not exist in the original data."
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+--------+------+\n",
            "|country| product|amount|\n",
            "+-------+--------+------+\n",
            "|    USA|ProductA|   100|\n",
            "|    USA|ProductB|   150|\n",
            "| Canada|ProductA|   120|\n",
            "| Canada|ProductC|   200|\n",
            "|    USA|ProductB|   180|\n",
            "| Canada|ProductA|   130|\n",
            "+-------+--------+------+\n",
            "\n",
            "Pivoted DataFrame:\n",
            "+-------+--------+--------+--------+\n",
            "|country|ProductA|ProductB|ProductC|\n",
            "+-------+--------+--------+--------+\n",
            "|    USA|     100|     330|    NULL|\n",
            "| Canada|     250|    NULL|     200|\n",
            "+-------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43bcd1d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36620be2-573d-48f5-c263-486c6f39a8be"
      },
      "source": [
        "# Example 2: pivot() with specified values\n",
        "\n",
        "# It's generally recommended to provide a list of values to the pivot function\n",
        "# to avoid collecting all unique values from a large dataset.\n",
        "\n",
        "product_values = [\"ProductA\", \"ProductB\", \"ProductC\"]\n",
        "\n",
        "pivot_df_specified = df_sales.groupBy(\"country\").pivot(\"product\", product_values).sum(\"amount\")\n",
        "\n",
        "print(\"Pivoted DataFrame with specified values:\")\n",
        "pivot_df_specified.show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pivoted DataFrame with specified values:\n",
            "+-------+--------+--------+--------+\n",
            "|country|ProductA|ProductB|ProductC|\n",
            "+-------+--------+--------+--------+\n",
            "|    USA|     100|     330|    NULL|\n",
            "| Canada|     250|    NULL|     200|\n",
            "+-------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca8fc1af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "1f4db15a-4c7e-4d93-cc37-36011d7f2a3d"
      },
      "source": [
        "# Example 3: pivot() with multiple aggregations\n",
        "\n",
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "pivot_df_multi_agg = df_sales.groupBy(\"country\").pivot(\"product\", product_values).agg(sum(\"amount\").alias(\"total_amount\"), count(\"amount\").alias(\"count\"))\n",
        "\n",
        "print(\"Pivoted DataFrame with multiple aggregations:\")\n",
        "pivot_df_multi_agg.show()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3485657122.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpivot_df_multi_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"country\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"product\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pivoted DataFrame with multiple aggregations:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "214ca9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e13778-0521-4da3-c4c7-956d92d96a20"
      },
      "source": [
        "# Example 4: pivot() on a different dataset (using the employee df from earlier)\n",
        "\n",
        "# Pivot the employee data to show average salary by department and gender\n",
        "pivot_employee_df = df.groupBy(\"department_id\").pivot(\"gender\").agg(avg(\"salary\"))\n",
        "\n",
        "print(\"Pivoted Employee DataFrame (Average Salary by Department and Gender):\")\n",
        "pivot_employee_df.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pivoted Employee DataFrame (Average Salary by Department and Gender):\n",
            "+-------------+------------------+-------+\n",
            "|department_id|            Female|   Male|\n",
            "+-------------+------------------+-------+\n",
            "|          101|           45000.0|60000.0|\n",
            "|          103|           52000.0|60000.0|\n",
            "|          107|           47500.0|   NULL|\n",
            "|          102|50666.666666666664|55000.0|\n",
            "|          105|           54000.0|57000.0|\n",
            "|          106|              NULL|69000.0|\n",
            "|          104|           48500.0|65000.0|\n",
            "+-------------+------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "njhKOehmUcVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37fa698c"
      },
      "source": [
        "### Sample Dataset for Practice\n",
        "\n",
        "Here's a sample PySpark DataFrame you can use to practice the methods discussed above. It contains information about orders, including `order_id`, `customer_id`, `product`, `quantity`, `price`, and `order_date`. This dataset includes various data types and potential scenarios for applying transformations and actions like `collect()`, `transform()`, `map()`, `flatMap()`, `foreach()`, `partitionBy()`, `MapType`, `explode()`, `create_map()`, `map_keys()`, `map_values()`, `collect_list()`, `collect_set()`, `sample()`, `sampleBy()`, `split()`, `concat_ws()`, `translate()`, `substring()`, `regexp_extract()`, `regexp_replace()`, and `pivot()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44f160ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8896de62-abfd-41b7-b142-4ec73569ab38"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "from datetime import date\n",
        "\n",
        "# Assume spark is already created\n",
        "# spark = SparkSession.builder.appName(\"PracticeDataset\").getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"product\", StringType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "    StructField(\"order_date\", DateType(), True),\n",
        "    StructField(\"tags\", StringType(), True), # For split() and regex examples\n",
        "    StructField(\"details\", StringType(), True), # For translate(), substring() examples\n",
        "    StructField(\"features\", StringType(), True), # For regexp_extract(), regexp_replace() examples\n",
        "    StructField(\"product_attributes\", MapType(StringType(), StringType()), True), # For MapType, explode, map_keys, map_values\n",
        "    StructField(\"related_products\", StringType(), True) # For collect_list, collect_set\n",
        "])\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, 101, \"Laptop\", 1, 1200.00, date(2023, 1, 15), \"electronics,office\", \"Serial: A1B2C3D4\", \"user_123_laptop\", {\"color\": \"silver\", \"brand\": \"XYZ\"}, \"Keyboard,Mouse,Monitor\"),\n",
        "    (2, 102, \"Mouse\", 2, 25.50, date(2023, 1, 15), \"electronics,accessory\", \"Model: M500\", \"user_456_mouse\", {\"color\": \"black\", \"wireless\": \"true\"}, \"Laptop,Monitor\"),\n",
        "    (3, 101, \"Keyboard\", 1, 75.00, date(2023, 1, 16), \"electronics,accessory\", \"SKU: KB-101\", \"id_789_keyboard\", {\"layout\": \"US\", \"mechanical\": \"false\"}, \"Mouse,Monitor\"),\n",
        "    (4, 103, \"Monitor\", 1, 300.00, date(2023, 1, 16), \"electronics,display\", \"DisplaySize: 27inch\", \"user_123_monitor\", {\"size\": \"27\", \"resolution\": \"1080p\"}, \"Laptop,Keyboard\"),\n",
        "    (5, 102, \"Desk Chair\", 1, 150.00, date(2023, 1, 17), \"furniture,office\", \"Weight: 20kg\", \"user_456_chair\", {\"material\": \"mesh\", \"adjustable\": \"true\"}, \"Desk,Lamp\"),\n",
        "    (6, 104, \"Lamp\", 2, 35.00, date(2023, 1, 17), \"furniture,lighting\", \"BulbType: LED\", \"id_abc_lamp\", None, \"Desk Chair,Desk\"), # Example with None map\n",
        "    (7, 101, \"Laptop\", 1, 1200.00, date(2023, 1, 18), \"electronics,office\", \"Serial: E5F6G7H8\", \"user_123_laptop\", {\"color\": \"silver\", \"brand\": \"XYZ\"}, \"Keyboard,Mouse,Monitor\"), # Duplicate order\n",
        "    (8, 105, \"Notebook\", 5, 3.00, date(2023, 1, 18), \"office,stationery\", \"Pages: 100\", \"user_xyz_notebook\", {}, \"Pen,Pencil\"), # Example with empty map\n",
        "    (9, 103, \"Desk\", 1, 250.00, date(2023, 1, 19), \"furniture,office\", \"Material: Wood\", \"user_789_desk\", {\"size\": \"medium\"}, \"Desk Chair,Lamp\"),\n",
        "    (10, 104, \"Pen\", 10, 1.50, date(2023, 1, 19), \"office,stationery\", \"InkColor: Blue\", \"id_def_pen\", {\"color\": \"blue\"}, \"Notebook,Pencil\"),\n",
        "    (11, 105, \"Pencil\", 12, 0.50, date(2023, 1, 20), \"office,stationery\", \"LeadSize: 0.7mm\", \"user_xyz_pencil\", {\"lead\": \"0.7\"}, \"Notebook,Pen\"),\n",
        "    (12, 106, \"Tablet\", 1, 400.00, date(2023, 1, 20), \"electronics\", \"Model: Tab-Pro\", \"user_abc_tablet\", {\"os\": \"Android\"}, None), # Example with None related_products\n",
        "    (13, 106, \"Protector\", 1, 15.00, date(2023, 1, 20), \"electronics,accessory\", \"Type: Screen\", \"user_abc_protector\", {}, \"\"), # Example with empty related_products\n",
        "    # Added new rows\n",
        "    (14, 101, \"Mouse\", 1, 25.50, date(2023, 1, 21), \"electronics,accessory\", \"Model: M600\", \"user_101_mouse\", {\"color\": \"white\", \"wireless\": \"false\"}, \"Keyboard\"),\n",
        "    (15, 102, \"Laptop\", 1, 1100.00, date(2023, 1, 21), \"electronics,office\", \"Serial: I9J10K11L12\", \"user_102_laptop\", {\"color\": \"black\", \"brand\": \"UVW\"}, \"Mouse,Monitor\"),\n",
        "    (16, 103, \"Keyboard\", 2, 70.00, date(2023, 1, 22), \"electronics,accessory\", \"SKU: KB-202\", \"user_103_keyboard\", {\"layout\": \"UK\", \"mechanical\": \"true\"}, \"Mouse\"),\n",
        "    (17, 104, \"Desk\", 1, 220.00, date(2023, 1, 22), \"furniture,office\", \"Material: Metal\", \"user_104_desk\", {\"size\": \"large\"}, \"Chair\"),\n",
        "    (18, 105, \"Lamp\", 1, 30.00, date(2023, 1, 23), \"furniture,lighting\", \"BulbType: Incandescent\", \"user_105_lamp\", None, \"Desk\"),\n",
        "    (19, 106, \"Notebook\", 3, 2.50, date(2023, 1, 23), \"office,stationery\", \"Pages: 150\", \"user_106_notebook\", {}, \"Pen,Pencil\"),\n",
        "    (20, 101, \"Monitor\", 1, 280.00, date(2023, 1, 24), \"electronics,display\", \"DisplaySize: 24inch\", \"user_101_monitor\", {\"size\": \"24\", \"resolution\": \"1080p\"}, \"Laptop\")\n",
        "]\n",
        "\n",
        "practice_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Show the DataFrame and its schema\n",
        "print(\"Sample Practice DataFrame:\")\n",
        "practice_df.show(truncate=False)\n",
        "practice_df.printSchema()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Practice DataFrame:\n",
            "+--------+-----------+----------+--------+------+----------+---------------------+----------------------+------------------+--------------------------------------+----------------------+\n",
            "|order_id|customer_id|product   |quantity|price |order_date|tags                 |details               |features          |product_attributes                    |related_products      |\n",
            "+--------+-----------+----------+--------+------+----------+---------------------+----------------------+------------------+--------------------------------------+----------------------+\n",
            "|1       |101        |Laptop    |1       |1200.0|2023-01-15|electronics,office   |Serial: A1B2C3D4      |user_123_laptop   |{color -> silver, brand -> XYZ}       |Keyboard,Mouse,Monitor|\n",
            "|2       |102        |Mouse     |2       |25.5  |2023-01-15|electronics,accessory|Model: M500           |user_456_mouse    |{color -> black, wireless -> true}    |Laptop,Monitor        |\n",
            "|3       |101        |Keyboard  |1       |75.0  |2023-01-16|electronics,accessory|SKU: KB-101           |id_789_keyboard   |{layout -> US, mechanical -> false}   |Mouse,Monitor         |\n",
            "|4       |103        |Monitor   |1       |300.0 |2023-01-16|electronics,display  |DisplaySize: 27inch   |user_123_monitor  |{size -> 27, resolution -> 1080p}     |Laptop,Keyboard       |\n",
            "|5       |102        |Desk Chair|1       |150.0 |2023-01-17|furniture,office     |Weight: 20kg          |user_456_chair    |{material -> mesh, adjustable -> true}|Desk,Lamp             |\n",
            "|6       |104        |Lamp      |2       |35.0  |2023-01-17|furniture,lighting   |BulbType: LED         |id_abc_lamp       |NULL                                  |Desk Chair,Desk       |\n",
            "|7       |101        |Laptop    |1       |1200.0|2023-01-18|electronics,office   |Serial: E5F6G7H8      |user_123_laptop   |{color -> silver, brand -> XYZ}       |Keyboard,Mouse,Monitor|\n",
            "|8       |105        |Notebook  |5       |3.0   |2023-01-18|office,stationery    |Pages: 100            |user_xyz_notebook |{}                                    |Pen,Pencil            |\n",
            "|9       |103        |Desk      |1       |250.0 |2023-01-19|furniture,office     |Material: Wood        |user_789_desk     |{size -> medium}                      |Desk Chair,Lamp       |\n",
            "|10      |104        |Pen       |10      |1.5   |2023-01-19|office,stationery    |InkColor: Blue        |id_def_pen        |{color -> blue}                       |Notebook,Pencil       |\n",
            "|11      |105        |Pencil    |12      |0.5   |2023-01-20|office,stationery    |LeadSize: 0.7mm       |user_xyz_pencil   |{lead -> 0.7}                         |Notebook,Pen          |\n",
            "|12      |106        |Tablet    |1       |400.0 |2023-01-20|electronics          |Model: Tab-Pro        |user_abc_tablet   |{os -> Android}                       |NULL                  |\n",
            "|13      |106        |Protector |1       |15.0  |2023-01-20|electronics,accessory|Type: Screen          |user_abc_protector|{}                                    |                      |\n",
            "|14      |101        |Mouse     |1       |25.5  |2023-01-21|electronics,accessory|Model: M600           |user_101_mouse    |{color -> white, wireless -> false}   |Keyboard              |\n",
            "|15      |102        |Laptop    |1       |1100.0|2023-01-21|electronics,office   |Serial: I9J10K11L12   |user_102_laptop   |{color -> black, brand -> UVW}        |Mouse,Monitor         |\n",
            "|16      |103        |Keyboard  |2       |70.0  |2023-01-22|electronics,accessory|SKU: KB-202           |user_103_keyboard |{layout -> UK, mechanical -> true}    |Mouse                 |\n",
            "|17      |104        |Desk      |1       |220.0 |2023-01-22|furniture,office     |Material: Metal       |user_104_desk     |{size -> large}                       |Chair                 |\n",
            "|18      |105        |Lamp      |1       |30.0  |2023-01-23|furniture,lighting   |BulbType: Incandescent|user_105_lamp     |NULL                                  |Desk                  |\n",
            "|19      |106        |Notebook  |3       |2.5   |2023-01-23|office,stationery    |Pages: 150            |user_106_notebook |{}                                    |Pen,Pencil            |\n",
            "|20      |101        |Monitor   |1       |280.0 |2023-01-24|electronics,display  |DisplaySize: 24inch   |user_101_monitor  |{size -> 24, resolution -> 1080p}     |Laptop                |\n",
            "+--------+-----------+----------+--------+------+----------+---------------------+----------------------+------------------+--------------------------------------+----------------------+\n",
            "\n",
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- tags: string (nullable = true)\n",
            " |-- details: string (nullable = true)\n",
            " |-- features: string (nullable = true)\n",
            " |-- product_attributes: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- related_products: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using split() and explode():\n",
        "Extract each individual tag from the tags column and create a new row for each tag, keeping the order_id.\n",
        "Count how many orders belong to each unique tag.\n",
        "\n",
        "Using regexp_extract():\n",
        "From the details column, extract the serial number (e.g., \"A1B2C3D4\") for products where the detail starts with \"Serial: \".\n",
        "From the features column, extract the numeric ID (e.g., \"123\") that appears after \"user_\".\n",
        "\n",
        "Using translate() and substring():\n",
        "Create a new column by removing all vowels (both uppercase and lowercase) from the details column.\n",
        "\n",
        "Extract the first 5 characters of the features column.\n",
        "\n",
        "Using concat_ws():\n",
        "Create a new column that combines the product and quantity columns into a single string like \"Laptop (1)\".\n",
        "\n",
        "Combine the elements of the related_products string into a list using a comma as a separator (you might need split() first).\n",
        "\n",
        "Using MapType, explode(), map_keys(), and map_values():\n",
        "Explode the product_attributes map to have one row per attribute key-value pair, keeping the order_id and product.\n",
        "\n",
        "Get a list of all unique attribute keys present in the product_attributes column across all orders.\n",
        "\n",
        "Extract the value associated with the key \"color\" from the product_attributes map.\n",
        "\n",
        "Using collect_list() and collect_set():\n",
        "For each customer_id, create a list of all products they have ordered (collect_list).\n",
        "\n",
        "For each customer_id, create a set of unique products they have ordered (collect_set).\n",
        "\n",
        "Using pivot():\n",
        "Pivot the data to show the total quantity of each product ordered by each customer_id.\n",
        "\n",
        "Pivot the data to show the average price of each product for each customer_id.\n",
        "\n",
        "Using sample() and sampleBy():\n",
        "Take a random sample of 20% of the rows from the DataFrame.\n",
        "\n",
        "Perform a stratified sample on the product column, taking 50% of \"Laptop\" orders and 100% of \"Mouse\" orders.\n",
        "\n",
        "\n",
        "Combining split(), explode(), and Aggregation:\n",
        "Find the total quantity of products ordered for each unique tag.\n",
        "\n",
        "Using substring() and concat_ws():\n",
        "Create a new column that takes the first 3 characters of the product name and concatenates it with the order_id, separated by a hyphen (e.g., \"Lap-1\").\n",
        "\n",
        "Using regexp_replace() and translate():\n",
        "Remove all non-digit characters from the details column.\n",
        "\n",
        "Replace all occurrences of the letter 'e' (case-insensitive) in the product column with the character '@'.\n",
        "\n",
        "Working with MapType and Filtering:\n",
        "Filter the DataFrame to show only the orders where the product_attributes map contains the key \"color\" and its value is \"silver\".\n",
        "\n",
        "Using collect_list() with struct():\n",
        "For each order_date, collect a list of structs containing the product and quantity for all orders placed on that date.\n",
        "\n",
        "Advanced pivot():\n",
        "Pivot the data to show the total price and total quantity for each product across different customer_ids.\n",
        "\n",
        "Combining sample() and groupBy():\n",
        "Take a random sample of 50% of the data and then group the sampled data by customer_id to find the total number of orders for each customer in the sample.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "32HUe9qIV0cb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iICVkUa3h1FZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}