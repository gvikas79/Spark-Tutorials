{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlQnV/bYq6m7Hq6IAfcAxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvikas79/Spark-Tutorials/blob/main/Spark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LaBYgouO1jMw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5fHGmnwM-n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d0f1944"
      },
      "source": [
        "### Need for Spark SQL\n",
        "\n",
        "Traditionally, data processing was done using various tools and frameworks depending on the data structure (structured, semi-structured, or unstructured). This often led to data silos and complex data pipelines. Spark SQL addresses this by providing a unified API for processing structured and semi-structured data using SQL queries or the DataFrame API. This allows developers to leverage the power of Spark's distributed processing engine while working with data in a familiar tabular format.\n",
        "\n",
        "### What is Spark SQL?\n",
        "\n",
        "Spark SQL is a module in Apache Spark that integrates relational processing with Spark's functional programming API. It provides a high-level abstraction called DataFrames and Datasets, which are distributed collections of data organized into named columns. Spark SQL allows you to query this data using SQL or the DataFrame API, enabling seamless integration with existing SQL-based tools and workflows.\n",
        "\n",
        "### Spark SQL Architecture\n",
        "\n",
        "The architecture of Spark SQL consists of several key components:\n",
        "\n",
        "1.  **SparkSession:** The entry point to Spark SQL, providing a unified interface for interacting with Spark's functionality.\n",
        "2.  **Catalyst Optimizer:** A powerful optimization engine that optimizes queries by applying various rules and techniques, such as predicate pushdown, column pruning, and join optimization.\n",
        "3.  **Tungsten Execution Engine:** An execution engine that optimizes query execution by generating efficient code for various operations.\n",
        "4.  **Data Sources API:** Provides connectors to various data sources, such as Hive, Parquet, JSON, Avro, and databases.\n",
        "5.  **SQL Parser:** Parses SQL queries and converts them into a logical plan.\n",
        "6.  **Logical Plan:** Represents the sequence of operations to be performed on the data.\n",
        "7.  **Physical Plan:** Represents the optimized execution plan for the query, considering factors like data partitioning and available resources.\n",
        "\n",
        "### SQL Context in Spark SQL (Note: Renamed to SparkSession in later versions)\n",
        "\n",
        "In older versions of Spark SQL (before Spark 2.0), `SQLContext` was the entry point for working with structured data. It was responsible for creating DataFrames and executing SQL queries. However, in Spark 2.0 and later, `SparkSession` replaced `SQLContext` and `HiveContext`, providing a unified entry point for all Spark functionality, including Spark Core, Spark SQL, Spark Streaming, and MLlib.\n",
        "\n",
        "### Schema RDDs (Note: Replaced by DataFrames)\n",
        "\n",
        "In earlier versions of Spark SQL, `SchemaRDD` was the primary abstraction for working with structured data. `SchemaRDD` was a RDD of `Row` objects with schema information. However, `SchemaRDD` was less optimized and lacked some features compared to DataFrames. With the introduction of DataFrames, `SchemaRDD` was deprecated and replaced by DataFrames, which provide better performance, a richer API, and improved optimization capabilities.\n",
        "\n",
        "### SQL Functions\n",
        "\n",
        "Spark SQL provides a rich set of built-in SQL functions for performing various operations on data, such as:\n",
        "\n",
        "*   **Aggregate Functions:** `COUNT`, `SUM`, `AVG`, `MIN`, `MAX`, etc.\n",
        "*   **Scalar Functions:** `LOWER`, `UPPER`, `SUBSTRING`, `DATE_FORMAT`, etc.\n",
        "*   **Window Functions:** For performing calculations across a set of table rows that are related to the current row.\n",
        "*   **User-Defined Functions (UDFs):** Allow users to define their own functions to extend the functionality of Spark SQL.\n",
        "\n",
        "These functions can be used in both SQL queries and the DataFrame API to perform complex data transformations and analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanations and Examples of Spark SQL Functions\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQLFunctionsExample\").getOrCreate()\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = [(\"Alice\", 1, \"2023-01-01\", '{\"city\": \"New York\", \"zip\": \"10001\"}'),\n",
        "        (\"Bob\", 2, \"2023-01-01\", '{\"city\": \"Los Angeles\", \"zip\": \"90001\"}'),\n",
        "        (\"Charlie\", 3, \"2023-01-02\", '{\"city\": \"Chicago\", \"zip\": \"60601\"}'),\n",
        "        (\"Alice\", 4, \"2023-01-02\", '{\"city\": \"New York\", \"zip\": \"10001\"}'),\n",
        "        (\"Bob\", 5, \"2023-01-03\", '{\"city\": \"Los Angeles\", \"zip\": \"90001\"}')]\n",
        "columns = [\"Name\", \"ID\", \"Date\", \"JsonString\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "### Aggregate Functions\n",
        "\n",
        "# Aggregate functions perform a calculation on a set of rows and return a single value.\n",
        "\n",
        "print(\"\\n--- Aggregate Functions ---\")\n",
        "\n",
        "# Example: Count the number of rows\n",
        "df.select(F.count(\"*\").alias(\"TotalRows\")).show()\n",
        "\n",
        "# Example: Calculate the average of a column\n",
        "df.select(F.avg(\"ID\").alias(\"AverageID\")).show()\n",
        "\n",
        "# Example: Group by a column and calculate sum\n",
        "df.groupBy(\"Name\").agg(F.sum(\"ID\").alias(\"SumID\")).show()\n",
        "\n",
        "\n",
        "### Window Functions\n",
        "\n",
        "# Window functions perform calculations across a set of table rows that are related to the current row.\n",
        "\n",
        "print(\"\\n--- Window Functions ---\")\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Example: Rank within a partition\n",
        "window_spec = Window.partitionBy(\"Name\").orderBy(\"ID\")\n",
        "df.withColumn(\"Rank\", F.rank().over(window_spec)).show()\n",
        "\n",
        "# Example: Lag function to access data from a previous row\n",
        "window_spec_lag = Window.partitionBy(\"Name\").orderBy(\"Date\")\n",
        "df.withColumn(\"PreviousID\", F.lag(\"ID\", 1).over(window_spec_lag)).show()\n",
        "\n",
        "\n",
        "### Date and Timestamp Functions\n",
        "\n",
        "# Spark SQL provides various functions for working with dates and timestamps.\n",
        "\n",
        "print(\"\\n--- Date and Timestamp Functions ---\")\n",
        "\n",
        "# Example: Convert string to date\n",
        "df.withColumn(\"DateType\", F.to_date(\"Date\")).show()\n",
        "\n",
        "# Example: Get the year from a date\n",
        "df.withColumn(\"Year\", F.year(\"Date\")).show()\n",
        "\n",
        "# Example: Get the month from a date\n",
        "df.withColumn(\"Month\", F.month(\"Date\")).show()\n",
        "\n",
        "\n",
        "### JSON Functions\n",
        "\n",
        "# Spark SQL provides functions for working with JSON data.\n",
        "\n",
        "print(\"\\n--- JSON Functions ---\")\n",
        "\n",
        "# Example: Get a value from a JSON string using get_json_object\n",
        "df.withColumn(\"City\", F.get_json_object(\"JsonString\", \"$.city\")).show()\n",
        "\n",
        "# Example: Convert a column to JSON string using to_json\n",
        "df.select(\"Name\", F.to_json(F.struct(\"Name\", \"ID\")).alias(\"NameID_Json\")).show()\n",
        "\n",
        "# Example: Parse a JSON string into a struct using from_json\n",
        "json_schema = \"struct<city:string, zip:string>\"\n",
        "df.withColumn(\"JsonStruct\", F.from_json(\"JsonString\", json_schema)).show()\n",
        "\n",
        "# Example: Extract values from JSON string using json_tuple\n",
        "df.select(\"Name\", F.json_tuple(\"JsonString\", \"city\", \"zip\").alias(\"City_json_tuple\", \"Zip_json_tuple\")).show()\n",
        "\n",
        "# Example: Get the schema of a JSON string using schema_of_json\n",
        "json_string_example = '{\"name\": \"Alice\", \"age\": 30}'\n",
        "schema = spark.sql(f\"SELECT schema_of_json('{json_string_example}')\").collect()[0][0]\n",
        "print(f\"Schema of JSON string: {schema}\")\n",
        "\n",
        "\n",
        "### Other Useful Functions\n",
        "\n",
        "# Example: Using when() for conditional logic\n",
        "df.withColumn(\"ID_Category\", F.when(df.ID > 2, \"High\").otherwise(\"Low\")).show()\n",
        "\n",
        "# Example: Using expr() to execute SQL-like expressions\n",
        "df.select(\"Name\", F.expr(\"ID * 10\").alias(\"ID_multiplied\")).show()\n",
        "\n",
        "# Example: Using array_contains() with an array column (create an array column first)\n",
        "array_data = [(\"Alice\", [\"apple\", \"banana\"]), (\"Bob\", [\"orange\", \"grape\"])]\n",
        "array_df = spark.createDataFrame(array_data, [\"Name\", \"Fruits\"])\n",
        "array_df.withColumn(\"Has_apple\", F.array_contains(array_df.Fruits, \"apple\")).show()\n",
        "\n",
        "# Example: Using array() to create an array column\n",
        "df.withColumn(\"NameAndID_Array\", F.array(\"Name\", \"ID\")).show()\n",
        "\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLUFYEo8NfMf",
        "outputId": "2c868943-fb8d-4569-9ff1-73479872164d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---+----------+--------------------+\n",
            "|   Name| ID|      Date|          JsonString|\n",
            "+-------+---+----------+--------------------+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|\n",
            "+-------+---+----------+--------------------+\n",
            "\n",
            "\n",
            "--- Aggregate Functions ---\n",
            "+---------+\n",
            "|TotalRows|\n",
            "+---------+\n",
            "|        5|\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|AverageID|\n",
            "+---------+\n",
            "|      3.0|\n",
            "+---------+\n",
            "\n",
            "+-------+-----+\n",
            "|   Name|SumID|\n",
            "+-------+-----+\n",
            "|    Bob|    7|\n",
            "|  Alice|    5|\n",
            "|Charlie|    3|\n",
            "+-------+-----+\n",
            "\n",
            "\n",
            "--- Window Functions ---\n",
            "+-------+---+----------+--------------------+----+\n",
            "|   Name| ID|      Date|          JsonString|Rank|\n",
            "+-------+---+----------+--------------------+----+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|   1|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|   2|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|   1|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|   2|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|   1|\n",
            "+-------+---+----------+--------------------+----+\n",
            "\n",
            "+-------+---+----------+--------------------+----------+\n",
            "|   Name| ID|      Date|          JsonString|PreviousID|\n",
            "+-------+---+----------+--------------------+----------+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|      NULL|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|         1|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|      NULL|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|         2|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|      NULL|\n",
            "+-------+---+----------+--------------------+----------+\n",
            "\n",
            "\n",
            "--- Date and Timestamp Functions ---\n",
            "+-------+---+----------+--------------------+----------+\n",
            "|   Name| ID|      Date|          JsonString|  DateType|\n",
            "+-------+---+----------+--------------------+----------+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|2023-01-01|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|2023-01-01|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|2023-01-02|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|2023-01-02|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|2023-01-03|\n",
            "+-------+---+----------+--------------------+----------+\n",
            "\n",
            "+-------+---+----------+--------------------+----+\n",
            "|   Name| ID|      Date|          JsonString|Year|\n",
            "+-------+---+----------+--------------------+----+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|2023|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|2023|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|2023|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|2023|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|2023|\n",
            "+-------+---+----------+--------------------+----+\n",
            "\n",
            "+-------+---+----------+--------------------+-----+\n",
            "|   Name| ID|      Date|          JsonString|Month|\n",
            "+-------+---+----------+--------------------+-----+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|    1|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|    1|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|    1|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|    1|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|    1|\n",
            "+-------+---+----------+--------------------+-----+\n",
            "\n",
            "\n",
            "--- JSON Functions ---\n",
            "+-------+---+----------+--------------------+-----------+\n",
            "|   Name| ID|      Date|          JsonString|       City|\n",
            "+-------+---+----------+--------------------+-----------+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|   New York|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|Los Angeles|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|    Chicago|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|   New York|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|Los Angeles|\n",
            "+-------+---+----------+--------------------+-----------+\n",
            "\n",
            "+-------+--------------------+\n",
            "|   Name|         NameID_Json|\n",
            "+-------+--------------------+\n",
            "|  Alice|{\"Name\":\"Alice\",\"...|\n",
            "|    Bob|{\"Name\":\"Bob\",\"ID...|\n",
            "|Charlie|{\"Name\":\"Charlie\"...|\n",
            "|  Alice|{\"Name\":\"Alice\",\"...|\n",
            "|    Bob|{\"Name\":\"Bob\",\"ID...|\n",
            "+-------+--------------------+\n",
            "\n",
            "+-------+---+----------+--------------------+--------------------+\n",
            "|   Name| ID|      Date|          JsonString|          JsonStruct|\n",
            "+-------+---+----------+--------------------+--------------------+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|   {New York, 10001}|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|{Los Angeles, 90001}|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|    {Chicago, 60601}|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|   {New York, 10001}|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|{Los Angeles, 90001}|\n",
            "+-------+---+----------+--------------------+--------------------+\n",
            "\n",
            "+-------+---------------+--------------+\n",
            "|   Name|City_json_tuple|Zip_json_tuple|\n",
            "+-------+---------------+--------------+\n",
            "|  Alice|       New York|         10001|\n",
            "|    Bob|    Los Angeles|         90001|\n",
            "|Charlie|        Chicago|         60601|\n",
            "|  Alice|       New York|         10001|\n",
            "|    Bob|    Los Angeles|         90001|\n",
            "+-------+---------------+--------------+\n",
            "\n",
            "Schema of JSON string: STRUCT<age: BIGINT, name: STRING>\n",
            "+-------+---+----------+--------------------+-----------+\n",
            "|   Name| ID|      Date|          JsonString|ID_Category|\n",
            "+-------+---+----------+--------------------+-----------+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|        Low|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|        Low|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|       High|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|       High|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|       High|\n",
            "+-------+---+----------+--------------------+-----------+\n",
            "\n",
            "+-------+-------------+\n",
            "|   Name|ID_multiplied|\n",
            "+-------+-------------+\n",
            "|  Alice|           10|\n",
            "|    Bob|           20|\n",
            "|Charlie|           30|\n",
            "|  Alice|           40|\n",
            "|    Bob|           50|\n",
            "+-------+-------------+\n",
            "\n",
            "+-----+---------------+---------+\n",
            "| Name|         Fruits|Has_apple|\n",
            "+-----+---------------+---------+\n",
            "|Alice|[apple, banana]|     true|\n",
            "|  Bob|[orange, grape]|    false|\n",
            "+-----+---------------+---------+\n",
            "\n",
            "+-------+---+----------+--------------------+---------------+\n",
            "|   Name| ID|      Date|          JsonString|NameAndID_Array|\n",
            "+-------+---+----------+--------------------+---------------+\n",
            "|  Alice|  1|2023-01-01|{\"city\": \"New Yor...|     [Alice, 1]|\n",
            "|    Bob|  2|2023-01-01|{\"city\": \"Los Ang...|       [Bob, 2]|\n",
            "|Charlie|  3|2023-01-02|{\"city\": \"Chicago...|   [Charlie, 3]|\n",
            "|  Alice|  4|2023-01-02|{\"city\": \"New Yor...|     [Alice, 4]|\n",
            "|    Bob|  5|2023-01-03|{\"city\": \"Los Ang...|       [Bob, 5]|\n",
            "+-------+---+----------+--------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBYsV7oiOjjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ee7df43"
      },
      "source": [
        "# Task\n",
        "Explain the following concepts in detail with examples and codes: User Defined Functions, Data Frames & Datasets, Interoperating with RDDs, JSON and Parquet File Formats, Loading Data through Different Sources, Spark-Hive Integration, Transformation mapping using PySpark DataFrame, Advanced Transformations & Actions, and Optimization Techniques in Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f986d68"
      },
      "source": [
        "## Explain user defined functions (udfs)\n",
        "\n",
        "### Subtask:\n",
        "Provide a detailed explanation of UDFs, including how to create and register them, and show examples of using UDFs with Spark DataFrames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b60c9fb"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to explain what UDFs are and why they are useful, then describe how to create and register them in PySpark, including providing code examples for creating and using UDFs with single and multiple input columns. Finally, I will discuss performance considerations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bf82f447",
        "outputId": "30ea1a11-a504-4c0f-d8b9-acf711ff8843"
      },
      "source": [
        "# 1. Explain what User Defined Functions (UDFs) are in the context of Spark SQL and DataFrames, and why they are useful.\n",
        "print(\"--- User Defined Functions (UDFs) in Spark SQL and DataFrames ---\")\n",
        "print(\"\"\"\n",
        "User Defined Functions (UDFs) in Spark SQL and DataFrames allow you to define your own custom functions using Python (or other languages like Scala or Java) and use them within Spark SQL queries or DataFrame transformations. They are particularly useful when the built-in Spark functions are not sufficient for your specific data processing needs. UDFs enable you to encapsulate complex logic or leverage external libraries that are not directly available as Spark functions.\n",
        "\n",
        "Why are UDFs useful?\n",
        "- Extend Spark's functionality: You can implement custom transformations or calculations that are not provided by Spark's built-in functions.\n",
        "- Reusability: Once defined, a UDF can be reused across multiple queries or DataFrame operations.\n",
        "- Integration with Python libraries: You can leverage the vast ecosystem of Python libraries within your Spark jobs.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Describe the process of creating a UDF in PySpark, including defining a Python function and specifying the return type.\n",
        "print(\"\\n--- Creating a UDF in PySpark ---\")\n",
        "print(\"\"\"\n",
        "Creating a UDF in PySpark involves two main steps:\n",
        "1. Define a standard Python function that takes the input column(s) as arguments and returns the desired output.\n",
        "2. Register this Python function with Spark, specifying the return data type of the UDF. This is crucial for Spark to understand the schema of the output and optimize the execution plan.\n",
        "\"\"\")\n",
        "\n",
        "# 3. Explain how to register a Python function as a Spark UDF using spark.udf.register() or by using the @udf decorator.\n",
        "print(\"\\n--- Registering a UDF ---\")\n",
        "print(\"\"\"\n",
        "There are two primary ways to register a Python function as a Spark UDF:\n",
        "\n",
        "1. Using `spark.udf.register()`: This method registers the Python function as a UDF that can be used in Spark SQL queries.\n",
        "2. Using the `@udf` decorator: This is a more concise way to define and register a UDF, typically used when working with the DataFrame API.\n",
        "\"\"\")\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "# Example Python function\n",
        "def convert_to_uppercase(text):\n",
        "  if text is not None:\n",
        "    return text.upper()\n",
        "  return None\n",
        "\n",
        "# 4. Provide a clear example of how to use a registered UDF with a Spark DataFrame to apply a custom transformation to a column.\n",
        "print(\"\\n--- Example 1: Using a UDF with a single input column ---\")\n",
        "\n",
        "# Register the Python function as a UDF using the decorator\n",
        "# Specify the return type using PySpark data types\n",
        "uppercase_udf = udf(convert_to_uppercase, StringType())\n",
        "\n",
        "# Create a sample DataFrame (assuming 'df' from previous cells is available or create a new one)\n",
        "# If 'df' is not available, create a new one for demonstration\n",
        "try:\n",
        "    df.count() # Check if df exists\n",
        "except NameError:\n",
        "    print(\"Creating a new DataFrame for demonstration.\")\n",
        "    data = [(\"Alice\", \"New York\"), (\"Bob\", \"Los Angeles\"), (\"Charlie\", \"Chicago\")]\n",
        "    columns = [\"Name\", \"City\"]\n",
        "    df = spark.createDataFrame(data, columns)\n",
        "\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# Apply the UDF to a column\n",
        "df_with_uppercase_city = df.withColumn(\"City_Uppercase\", uppercase_udf(\"City\"))\n",
        "\n",
        "print(\"DataFrame with UDF applied:\")\n",
        "df_with_uppercase_city.show()\n",
        "\n",
        "# 5. Include an example demonstrating how to use a UDF with multiple input columns.\n",
        "print(\"\\n--- Example 2: Using a UDF with multiple input columns ---\")\n",
        "\n",
        "# Example Python function with multiple inputs\n",
        "def concatenate_strings(str1, str2):\n",
        "  if str1 is not None and str2 is not None:\n",
        "    return f\"{str1}, {str2}\"\n",
        "  return None\n",
        "\n",
        "# Register the Python function as a UDF\n",
        "concatenate_udf = udf(concatenate_strings, StringType())\n",
        "\n",
        "# Apply the UDF to multiple columns\n",
        "df_with_concatenated_columns = df.withColumn(\"Name_and_City\", concatenate_udf(\"Name\", \"City\"))\n",
        "\n",
        "print(\"DataFrame with UDF on multiple columns:\")\n",
        "df_with_concatenated_columns.show()\n",
        "\n",
        "# 6. Explain any potential performance considerations or limitations when using UDFs.\n",
        "print(\"\\n--- Performance Considerations and Limitations of UDFs ---\")\n",
        "print(\"\"\"\n",
        "While UDFs provide flexibility, they can have performance implications compared to using built-in Spark functions:\n",
        "\n",
        "- Serialization and Deserialization: Data needs to be serialized from the JVM to Python processes and then deserialized back, which introduces overhead.\n",
        "- Lack of Spark Optimization: Spark's Catalyst optimizer cannot fully optimize the code within a UDF because it's a black box to the optimizer. This means that optimizations like predicate pushdown or column pruning might not be applied within the UDF logic.\n",
        "- Row-by-Row Processing: UDFs typically operate on a row-by-row basis, which can be less efficient than vectorized operations performed by built-in Spark functions.\n",
        "\n",
        "To mitigate performance issues:\n",
        "- Prefer built-in Spark functions whenever possible.\n",
        "- If a UDF is necessary, try to perform as much logic as possible using Spark DataFrame operations before or after the UDF call.\n",
        "- Consider using Pandas UDFs (Vectorized UDFs) for potential performance improvements, especially for operations that can be vectorized using Pandas or NumPy. Pandas UDFs process data in batches, reducing the serialization/deserialization overhead.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- User Defined Functions (UDFs) in Spark SQL and DataFrames ---\n",
            "\n",
            "User Defined Functions (UDFs) in Spark SQL and DataFrames allow you to define your own custom functions using Python (or other languages like Scala or Java) and use them within Spark SQL queries or DataFrame transformations. They are particularly useful when the built-in Spark functions are not sufficient for your specific data processing needs. UDFs enable you to encapsulate complex logic or leverage external libraries that are not directly available as Spark functions.\n",
            "\n",
            "Why are UDFs useful?\n",
            "- Extend Spark's functionality: You can implement custom transformations or calculations that are not provided by Spark's built-in functions.\n",
            "- Reusability: Once defined, a UDF can be reused across multiple queries or DataFrame operations.\n",
            "- Integration with Python libraries: You can leverage the vast ecosystem of Python libraries within your Spark jobs.\n",
            "\n",
            "\n",
            "--- Creating a UDF in PySpark ---\n",
            "\n",
            "Creating a UDF in PySpark involves two main steps:\n",
            "1. Define a standard Python function that takes the input column(s) as arguments and returns the desired output.\n",
            "2. Register this Python function with Spark, specifying the return data type of the UDF. This is crucial for Spark to understand the schema of the output and optimize the execution plan.\n",
            "\n",
            "\n",
            "--- Registering a UDF ---\n",
            "\n",
            "There are two primary ways to register a Python function as a Spark UDF:\n",
            "\n",
            "1. Using `spark.udf.register()`: This method registers the Python function as a UDF that can be used in Spark SQL queries.\n",
            "2. Using the `@udf` decorator: This is a more concise way to define and register a UDF, typically used when working with the DataFrame API.\n",
            "\n",
            "\n",
            "--- Example 1: Using a UDF with a single input column ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o168.count.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3613)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3613)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3788605621.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# If 'df' is not available, create a new one for demonstration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Check if df exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating a new DataFrame for demonstration.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \"\"\"\n\u001b[0;32m-> 1238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o168.count.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3613)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3613)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0a1ac33"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the Spark session was stopped. I need to recreate the Spark session and the DataFrame before attempting to use the UDFs again. The rest of the code for explaining UDFs, creating them, registering them, and showing examples with single and multiple columns, along with performance considerations, is already present and correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdHRIAJvO8g-",
        "outputId": "5a9b2fe9-6975-4599-8446-969b406f1953"
      },
      "source": [
        "# Recreate SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQLFunctionsExample\").getOrCreate()\n",
        "\n",
        "# Recreate a sample DataFrame\n",
        "data = [(\"Alice\", \"New York\"), (\"Bob\", \"Los Angeles\"), (\"Charlie\", \"Chicago\")]\n",
        "columns = [\"Name\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# 1. Explain what User Defined Functions (UDFs) are in the context of Spark SQL and DataFrames, and why they are useful.\n",
        "print(\"--- User Defined Functions (UDFs) in Spark SQL and DataFrames ---\")\n",
        "print(\"\"\"\n",
        "User Defined Functions (UDFs) in Spark SQL and DataFrames allow you to define your own custom functions using Python (or other languages like Scala or Java) and use them within Spark SQL queries or DataFrame transformations. They are particularly useful when the built-in Spark functions are not sufficient for your specific data processing needs. UDFs enable you to encapsulate complex logic or leverage external libraries that are not directly available as Spark functions.\n",
        "\n",
        "Why are UDFs useful?\n",
        "- Extend Spark's functionality: You can implement custom transformations or calculations that are not provided by Spark's built-in functions.\n",
        "- Reusability: Once defined, a UDF can be reused across multiple queries or DataFrame operations.\n",
        "- Integration with Python libraries: You can leverage the vast ecosystem of Python libraries within your Spark jobs.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Describe the process of creating a UDF in PySpark, including defining a Python function and specifying the return type.\n",
        "print(\"\\n--- Creating a UDF in PySpark ---\")\n",
        "print(\"\"\"\n",
        "Creating a UDF in PySpark involves two main steps:\n",
        "1. Define a standard Python function that takes the input column(s) as arguments and returns the desired output.\n",
        "2. Register this Python function with Spark, specifying the return data type of the UDF. This is crucial for Spark to understand the schema of the output and optimize the execution plan.\n",
        "\"\"\")\n",
        "\n",
        "# 3. Explain how to register a Python function as a Spark UDF using spark.udf.register() or by using the @udf decorator.\n",
        "print(\"\\n--- Registering a UDF ---\")\n",
        "print(\"\"\"\n",
        "There are two primary ways to register a Python function as a Spark UDF:\n",
        "\n",
        "1. Using `spark.udf.register()`: This method registers the Python function as a UDF that can be used in Spark SQL queries.\n",
        "2. Using the `@udf` decorator: This is a more concise way to define and register a UDF, typically used when working with the DataFrame API.\n",
        "\"\"\")\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Example Python function\n",
        "def convert_to_uppercase(text):\n",
        "  if text is not None:\n",
        "    return text.upper()\n",
        "  return None\n",
        "\n",
        "# 4. Provide a clear example of how to use a registered UDF with a Spark DataFrame to apply a custom transformation to a column.\n",
        "print(\"\\n--- Example 1: Using a UDF with a single input column ---\")\n",
        "\n",
        "# Register the Python function as a UDF using the decorator\n",
        "# Specify the return type using PySpark data types\n",
        "uppercase_udf = udf(convert_to_uppercase, StringType())\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# Apply the UDF to a column\n",
        "df_with_uppercase_city = df.withColumn(\"City_Uppercase\", uppercase_udf(\"City\"))\n",
        "\n",
        "print(\"DataFrame with UDF applied:\")\n",
        "df_with_uppercase_city.show()\n",
        "\n",
        "# 5. Include an example demonstrating how to use a UDF with multiple input columns.\n",
        "print(\"\\n--- Example 2: Using a UDF with multiple input columns ---\")\n",
        "\n",
        "# Example Python function with multiple inputs\n",
        "def concatenate_strings(str1, str2):\n",
        "  if str1 is not None and str2 is not None:\n",
        "    return f\"{str1}, {str2}\"\n",
        "  return None\n",
        "\n",
        "# Register the Python function as a UDF\n",
        "concatenate_udf = udf(concatenate_strings, StringType())\n",
        "\n",
        "# Apply the UDF to multiple columns\n",
        "df_with_concatenated_columns = df.withColumn(\"Name_and_City\", concatenate_udf(\"Name\", \"City\"))\n",
        "\n",
        "print(\"DataFrame with UDF on multiple columns:\")\n",
        "df_with_concatenated_columns.show()\n",
        "\n",
        "# 6. Explain any potential performance considerations or limitations when using UDFs.\n",
        "print(\"\\n--- Performance Considerations and Limitations of UDFs ---\")\n",
        "print(\"\"\"\n",
        "While UDFs provide flexibility, they can have performance implications compared to using built-in Spark functions:\n",
        "\n",
        "- Serialization and Deserialization: Data needs to be serialized from the JVM to Python processes and then deserialized back, which introduces overhead.\n",
        "- Lack of Spark Optimization: Spark's Catalyst optimizer cannot fully optimize the code within a UDF because it's a black box to the optimizer. This means that optimizations like predicate pushdown or column pruning might not be applied within the UDF logic.\n",
        "- Row-by-Row Processing: UDFs typically operate on a row-by-row basis, which can be less efficient than vectorized operations performed by built-in Spark functions.\n",
        "\n",
        "To mitigate performance issues:\n",
        "- Prefer built-in Spark functions whenever possible.\n",
        "- If a UDF is necessary, try to perform as much logic as possible using Spark DataFrame operations before or after the UDF call.\n",
        "- Consider using Pandas UDFs (Vectorized UDFs) for potential performance improvements, especially for operations that can be vectorized using Pandas or NumPy. Pandas UDFs process data in batches, reducing the serialization/deserialization overhead.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- User Defined Functions (UDFs) in Spark SQL and DataFrames ---\n",
            "\n",
            "User Defined Functions (UDFs) in Spark SQL and DataFrames allow you to define your own custom functions using Python (or other languages like Scala or Java) and use them within Spark SQL queries or DataFrame transformations. They are particularly useful when the built-in Spark functions are not sufficient for your specific data processing needs. UDFs enable you to encapsulate complex logic or leverage external libraries that are not directly available as Spark functions.\n",
            "\n",
            "Why are UDFs useful?\n",
            "- Extend Spark's functionality: You can implement custom transformations or calculations that are not provided by Spark's built-in functions.\n",
            "- Reusability: Once defined, a UDF can be reused across multiple queries or DataFrame operations.\n",
            "- Integration with Python libraries: You can leverage the vast ecosystem of Python libraries within your Spark jobs.\n",
            "\n",
            "\n",
            "--- Creating a UDF in PySpark ---\n",
            "\n",
            "Creating a UDF in PySpark involves two main steps:\n",
            "1. Define a standard Python function that takes the input column(s) as arguments and returns the desired output.\n",
            "2. Register this Python function with Spark, specifying the return data type of the UDF. This is crucial for Spark to understand the schema of the output and optimize the execution plan.\n",
            "\n",
            "\n",
            "--- Registering a UDF ---\n",
            "\n",
            "There are two primary ways to register a Python function as a Spark UDF:\n",
            "\n",
            "1. Using `spark.udf.register()`: This method registers the Python function as a UDF that can be used in Spark SQL queries.\n",
            "2. Using the `@udf` decorator: This is a more concise way to define and register a UDF, typically used when working with the DataFrame API.\n",
            "\n",
            "\n",
            "--- Example 1: Using a UDF with a single input column ---\n",
            "Original DataFrame:\n",
            "+-------+-----------+\n",
            "|   Name|       City|\n",
            "+-------+-----------+\n",
            "|  Alice|   New York|\n",
            "|    Bob|Los Angeles|\n",
            "|Charlie|    Chicago|\n",
            "+-------+-----------+\n",
            "\n",
            "DataFrame with UDF applied:\n",
            "+-------+-----------+--------------+\n",
            "|   Name|       City|City_Uppercase|\n",
            "+-------+-----------+--------------+\n",
            "|  Alice|   New York|      NEW YORK|\n",
            "|    Bob|Los Angeles|   LOS ANGELES|\n",
            "|Charlie|    Chicago|       CHICAGO|\n",
            "+-------+-----------+--------------+\n",
            "\n",
            "\n",
            "--- Example 2: Using a UDF with multiple input columns ---\n",
            "DataFrame with UDF on multiple columns:\n",
            "+-------+-----------+----------------+\n",
            "|   Name|       City|   Name_and_City|\n",
            "+-------+-----------+----------------+\n",
            "|  Alice|   New York| Alice, New York|\n",
            "|    Bob|Los Angeles|Bob, Los Angeles|\n",
            "|Charlie|    Chicago|Charlie, Chicago|\n",
            "+-------+-----------+----------------+\n",
            "\n",
            "\n",
            "--- Performance Considerations and Limitations of UDFs ---\n",
            "\n",
            "While UDFs provide flexibility, they can have performance implications compared to using built-in Spark functions:\n",
            "\n",
            "- Serialization and Deserialization: Data needs to be serialized from the JVM to Python processes and then deserialized back, which introduces overhead.\n",
            "- Lack of Spark Optimization: Spark's Catalyst optimizer cannot fully optimize the code within a UDF because it's a black box to the optimizer. This means that optimizations like predicate pushdown or column pruning might not be applied within the UDF logic.\n",
            "- Row-by-Row Processing: UDFs typically operate on a row-by-row basis, which can be less efficient than vectorized operations performed by built-in Spark functions.\n",
            "\n",
            "To mitigate performance issues:\n",
            "- Prefer built-in Spark functions whenever possible.\n",
            "- If a UDF is necessary, try to perform as much logic as possible using Spark DataFrame operations before or after the UDF call.\n",
            "- Consider using Pandas UDFs (Vectorized UDFs) for potential performance improvements, especially for operations that can be vectorized using Pandas or NumPy. Pandas UDFs process data in batches, reducing the serialization/deserialization overhead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "676a4f48"
      },
      "source": [
        "## Explain dataframes & datasets\n",
        "\n",
        "### Subtask:\n",
        "Detail the concepts of DataFrames and Datasets, highlighting their differences, advantages, and when to use each. Include code examples for creating and manipulating DataFrames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87885035"
      },
      "source": [
        "**Reasoning**:\n",
        "Start by explaining what Spark DataFrames are, highlighting their structure and relationship to distributed data collections. Then, describe the key advantages of using DataFrames. Provide code examples to create a Spark DataFrame from a list of tuples and a Pandas DataFrame. Finally, illustrate basic DataFrame manipulation operations like selecting columns, filtering rows, and adding new columns with code examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aa74b8c",
        "outputId": "8e20e371-c56f-47ad-ced2-aa336ee3acc0"
      },
      "source": [
        "# Recreate SparkSession if it's stopped\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"DataFramesAndDatasetsExample\").getOrCreate()\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# 1. Explain what Spark DataFrames are\n",
        "print(\"--- Spark DataFrames ---\")\n",
        "print(\"\"\"\n",
        "Spark DataFrames are a distributed collection of data organized into named columns. Conceptually, they are equivalent to a table in a relational database or a data frame in R/Python, but with rich optimizations under the hood. DataFrames can be constructed from a wide array of data sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
        "\n",
        "Key Characteristics:\n",
        "- Schema: DataFrames have a schema, which defines the names and data types of the columns. This schema allows Spark to understand the structure of the data and apply optimizations.\n",
        "- Distributed: Like RDDs, DataFrames are distributed across the nodes in a Spark cluster, enabling parallel processing of large datasets.\n",
        "- Immutable: DataFrames are immutable, meaning that transformations on a DataFrame create a new DataFrame without modifying the original one.\n",
        "- High-level API: DataFrames provide a high-level, domain-specific language (DSL) for structured data processing, which is easier to use and more expressive than the low-level RDD API for many tasks.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Describe the key advantages of using DataFrames\n",
        "print(\"\\n--- Advantages of DataFrames ---\")\n",
        "print(\"\"\"\n",
        "- Optimization: DataFrames are optimized by the Catalyst optimizer, Spark's execution optimization engine. Catalyst can optimize the query plan by applying techniques like filter pushdown, column pruning, and join optimization, leading to significant performance improvements.\n",
        "- Ease of Use: The high-level API allows users to express complex data transformations in a more concise and readable way compared to RDDs.\n",
        "- Interoperability: DataFrames can easily interoperate with various data sources and formats (e.g., Parquet, ORC, JSON, CSV, JDBC, Hive).\n",
        "- Performance: Due to Catalyst optimization and Tungsten execution engine, DataFrames often outperform RDDs for structured data processing.\n",
        "- Integration with SQL: DataFrames can be queried using Spark SQL, allowing seamless integration with existing SQL-based tools and workflows.\n",
        "\"\"\")\n",
        "\n",
        "# 3. Provide code examples to demonstrate how to create a Spark DataFrame\n",
        "print(\"\\n--- Creating DataFrames ---\")\n",
        "\n",
        "# Example 1: Creating a DataFrame from a list of tuples\n",
        "data_tuples = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
        "columns_tuples = [\"Name\", \"ID\"]\n",
        "df_from_tuples = spark.createDataFrame(data_tuples, columns_tuples)\n",
        "print(\"DataFrame created from a list of tuples:\")\n",
        "df_from_tuples.show()\n",
        "\n",
        "# Example 2: Creating a DataFrame from a Pandas DataFrame\n",
        "data_pandas = {'Name': [\"Alice\", \"Bob\", \"Charlie\"], 'Age': [30, 35, 40]}\n",
        "pandas_df = pd.DataFrame(data_pandas)\n",
        "df_from_pandas = spark.createDataFrame(pandas_df)\n",
        "print(\"DataFrame created from a Pandas DataFrame:\")\n",
        "df_from_pandas.show()\n",
        "\n",
        "# Example 3: Creating a DataFrame from a list of Row objects\n",
        "data_rows = [Row(Name=\"Alice\", City=\"New York\"),\n",
        "             Row(Name=\"Bob\", City=\"Los Angeles\"),\n",
        "             Row(Name=\"Charlie\", City=\"Chicago\")]\n",
        "df_from_rows = spark.createDataFrame(data_rows)\n",
        "print(\"DataFrame created from a list of Row objects:\")\n",
        "df_from_rows.show()\n",
        "\n",
        "\n",
        "# 4. Illustrate basic DataFrame manipulation operations\n",
        "print(\"\\n--- Basic DataFrame Manipulation ---\")\n",
        "\n",
        "# Create a sample DataFrame for manipulation\n",
        "data_manipulation = [(\"Alice\", 1, \"New York\"),\n",
        "                     (\"Bob\", 2, \"Los Angeles\"),\n",
        "                     (\"Charlie\", 3, \"Chicago\"),\n",
        "                     (\"Alice\", 4, \"New York\")]\n",
        "columns_manipulation = [\"Name\", \"ID\", \"City\"]\n",
        "df_manipulation = spark.createDataFrame(data_manipulation, columns_manipulation)\n",
        "print(\"Original DataFrame for manipulation:\")\n",
        "df_manipulation.show()\n",
        "\n",
        "# Example: Selecting columns\n",
        "print(\"Selecting 'Name' and 'City' columns:\")\n",
        "df_manipulation.select(\"Name\", \"City\").show()\n",
        "\n",
        "# Example: Filtering rows\n",
        "print(\"Filtering rows where ID > 2:\")\n",
        "df_manipulation.filter(df_manipulation.ID > 2).show()\n",
        "\n",
        "# Example: Adding a new column using withColumn\n",
        "from pyspark.sql import functions as F\n",
        "print(\"Adding a new column 'ID_plus_10':\")\n",
        "df_with_new_column = df_manipulation.withColumn(\"ID_plus_10\", df_manipulation.ID + 10)\n",
        "df_with_new_column.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark DataFrames ---\n",
            "\n",
            "Spark DataFrames are a distributed collection of data organized into named columns. Conceptually, they are equivalent to a table in a relational database or a data frame in R/Python, but with rich optimizations under the hood. DataFrames can be constructed from a wide array of data sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
            "\n",
            "Key Characteristics:\n",
            "- Schema: DataFrames have a schema, which defines the names and data types of the columns. This schema allows Spark to understand the structure of the data and apply optimizations.\n",
            "- Distributed: Like RDDs, DataFrames are distributed across the nodes in a Spark cluster, enabling parallel processing of large datasets.\n",
            "- Immutable: DataFrames are immutable, meaning that transformations on a DataFrame create a new DataFrame without modifying the original one.\n",
            "- High-level API: DataFrames provide a high-level, domain-specific language (DSL) for structured data processing, which is easier to use and more expressive than the low-level RDD API for many tasks.\n",
            "\n",
            "\n",
            "--- Advantages of DataFrames ---\n",
            "\n",
            "- Optimization: DataFrames are optimized by the Catalyst optimizer, Spark's execution optimization engine. Catalyst can optimize the query plan by applying techniques like filter pushdown, column pruning, and join optimization, leading to significant performance improvements.\n",
            "- Ease of Use: The high-level API allows users to express complex data transformations in a more concise and readable way compared to RDDs.\n",
            "- Interoperability: DataFrames can easily interoperate with various data sources and formats (e.g., Parquet, ORC, JSON, CSV, JDBC, Hive).\n",
            "- Performance: Due to Catalyst optimization and Tungsten execution engine, DataFrames often outperform RDDs for structured data processing.\n",
            "- Integration with SQL: DataFrames can be queried using Spark SQL, allowing seamless integration with existing SQL-based tools and workflows.\n",
            "\n",
            "\n",
            "--- Creating DataFrames ---\n",
            "DataFrame created from a list of tuples:\n",
            "+-------+---+\n",
            "|   Name| ID|\n",
            "+-------+---+\n",
            "|  Alice|  1|\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "+-------+---+\n",
            "\n",
            "DataFrame created from a Pandas DataFrame:\n",
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 35|\n",
            "|Charlie| 40|\n",
            "+-------+---+\n",
            "\n",
            "DataFrame created from a list of Row objects:\n",
            "+-------+-----------+\n",
            "|   Name|       City|\n",
            "+-------+-----------+\n",
            "|  Alice|   New York|\n",
            "|    Bob|Los Angeles|\n",
            "|Charlie|    Chicago|\n",
            "+-------+-----------+\n",
            "\n",
            "\n",
            "--- Basic DataFrame Manipulation ---\n",
            "Original DataFrame for manipulation:\n",
            "+-------+---+-----------+\n",
            "|   Name| ID|       City|\n",
            "+-------+---+-----------+\n",
            "|  Alice|  1|   New York|\n",
            "|    Bob|  2|Los Angeles|\n",
            "|Charlie|  3|    Chicago|\n",
            "|  Alice|  4|   New York|\n",
            "+-------+---+-----------+\n",
            "\n",
            "Selecting 'Name' and 'City' columns:\n",
            "+-------+-----------+\n",
            "|   Name|       City|\n",
            "+-------+-----------+\n",
            "|  Alice|   New York|\n",
            "|    Bob|Los Angeles|\n",
            "|Charlie|    Chicago|\n",
            "|  Alice|   New York|\n",
            "+-------+-----------+\n",
            "\n",
            "Filtering rows where ID > 2:\n",
            "+-------+---+--------+\n",
            "|   Name| ID|    City|\n",
            "+-------+---+--------+\n",
            "|Charlie|  3| Chicago|\n",
            "|  Alice|  4|New York|\n",
            "+-------+---+--------+\n",
            "\n",
            "Adding a new column 'ID_plus_10':\n",
            "+-------+---+-----------+----------+\n",
            "|   Name| ID|       City|ID_plus_10|\n",
            "+-------+---+-----------+----------+\n",
            "|  Alice|  1|   New York|        11|\n",
            "|    Bob|  2|Los Angeles|        12|\n",
            "|Charlie|  3|    Chicago|        13|\n",
            "|  Alice|  4|   New York|        14|\n",
            "+-------+---+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "589ab320"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the concept of Spark Datasets, their differences from DataFrames, particularly regarding type safety and encoding. Briefly mention why Datasets are less common in PySpark. Finally, summarize the key differences and provide guidance on when to choose DataFrames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35b301e6",
        "outputId": "004778ba-5852-4b79-bb0e-a06072f197b7"
      },
      "source": [
        "# 5. Explain the concept of Spark Datasets\n",
        "print(\"\\n--- Spark Datasets ---\")\n",
        "print(\"\"\"\n",
        "Spark Datasets were introduced in Spark 1.6 as an extension of DataFrames, primarily for Scala and Java APIs. Datasets provide the benefits of RDDs (strong typing and the ability to use powerful lambda functions) with the benefits of Spark SQL's optimized execution engine.\n",
        "\n",
        "Key Characteristics of Datasets (Scala/Java):\n",
        "- Type Safety: Datasets are strongly typed. This means that you define a case class (in Scala) or a Java class that represents the schema of your data. Spark enforces this type safety at compile time, catching potential errors early.\n",
        "- Encoder: Datasets use Encoders to serialize and deserialize JVM objects to and from Spark's internal Tungsten binary format. This allows for efficient processing and storage.\n",
        "- Unified API: Datasets aim to provide a unified API for working with structured and semi-structured data.\n",
        "\n",
        "Datasets in PySpark:\n",
        "While Datasets are a core concept in Scala and Java Spark, they are less prominent in PySpark. The DataFrame API in PySpark already provides a high-level, user-friendly interface that is well-integrated with Python's data science ecosystem (e.g., Pandas). PySpark DataFrames are essentially untyped Datasets of `Row` objects. The benefits of compile-time type safety offered by Datasets in Scala/Java are not directly applicable in Python's dynamic typing environment. Therefore, you will primarily work with DataFrames when using PySpark.\n",
        "\"\"\")\n",
        "\n",
        "# 6. Summarize the key differences and provide guidance on when to choose DataFrames\n",
        "print(\"\\n--- DataFrames vs. Datasets (Summary) ---\")\n",
        "print(\"\"\"\n",
        "| Feature         | DataFrame (PySpark)                     | Dataset (Scala/Java)                     |\n",
        "|-----------------|-----------------------------------------|------------------------------------------|\n",
        "| Type Safety     | Runtime type checking (untyped)         | Compile-time type checking (strongly typed)|\n",
        "| API Language    | Python, Scala, Java, R                  | Scala, Java                              |\n",
        "| Performance     | Optimized by Catalyst and Tungsten      | Optimized by Catalyst and Tungsten       |\n",
        "| Readability     | High-level, expressive DSL              | High-level, expressive DSL               |\n",
        "| Use Case        | General structured data processing      | Structured data processing with type safety needs |\n",
        "| PySpark Usage   | Primary abstraction for structured data | Less common, essentially untyped DataFrames of Rows |\n",
        "\n",
        "When to choose DataFrames:\n",
        "- You are working with PySpark, as DataFrames are the primary and most feature-rich API for structured data processing in Python.\n",
        "- You need a high-level, easy-to-use API for common data transformations and analysis.\n",
        "- You want to leverage Spark's optimization engine for performance.\n",
        "- You are working with various data sources and formats.\n",
        "\n",
        "When to choose Datasets (in Scala/Java):\n",
        "- You require compile-time type safety to catch errors early in development.\n",
        "- You want to work with custom JVM objects directly in a distributed collection.\n",
        "\n",
        "In PySpark, the choice is almost always to use DataFrames due to the language's dynamic nature and the comprehensive capabilities of the DataFrame API.\n",
        "\"\"\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Spark Datasets ---\n",
            "\n",
            "Spark Datasets were introduced in Spark 1.6 as an extension of DataFrames, primarily for Scala and Java APIs. Datasets provide the benefits of RDDs (strong typing and the ability to use powerful lambda functions) with the benefits of Spark SQL's optimized execution engine.\n",
            "\n",
            "Key Characteristics of Datasets (Scala/Java):\n",
            "- Type Safety: Datasets are strongly typed. This means that you define a case class (in Scala) or a Java class that represents the schema of your data. Spark enforces this type safety at compile time, catching potential errors early.\n",
            "- Encoder: Datasets use Encoders to serialize and deserialize JVM objects to and from Spark's internal Tungsten binary format. This allows for efficient processing and storage.\n",
            "- Unified API: Datasets aim to provide a unified API for working with structured and semi-structured data.\n",
            "\n",
            "Datasets in PySpark:\n",
            "While Datasets are a core concept in Scala and Java Spark, they are less prominent in PySpark. The DataFrame API in PySpark already provides a high-level, user-friendly interface that is well-integrated with Python's data science ecosystem (e.g., Pandas). PySpark DataFrames are essentially untyped Datasets of `Row` objects. The benefits of compile-time type safety offered by Datasets in Scala/Java are not directly applicable in Python's dynamic typing environment. Therefore, you will primarily work with DataFrames when using PySpark.\n",
            "\n",
            "\n",
            "--- DataFrames vs. Datasets (Summary) ---\n",
            "\n",
            "| Feature         | DataFrame (PySpark)                     | Dataset (Scala/Java)                     |\n",
            "|-----------------|-----------------------------------------|------------------------------------------|\n",
            "| Type Safety     | Runtime type checking (untyped)         | Compile-time type checking (strongly typed)|\n",
            "| API Language    | Python, Scala, Java, R                  | Scala, Java                              |\n",
            "| Performance     | Optimized by Catalyst and Tungsten      | Optimized by Catalyst and Tungsten       |\n",
            "| Readability     | High-level, expressive DSL              | High-level, expressive DSL               |\n",
            "| Use Case        | General structured data processing      | Structured data processing with type safety needs |\n",
            "| PySpark Usage   | Primary abstraction for structured data | Less common, essentially untyped DataFrames of Rows |\n",
            "\n",
            "When to choose DataFrames:\n",
            "- You are working with PySpark, as DataFrames are the primary and most feature-rich API for structured data processing in Python.\n",
            "- You need a high-level, easy-to-use API for common data transformations and analysis.\n",
            "- You want to leverage Spark's optimization engine for performance.\n",
            "- You are working with various data sources and formats.\n",
            "\n",
            "When to choose Datasets (in Scala/Java):\n",
            "- You require compile-time type safety to catch errors early in development.\n",
            "- You want to work with custom JVM objects directly in a distributed collection.\n",
            "\n",
            "In PySpark, the choice is almost always to use DataFrames due to the language's dynamic nature and the comprehensive capabilities of the DataFrame API.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b423d8"
      },
      "source": [
        "## Explain interoperating with rdds\n",
        "\n",
        "### Subtask:\n",
        "Explain how to convert between RDDs and DataFrames, demonstrating the interoperability between these two Spark abstractions with code examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6235151"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the relationship between RDDs and DataFrames/Datasets, demonstrate converting an RDD to a DataFrame with schema, and show converting a DataFrame back to an RDD, including use cases for RDDs after DataFrame creation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "a4c56d4a",
        "outputId": "35c48211-9b5c-444a-8e5a-c4edf02198e2"
      },
      "source": [
        "# Recreate SparkSession if it's stopped\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"RDDandDataFrameInteroperability\").getOrCreate()\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# 1. Explain the relationship between RDDs and DataFrames/Datasets in Spark.\n",
        "print(\"--- RDDs and DataFrames/Datasets Relationship ---\")\n",
        "print(\"\"\"\n",
        "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark, representing an immutable, fault-tolerant, distributed collection of objects. They are low-level and provide fine-grained control over data processing.\n",
        "\n",
        "DataFrames and Datasets are built on top of RDDs. They provide a higher-level abstraction that organizes data into named columns with a schema. While DataFrames/Datasets internally use RDDs to distribute and process data, they offer significant advantages for structured and semi-structured data through optimization (Catalyst optimizer) and a more user-friendly API.\n",
        "\n",
        "In essence:\n",
        "- RDDs: Low-level, unstructured or semi-structured data, manual optimization, strong typing (Scala/Java).\n",
        "- DataFrames/Datasets: High-level, structured data, automatically optimized, schema-aware, easier API for many tasks.\n",
        "\n",
        "Spark provides mechanisms to seamlessly convert between these abstractions, allowing developers to choose the most appropriate tool for different parts of their data processing pipeline.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Demonstrate how to convert an existing RDD into a DataFrame in PySpark.\n",
        "print(\"\\n--- Converting RDD to DataFrame ---\")\n",
        "\n",
        "# Create a sample RDD\n",
        "data_rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)])\n",
        "print(\"Original RDD:\")\n",
        "print(data_rdd.collect())\n",
        "\n",
        "# Convert RDD to DataFrame (inferred schema - generally not recommended for production)\n",
        "df_from_rdd_inferred = data_rdd.toDF([\"Name\", \"ID\"])\n",
        "print(\"\\nDataFrame converted from RDD (inferred schema):\")\n",
        "df_from_rdd_inferred.show()\n",
        "\n",
        "# Demonstrate how to explicitly define the schema during conversion.\n",
        "print(\"\\nConverting RDD to DataFrame with explicit schema:\")\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"ID\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert RDD to DataFrame with explicit schema\n",
        "df_from_rdd_explicit = spark.createDataFrame(data_rdd, schema)\n",
        "print(\"DataFrame converted from RDD (explicit schema):\")\n",
        "df_from_rdd_explicit.show()\n",
        "\n",
        "\n",
        "# 3. Demonstrate how to convert a Spark DataFrame back into an RDD in PySpark.\n",
        "print(\"\\n--- Converting DataFrame to RDD ---\")\n",
        "\n",
        "# Convert the DataFrame back to an RDD\n",
        "rdd_from_df = df_from_rdd_explicit.rdd\n",
        "print(\"RDD converted from DataFrame:\")\n",
        "print(rdd_from_df.collect())\n",
        "\n",
        "# The elements in the RDD converted from a DataFrame are Row objects\n",
        "print(\"\\nElements in RDD converted from DataFrame are Row objects:\")\n",
        "print(rdd_from_df.first())\n",
        "\n",
        "\n",
        "# 4. Provide examples of when you might choose to work with RDDs even after creating a DataFrame.\n",
        "print(\"\\n--- Use Cases for RDDs after DataFrame Creation ---\")\n",
        "#print(\"\"\"\n",
        "While DataFrames are generally preferred for structured data due to optimizations, there are scenarios where working with the underlying RDD might be necessary or more convenient:\n",
        "\n",
        "1. Low-level Transformations: For complex transformations that are difficult or inefficient to express using the DataFrame API or Spark SQL functions. RDDs provide full control over each element's processing.\n",
        "   Example: Applying a complex, non-standard algorithm to each record that involves operations not supported by built-in DataFrame functions.\n",
        "\n",
        "2. Unstructured Data Processing: If you need to switch to processing unstructured or semi-structured parts of your data (e.g., log files) that don't fit neatly into a DataFrame schema for a specific step.\n",
        "\n",
        "3. Legacy Code Integration: Integrating with existing Spark code that is based on the RDD API.\n",
        "\n",
        "4. Custom Partitioning or Control: When you need very fine-grained control over data partitioning or other low-level aspects of processing that are not exposed through the DataFrame API.\n",
        "\n",
        "Example (conceptual) - using RDD map for a complex, non-standard calculation per row:\n",
        "While `withColumn` and UDFs cover many cases, for highly specialized per-row logic, dropping to RDD might sometimes be considered if other methods are too cumbersome or perform poorly for that specific task."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 70) (ipython-input-3508088410.py, line 70)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3508088410.py\"\u001b[0;36m, line \u001b[0;32m70\u001b[0m\n\u001b[0;31m    1. Low-level Transformations: For complex transformations that are difficult or inefficient to express using the DataFrame API or Spark SQL functions. RDDs provide full control over each element's processing.\u001b[0m\n\u001b[0m                                                                                                                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cad9dbb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed due to a `SyntaxError: incomplete input`. This is likely caused by an unclosed multiline string or a similar syntax issue within the code block. I need to regenerate the code block for the same steps, ensuring correct syntax.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "tCKlp6aNPR4n",
        "outputId": "3f74f7f7-1f9c-4081-9e85-dd69e3924d5f"
      },
      "source": [
        "# Recreate SparkSession if it's stopped\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"RDDandDataFrameInteroperability\").getOrCreate()\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# 1. Explain the relationship between RDDs and DataFrames/Datasets in Spark.\n",
        "print(\"--- RDDs and DataFrames/Datasets Relationship ---\")\n",
        "print(\"\"\"\n",
        "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark, representing an immutable, fault-tolerant, distributed collection of objects. They are low-level and provide fine-grained control over data processing.\n",
        "\n",
        "DataFrames and Datasets are built on top of RDDs. They provide a higher-level abstraction that organizes data into named columns with a schema. While DataFrames/Datasets internally use RDDs to distribute and process data, they offer significant advantages for structured and semi-structured data through optimization (Catalyst optimizer) and a more user-friendly API.\n",
        "\n",
        "In essence:\n",
        "- RDDs: Low-level, unstructured or semi-structured data, manual optimization, strong typing (Scala/Java).\n",
        "- DataFrames/Datasets: High-level, structured data, automatically optimized, schema-aware, easier API for many tasks.\n",
        "\n",
        "Spark provides mechanisms to seamlessly convert between these abstractions, allowing developers to choose the most appropriate tool for different parts of their data processing pipeline.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Demonstrate how to convert an existing RDD into a DataFrame in PySpark.\n",
        "print(\"\\n--- Converting RDD to DataFrame ---\")\n",
        "\n",
        "# Create a sample RDD\n",
        "data_rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)])\n",
        "print(\"Original RDD:\")\n",
        "print(data_rdd.collect())\n",
        "\n",
        "# Convert RDD to DataFrame (inferred schema - generally not recommended for production)\n",
        "df_from_rdd_inferred = data_rdd.toDF([\"Name\", \"ID\"])\n",
        "print(\"\\nDataFrame converted from RDD (inferred schema):\")\n",
        "df_from_rdd_inferred.show()\n",
        "\n",
        "# Demonstrate how to explicitly define the schema during conversion.\n",
        "print(\"\\nConverting RDD to DataFrame with explicit schema:\")\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"ID\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert RDD to DataFrame with explicit schema\n",
        "df_from_rdd_explicit = spark.createDataFrame(data_rdd, schema)\n",
        "print(\"DataFrame converted from RDD (explicit schema):\")\n",
        "df_from_rdd_explicit.show()\n",
        "\n",
        "\n",
        "# 3. Demonstrate how to convert a Spark DataFrame back into an RDD in PySpark.\n",
        "print(\"\\n--- Converting DataFrame to RDD ---\")\n",
        "\n",
        "# Convert the DataFrame back to an RDD\n",
        "rdd_from_df = df_from_rdd_explicit.rdd\n",
        "print(\"RDD converted from DataFrame:\")\n",
        "print(rdd_from_df.collect())\n",
        "\n",
        "# The elements in the RDD converted from a DataFrame are Row objects\n",
        "print(\"\\nElements in RDD converted from DataFrame are Row objects:\")\n",
        "print(rdd_from_df.first())\n",
        "\n",
        "\n",
        "# 4. Provide examples of when you might choose to work with RDDs even after creating a DataFrame.\n",
        "print(\"\\n--- Use Cases for RDDs after DataFrame Creation ---\")\n",
        "print(\"\"\"\n",
        "While DataFrames are generally preferred for structured data due to optimizations, there are scenarios where working with the underlying RDD might be necessary or more convenient:\n",
        "\n",
        "1. Low-level Transformations: For complex transformations that are difficult or inefficient to express using the DataFrame API or Spark SQL functions. RDDs provide full control over each element's processing.\n",
        "   Example: Applying a complex, non-standard algorithm to each record that involves operations not supported by built-in DataFrame functions.\n",
        "\n",
        "2. Unstructured Data Processing: If you need to switch to processing unstructured or semi-structured parts of your data (e.g., log files) that don't fit neatly into a DataFrame schema for a specific step.\n",
        "\n",
        "3. Legacy Code Integration: Integrating with existing Spark code that is based on the RDD API.\n",
        "\n",
        "4. Custom Partitioning or Control: When you need very fine-grained control over data partitioning or other low-level aspects of processing that are not exposed through the DataFrame API.\n",
        "\n",
        "Example (conceptual) - using RDD map for a complex, non-standard calculation per row:\n",
        "While `withColumn` and UDFs cover many cases, for highly specialized per-row logic, dropping to RDD might sometimes be considered if other methods are too cumbersome or perform poorly for that specific task.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RDDs and DataFrames/Datasets Relationship ---\n",
            "\n",
            "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark, representing an immutable, fault-tolerant, distributed collection of objects. They are low-level and provide fine-grained control over data processing.\n",
            "\n",
            "DataFrames and Datasets are built on top of RDDs. They provide a higher-level abstraction that organizes data into named columns with a schema. While DataFrames/Datasets internally use RDDs to distribute and process data, they offer significant advantages for structured and semi-structured data through optimization (Catalyst optimizer) and a more user-friendly API.\n",
            "\n",
            "In essence:\n",
            "- RDDs: Low-level, unstructured or semi-structured data, manual optimization, strong typing (Scala/Java).\n",
            "- DataFrames/Datasets: High-level, structured data, automatically optimized, schema-aware, easier API for many tasks.\n",
            "\n",
            "Spark provides mechanisms to seamlessly convert between these abstractions, allowing developers to choose the most appropriate tool for different parts of their data processing pipeline.\n",
            "\n",
            "\n",
            "--- Converting RDD to DataFrame ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'sc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2357972862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Create a sample RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdata_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Alice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Bob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Charlie\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original RDD:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \"\"\"\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e4d410b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the SparkSession was likely stopped or not properly initialized, leading to an `AttributeError: 'NoneType' object has no attribute 'sc'`. I need to ensure the SparkSession is active before attempting to create an RDD using `spark.sparkContext`. I will regenerate the code block, including the check for an active SparkSession and the steps to demonstrate RDD-DataFrame interoperability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "MM1T4UjJPVSg",
        "outputId": "a1a4ef51-67d4-4778-eb18-22bf69a977da"
      },
      "source": [
        "# Ensure SparkSession is active\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"RDDandDataFrameInteroperability\").getOrCreate()\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# 1. Explain the relationship between RDDs and DataFrames/Datasets in Spark.\n",
        "print(\"--- RDDs and DataFrames/Datasets Relationship ---\")\n",
        "print(\"\"\"\n",
        "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark, representing an immutable, fault-tolerant, distributed collection of objects. They are low-level and provide fine-grained control over data processing.\n",
        "\n",
        "DataFrames and Datasets are built on top of RDDs. They provide a higher-level abstraction that organizes data into named columns with a schema. While DataFrames/Datasets internally use RDDs to distribute and process data, they offer significant advantages for structured and semi-structured data through optimization (Catalyst optimizer) and a more user-friendly API.\n",
        "\n",
        "In essence:\n",
        "- RDDs: Low-level, unstructured or semi-structured data, manual optimization, strong typing (Scala/Java).\n",
        "- DataFrames/Datasets: High-level, structured data, automatically optimized, schema-aware, easier API for many tasks.\n",
        "\n",
        "Spark provides mechanisms to seamlessly convert between these abstractions, allowing developers to choose the most appropriate tool for different parts of their data processing pipeline.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Demonstrate how to convert an existing RDD into a DataFrame in PySpark.\n",
        "print(\"\\n--- Converting RDD to DataFrame ---\")\n",
        "\n",
        "# Create a sample RDD\n",
        "data_rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)])\n",
        "print(\"Original RDD:\")\n",
        "print(data_rdd.collect())\n",
        "\n",
        "# Convert RDD to DataFrame (inferred schema - generally not recommended for production)\n",
        "df_from_rdd_inferred = data_rdd.toDF([\"Name\", \"ID\"])\n",
        "print(\"\\nDataFrame converted from RDD (inferred schema):\")\n",
        "df_from_rdd_inferred.show()\n",
        "\n",
        "# Demonstrate how to explicitly define the schema during conversion.\n",
        "print(\"\\nConverting RDD to DataFrame with explicit schema:\")\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"ID\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert RDD to DataFrame with explicit schema\n",
        "df_from_rdd_explicit = spark.createDataFrame(data_rdd, schema)\n",
        "print(\"DataFrame converted from RDD (explicit schema):\")\n",
        "df_from_rdd_explicit.show()\n",
        "\n",
        "\n",
        "# 3. Demonstrate how to convert a Spark DataFrame back into an RDD in PySpark.\n",
        "print(\"\\n--- Converting DataFrame to RDD ---\")\n",
        "\n",
        "# Convert the DataFrame back to an RDD\n",
        "rdd_from_df = df_from_rdd_explicit.rdd\n",
        "print(\"RDD converted from DataFrame:\")\n",
        "print(rdd_from_df.collect())\n",
        "\n",
        "# The elements in the RDD converted from a DataFrame are Row objects\n",
        "print(\"\\nElements in RDD converted from DataFrame are Row objects:\")\n",
        "print(rdd_from_df.first())\n",
        "\n",
        "\n",
        "# 4. Provide examples of when you might choose to work with RDDs even after creating a DataFrame.\n",
        "print(\"\\n--- Use Cases for RDDs after DataFrame Creation ---\")\n",
        "print(\"\"\"\n",
        "While DataFrames are generally preferred for structured data due to optimizations, there are scenarios where working with the underlying RDD might be necessary or more convenient:\n",
        "\n",
        "1. Low-level Transformations: For complex transformations that are difficult or inefficient to express using the DataFrame API or Spark SQL functions. RDDs provide full control over each element's processing.\n",
        "   Example: Applying a complex, non-standard algorithm to each record that involves operations not supported by built-in DataFrame functions.\n",
        "\n",
        "2. Unstructured Data Processing: If you need to switch to processing unstructured or semi-structured parts of your data (e.g., log files) that don't fit neatly into a DataFrame schema for a specific step.\n",
        "\n",
        "3. Legacy Code Integration: Integrating with existing Spark code that is based on the RDD API.\n",
        "\n",
        "4. Custom Partitioning or Control: When you need very fine-grained control over data partitioning or other low-level aspects of processing that are not exposed through the DataFrame API.\n",
        "\n",
        "Example (conceptual) - using RDD map for a complex, non-standard calculation per row:\n",
        "While `withColumn` and UDFs cover many cases, for highly specialized per-row logic, dropping to RDD might sometimes be considered if other methods are too cumbersome or perform poorly for that specific task.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RDDs and DataFrames/Datasets Relationship ---\n",
            "\n",
            "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark, representing an immutable, fault-tolerant, distributed collection of objects. They are low-level and provide fine-grained control over data processing.\n",
            "\n",
            "DataFrames and Datasets are built on top of RDDs. They provide a higher-level abstraction that organizes data into named columns with a schema. While DataFrames/Datasets internally use RDDs to distribute and process data, they offer significant advantages for structured and semi-structured data through optimization (Catalyst optimizer) and a more user-friendly API.\n",
            "\n",
            "In essence:\n",
            "- RDDs: Low-level, unstructured or semi-structured data, manual optimization, strong typing (Scala/Java).\n",
            "- DataFrames/Datasets: High-level, structured data, automatically optimized, schema-aware, easier API for many tasks.\n",
            "\n",
            "Spark provides mechanisms to seamlessly convert between these abstractions, allowing developers to choose the most appropriate tool for different parts of their data processing pipeline.\n",
            "\n",
            "\n",
            "--- Converting RDD to DataFrame ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'sc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2610923565.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Create a sample RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdata_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Alice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Bob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Charlie\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original RDD:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \"\"\"\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d17ae2ba"
      },
      "source": [
        "**Reasoning**:\n",
        "The `AttributeError: 'NoneType' object has no attribute 'sc'` indicates that the SparkContext is not available through the `spark.sparkContext` attribute, likely because the SparkSession was not properly created or has been stopped. I will try to explicitly create a new SparkSession and then proceed with the RDD and DataFrame operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgk_VDMpPYi2",
        "outputId": "633fb63d-37ac-462d-b47a-7325ed9a5df8"
      },
      "source": [
        "# Explicitly create a new SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RDDandDataFrameInteroperability\").getOrCreate()\n",
        "\n",
        "\n",
        "# 1. Explain the relationship between RDDs and DataFrames/Datasets in Spark.\n",
        "print(\"--- RDDs and DataFrames/Datasets Relationship ---\")\n",
        "print(\"\"\"\n",
        "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark, representing an immutable, fault-tolerant, distributed collection of objects. They are low-level and provide fine-grained control over data processing.\n",
        "\n",
        "DataFrames and Datasets are built on top of RDDs. They provide a higher-level abstraction that organizes data into named columns with a schema. While DataFrames/Datasets internally use RDDs to distribute and process data, they offer significant advantages for structured and semi-structured data through optimization (Catalyst optimizer) and a more user-friendly API.\n",
        "\n",
        "In essence:\n",
        "- RDDs: Low-level, unstructured or semi-structured data, manual optimization, strong typing (Scala/Java).\n",
        "- DataFrames/Datasets: High-level, structured data, automatically optimized, schema-aware, easier API for many tasks.\n",
        "\n",
        "Spark provides mechanisms to seamlessly convert between these abstractions, allowing developers to choose the most appropriate tool for different parts of their data processing pipeline.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Demonstrate how to convert an existing RDD into a DataFrame in PySpark.\n",
        "print(\"\\n--- Converting RDD to DataFrame ---\")\n",
        "\n",
        "# Create a sample RDD\n",
        "data_rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)])\n",
        "print(\"Original RDD:\")\n",
        "print(data_rdd.collect())\n",
        "\n",
        "# Convert RDD to DataFrame (inferred schema - generally not recommended for production)\n",
        "df_from_rdd_inferred = data_rdd.toDF([\"Name\", \"ID\"])\n",
        "print(\"\\nDataFrame converted from RDD (inferred schema):\")\n",
        "df_from_rdd_inferred.show()\n",
        "\n",
        "# Demonstrate how to explicitly define the schema during conversion.\n",
        "print(\"\\nConverting RDD to DataFrame with explicit schema:\")\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"ID\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert RDD to DataFrame with explicit schema\n",
        "df_from_rdd_explicit = spark.createDataFrame(data_rdd, schema)\n",
        "print(\"DataFrame converted from RDD (explicit schema):\")\n",
        "df_from_rdd_explicit.show()\n",
        "\n",
        "\n",
        "# 3. Demonstrate how to convert a Spark DataFrame back into an RDD in PySpark.\n",
        "print(\"\\n--- Converting DataFrame to RDD ---\")\n",
        "\n",
        "# Convert the DataFrame back to an RDD\n",
        "rdd_from_df = df_from_rdd_explicit.rdd\n",
        "print(\"RDD converted from DataFrame:\")\n",
        "print(rdd_from_df.collect())\n",
        "\n",
        "# The elements in the RDD converted from a DataFrame are Row objects\n",
        "print(\"\\nElements in RDD converted from DataFrame are Row objects:\")\n",
        "print(rdd_from_df.first())\n",
        "\n",
        "\n",
        "# 4. Provide examples of when you might choose to work with RDDs even after creating a DataFrame.\n",
        "print(\"\\n--- Use Cases for RDDs after DataFrame Creation ---\")\n",
        "print(\"\"\"\n",
        "While DataFrames are generally preferred for structured data due to optimizations, there are scenarios where working with the underlying RDD might be necessary or more convenient:\n",
        "\n",
        "1. Low-level Transformations: For complex transformations that are difficult or inefficient to express using the DataFrame API or Spark SQL functions. RDDs provide full control over each element's processing.\n",
        "   Example: Applying a complex, non-standard algorithm to each record that involves operations not supported by built-in DataFrame functions.\n",
        "\n",
        "2. Unstructured Data Processing: If you need to switch to processing unstructured or semi-structured parts of your data (e.g., log files) that don't fit neatly into a DataFrame schema for a specific step.\n",
        "\n",
        "3. Legacy Code Integration: Integrating with existing Spark code that is based on the RDD API.\n",
        "\n",
        "4. Custom Partitioning or Control: When you need very fine-grained control over data partitioning or other low-level aspects of processing that are not exposed through the DataFrame API.\n",
        "\n",
        "Example (conceptual) - using RDD map for a complex, non-standard calculation per row:\n",
        "While `withColumn` and UDFs cover many cases, for highly specialized per-row logic, dropping to RDD might sometimes be considered if other methods are too cumbersome or perform poorly for that specific task.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RDDs and DataFrames/Datasets Relationship ---\n",
            "\n",
            "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark, representing an immutable, fault-tolerant, distributed collection of objects. They are low-level and provide fine-grained control over data processing.\n",
            "\n",
            "DataFrames and Datasets are built on top of RDDs. They provide a higher-level abstraction that organizes data into named columns with a schema. While DataFrames/Datasets internally use RDDs to distribute and process data, they offer significant advantages for structured and semi-structured data through optimization (Catalyst optimizer) and a more user-friendly API.\n",
            "\n",
            "In essence:\n",
            "- RDDs: Low-level, unstructured or semi-structured data, manual optimization, strong typing (Scala/Java).\n",
            "- DataFrames/Datasets: High-level, structured data, automatically optimized, schema-aware, easier API for many tasks.\n",
            "\n",
            "Spark provides mechanisms to seamlessly convert between these abstractions, allowing developers to choose the most appropriate tool for different parts of their data processing pipeline.\n",
            "\n",
            "\n",
            "--- Converting RDD to DataFrame ---\n",
            "Original RDD:\n",
            "[('Alice', 1), ('Bob', 2), ('Charlie', 3)]\n",
            "\n",
            "DataFrame converted from RDD (inferred schema):\n",
            "+-------+---+\n",
            "|   Name| ID|\n",
            "+-------+---+\n",
            "|  Alice|  1|\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "Converting RDD to DataFrame with explicit schema:\n",
            "DataFrame converted from RDD (explicit schema):\n",
            "+-------+---+\n",
            "|   Name| ID|\n",
            "+-------+---+\n",
            "|  Alice|  1|\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "--- Converting DataFrame to RDD ---\n",
            "RDD converted from DataFrame:\n",
            "[Row(Name='Alice', ID=1), Row(Name='Bob', ID=2), Row(Name='Charlie', ID=3)]\n",
            "\n",
            "Elements in RDD converted from DataFrame are Row objects:\n",
            "Row(Name='Alice', ID=1)\n",
            "\n",
            "--- Use Cases for RDDs after DataFrame Creation ---\n",
            "\n",
            "While DataFrames are generally preferred for structured data due to optimizations, there are scenarios where working with the underlying RDD might be necessary or more convenient:\n",
            "\n",
            "1. Low-level Transformations: For complex transformations that are difficult or inefficient to express using the DataFrame API or Spark SQL functions. RDDs provide full control over each element's processing.\n",
            "   Example: Applying a complex, non-standard algorithm to each record that involves operations not supported by built-in DataFrame functions.\n",
            "\n",
            "2. Unstructured Data Processing: If you need to switch to processing unstructured or semi-structured parts of your data (e.g., log files) that don't fit neatly into a DataFrame schema for a specific step.\n",
            "\n",
            "3. Legacy Code Integration: Integrating with existing Spark code that is based on the RDD API.\n",
            "\n",
            "4. Custom Partitioning or Control: When you need very fine-grained control over data partitioning or other low-level aspects of processing that are not exposed through the DataFrame API.\n",
            "\n",
            "Example (conceptual) - using RDD map for a complex, non-standard calculation per row:\n",
            "While `withColumn` and UDFs cover many cases, for highly specialized per-row logic, dropping to RDD might sometimes be considered if other methods are too cumbersome or perform poorly for that specific task.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2edbead"
      },
      "source": [
        "## Explain json and parquet file formats\n",
        "\n",
        "### Subtask:\n",
        "Describe the JSON and Parquet file formats, their benefits for Spark, and provide code examples for reading and writing data in these formats using Spark.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3db4e97a"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain JSON and Parquet file formats, their benefits for Spark, and provide code examples for reading and writing data in these formats using Spark DataFrames, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "3f62df6c",
        "outputId": "65bf0bf6-4e67-413e-a5fb-adbaab92a14a"
      },
      "source": [
        "# Recreate SparkSession if it's stopped\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"FileFormatsExample\").getOrCreate()\n",
        "\n",
        "import os\n",
        "\n",
        "# Define file paths\n",
        "json_file_path = \"/tmp/sample_data.json\"\n",
        "parquet_file_path = \"/tmp/sample_data.parquet\"\n",
        "\n",
        "# 1. Explain what JSON and Parquet file formats are.\n",
        "print(\"--- JSON and Parquet File Formats ---\")\n",
        "print(\"\"\"\n",
        "JSON (JavaScript Object Notation):\n",
        "JSON is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate. It is a text format that is completely language independent. JSON is built on two structures:\n",
        "- A collection of name/value pairs (e.g., a Python dictionary, Java object, struct, record, struct, keyed list, hash table, or associative array).\n",
        "- An ordered list of values (e.g., a Python list, array, vector, or sequence).\n",
        "JSON is commonly used for transmitting data in web applications and for storing semi-structured data.\n",
        "\n",
        "Parquet:\n",
        "Parquet is an open-source columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or version of Hadoop. It is designed for efficient data storage and retrieval, especially for large datasets. In a columnar format, values of the same column are stored together, which is different from row-based formats (like CSV or JSON) where values of the same row are stored together.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Describe the benefits of using JSON and Parquet formats with Spark, specifically highlighting the advantages of Parquet (columnar storage, schema evolution, compression).\n",
        "print(\"\\n--- Benefits for Spark ---\")\n",
        "print(\"\"\"\n",
        "Benefits of JSON with Spark:\n",
        "- Flexibility: JSON's semi-structured nature makes it easy to handle data with varying or evolving schemas.\n",
        "- Readability: Being a text format, JSON files are human-readable, which can be helpful for debugging and inspection.\n",
        "- Wide Adoption: JSON is a very popular format and is widely used, making integration with various systems straightforward.\n",
        "\n",
        "Benefits of Parquet with Spark (specifically highlighting advantages):\n",
        "- Columnar Storage: This is the most significant advantage for analytical workloads. When you query only a subset of columns, Spark can read only the required columns from storage, drastically reducing I/O and improving performance. This is in contrast to row-based formats where Spark would have to read the entire row.\n",
        "- Schema Evolution: Parquet supports schema evolution, allowing you to add, remove, or modify columns over time without breaking compatibility with older data files. This is crucial in dynamic data environments.\n",
        "- Compression: Parquet supports various compression codecs (like Snappy, Gzip, LZO), which significantly reduce the storage space required and further improve I/O performance. Columnar storage also lends itself well to higher compression ratios because values within a column are of the same data type and often have similar patterns.\n",
        "- Optimized for Analytical Queries: The columnar nature and compression make Parquet highly efficient for analytical queries that typically read specific columns across many rows (e.g., aggregations, filtering).\n",
        "- Integration with Spark SQL and Catalyst: Spark's Catalyst optimizer is highly optimized for working with columnar formats like Parquet, enabling efficient query execution plans.\n",
        "\"\"\")\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "data = [(\"Alice\", 1, \"New York\"),\n",
        "        (\"Bob\", 2, \"Los Angeles\"),\n",
        "        (\"Charlie\", 3, \"Chicago\")]\n",
        "columns = [\"Name\", \"ID\", \"City\"]\n",
        "df_sample = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"\\nSample DataFrame:\")\n",
        "df_sample.show()\n",
        "\n",
        "# 3. Provide code examples for writing a Spark DataFrame to a JSON file.\n",
        "print(\"\\n--- Writing DataFrame to JSON ---\")\n",
        "try:\n",
        "    df_sample.write.mode(\"overwrite\").json(json_file_path)\n",
        "    print(f\"DataFrame successfully written to JSON at: {json_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error writing DataFrame to JSON: {e}\")\n",
        "\n",
        "\n",
        "# 4. Provide code examples for reading data from a JSON file into a Spark DataFrame.\n",
        "print(\"\\n--- Reading Data from JSON ---\")\n",
        "try:\n",
        "    df_json = spark.read.json(json_file_path)\n",
        "    print(\"DataFrame successfully read from JSON:\")\n",
        "    df_json.show()\n",
        "    df_json.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading data from JSON: {e}\")\n",
        "\n",
        "\n",
        "# 5. Provide code examples for writing a Spark DataFrame to a Parquet file.\n",
        "print(\"\\n--- Writing DataFrame to Parquet ---\")\n",
        "try:\n",
        "    df_sample.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
        "    print(f\"DataFrame successfully written to Parquet at: {parquet_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error writing DataFrame to Parquet: {e}\")\n",
        "\n",
        "\n",
        "# 6. Provide code examples for reading data from a Parquet file into a Spark DataFrame.\n",
        "print(\"\\n--- Reading Data from Parquet ---\")\n",
        "try:\n",
        "    df_parquet = spark.read.parquet(parquet_file_path)\n",
        "    print(\"DataFrame successfully read from Parquet:\")\n",
        "    df_parquet.show()\n",
        "    df_parquet.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading data from Parquet: {e}\")\n",
        "\n",
        "# Clean up created files (optional)\n",
        "# import shutil\n",
        "# if os.path.exists(json_file_path):\n",
        "#     shutil.rmtree(json_file_path)\n",
        "# if os.path.exists(parquet_file_path):\n",
        "#     shutil.rmtree(parquet_file_path)\n",
        "\n",
        "# Stop the SparkSession (optional, if this is the last step)\n",
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- JSON and Parquet File Formats ---\n",
            "\n",
            "JSON (JavaScript Object Notation):\n",
            "JSON is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate. It is a text format that is completely language independent. JSON is built on two structures:\n",
            "- A collection of name/value pairs (e.g., a Python dictionary, Java object, struct, record, struct, keyed list, hash table, or associative array).\n",
            "- An ordered list of values (e.g., a Python list, array, vector, or sequence).\n",
            "JSON is commonly used for transmitting data in web applications and for storing semi-structured data.\n",
            "\n",
            "Parquet:\n",
            "Parquet is an open-source columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or version of Hadoop. It is designed for efficient data storage and retrieval, especially for large datasets. In a columnar format, values of the same column are stored together, which is different from row-based formats (like CSV or JSON) where values of the same row are stored together.\n",
            "\n",
            "\n",
            "--- Benefits for Spark ---\n",
            "\n",
            "Benefits of JSON with Spark:\n",
            "- Flexibility: JSON's semi-structured nature makes it easy to handle data with varying or evolving schemas.\n",
            "- Readability: Being a text format, JSON files are human-readable, which can be helpful for debugging and inspection.\n",
            "- Wide Adoption: JSON is a very popular format and is widely used, making integration with various systems straightforward.\n",
            "\n",
            "Benefits of Parquet with Spark (specifically highlighting advantages):\n",
            "- Columnar Storage: This is the most significant advantage for analytical workloads. When you query only a subset of columns, Spark can read only the required columns from storage, drastically reducing I/O and improving performance. This is in contrast to row-based formats where Spark would have to read the entire row.\n",
            "- Schema Evolution: Parquet supports schema evolution, allowing you to add, remove, or modify columns over time without breaking compatibility with older data files. This is crucial in dynamic data environments.\n",
            "- Compression: Parquet supports various compression codecs (like Snappy, Gzip, LZO), which significantly reduce the storage space required and further improve I/O performance. Columnar storage also lends itself well to higher compression ratios because values within a column are of the same data type and often have similar patterns.\n",
            "- Optimized for Analytical Queries: The columnar nature and compression make Parquet highly efficient for analytical queries that typically read specific columns across many rows (e.g., aggregations, filtering).\n",
            "- Integration with Spark SQL and Catalyst: Spark's Catalyst optimizer is highly optimized for working with columnar formats like Parquet, enabling efficient query execution plans.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'sc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2928651791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         (\"Charlie\", 3, \"Chicago\")]\n\u001b[1;32m     47\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"City\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mdf_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSample DataFrame:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# convert python objects to sql data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0minternal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtupled_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minternal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \"\"\"\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8286821"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the SparkSession or SparkContext was not properly initialized. The traceback indicates an `AttributeError: 'NoneType' object has no attribute 'sc'`. This means `spark.sparkContext` is None. Although there was a check for `spark`'s existence, it seems the session was still in an invalid state. I will regenerate the code block, ensuring that a new SparkSession is created explicitly at the beginning to guarantee a valid context for creating DataFrames and performing file operations. I will also keep the file path definitions and the core logic for explaining and demonstrating JSON and Parquet operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ngtcx4TPkdf",
        "outputId": "b72ff11a-9cdd-45bd-825f-e4bdaf117324"
      },
      "source": [
        "# Explicitly create a new SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FileFormatsExample\").getOrCreate()\n",
        "\n",
        "import os\n",
        "\n",
        "# Define file paths\n",
        "json_file_path = \"/tmp/sample_data.json\"\n",
        "parquet_file_path = \"/tmp/sample_data.parquet\"\n",
        "\n",
        "# 1. Explain what JSON and Parquet file formats are.\n",
        "print(\"--- JSON and Parquet File Formats ---\")\n",
        "print(\"\"\"\n",
        "JSON (JavaScript Object Notation):\n",
        "JSON is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate. It is a text format that is completely language independent. JSON is built on two structures:\n",
        "- A collection of name/value pairs (e.g., a Python dictionary, Java object, struct, record, struct, keyed list, hash table, or associative array).\n",
        "- An ordered list of values (e.g., a Python list, array, vector, or sequence).\n",
        "JSON is commonly used for transmitting data in web applications and for storing semi-structured data.\n",
        "\n",
        "Parquet:\n",
        "Parquet is an open-source columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or version of Hadoop. It is designed for efficient data storage and retrieval, especially for large datasets. In a columnar format, values of the same column are stored together, which is different from row-based formats (like CSV or JSON) where values of the same row are stored together.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Describe the benefits of using JSON and Parquet formats with Spark, specifically highlighting the advantages of Parquet (columnar storage, schema evolution, compression).\n",
        "print(\"\\n--- Benefits for Spark ---\")\n",
        "print(\"\"\"\n",
        "Benefits of JSON with Spark:\n",
        "- Flexibility: JSON's semi-structured nature makes it easy to handle data with varying or evolving schemas.\n",
        "- Readability: Being a text format, JSON files are human-readable, which can be helpful for debugging and inspection.\n",
        "- Wide Adoption: JSON is a very popular format and is widely used, making integration with various systems straightforward.\n",
        "\n",
        "Benefits of Parquet with Spark (specifically highlighting advantages):\n",
        "- Columnar Storage: This is the most significant advantage for analytical workloads. When you query only a subset of columns, Spark can read only the required columns from storage, drastically reducing I/O and improving performance. This is in contrast to row-based formats where Spark would have to read the entire row.\n",
        "- Schema Evolution: Parquet supports schema evolution, allowing you to add, remove, or modify columns over time without breaking compatibility with older data files. This is crucial in dynamic data environments.\n",
        "- Compression: Parquet supports various compression codecs (like Snappy, Gzip, LZO), which significantly reduce the storage space required and further improve I/O performance. Columnar storage also lends itself well to higher compression ratios because values within a column are of the same data type and often have similar patterns.\n",
        "- Optimized for Analytical Queries: The columnar nature and compression make Parquet highly efficient for analytical queries that typically read specific columns across many rows (e.g., aggregations, filtering).\n",
        "- Integration with Spark SQL and Catalyst: Spark's Catalyst optimizer is highly optimized for working with columnar formats like Parquet, enabling efficient query execution plans.\n",
        "\"\"\")\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "data = [(\"Alice\", 1, \"New York\"),\n",
        "        (\"Bob\", 2, \"Los Angeles\"),\n",
        "        (\"Charlie\", 3, \"Chicago\")]\n",
        "columns = [\"Name\", \"ID\", \"City\"]\n",
        "df_sample = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"\\nSample DataFrame:\")\n",
        "df_sample.show()\n",
        "\n",
        "# 3. Provide code examples for writing a Spark DataFrame to a JSON file.\n",
        "print(\"\\n--- Writing DataFrame to JSON ---\")\n",
        "try:\n",
        "    # Ensure directory exists for JSON file\n",
        "    json_dir = os.path.dirname(json_file_path)\n",
        "    if json_dir and not os.path.exists(json_dir):\n",
        "        os.makedirs(json_dir)\n",
        "    df_sample.write.mode(\"overwrite\").json(json_file_path)\n",
        "    print(f\"DataFrame successfully written to JSON at: {json_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error writing DataFrame to JSON: {e}\")\n",
        "\n",
        "\n",
        "# 4. Provide code examples for reading data from a JSON file into a Spark DataFrame.\n",
        "print(\"\\n--- Reading Data from JSON ---\")\n",
        "try:\n",
        "    df_json = spark.read.json(json_file_path)\n",
        "    print(\"DataFrame successfully read from JSON:\")\n",
        "    df_json.show()\n",
        "    df_json.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading data from JSON: {e}\")\n",
        "\n",
        "\n",
        "# 5. Provide code examples for writing a Spark DataFrame to a Parquet file.\n",
        "print(\"\\n--- Writing DataFrame to Parquet ---\")\n",
        "try:\n",
        "    # Ensure directory exists for Parquet file\n",
        "    parquet_dir = os.path.dirname(parquet_file_path)\n",
        "    if parquet_dir and not os.path.exists(parquet_dir):\n",
        "        os.makedirs(parquet_dir)\n",
        "    df_sample.write.mode(\"overwrite\").parquet(parquet_file_path)\n",
        "    print(f\"DataFrame successfully written to Parquet at: {parquet_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error writing DataFrame to Parquet: {e}\")\n",
        "\n",
        "\n",
        "# 6. Provide code examples for reading data from a Parquet file into a Spark DataFrame.\n",
        "print(\"\\n--- Reading Data from Parquet ---\")\n",
        "try:\n",
        "    df_parquet = spark.read.parquet(parquet_file_path)\n",
        "    print(\"DataFrame successfully read from Parquet:\")\n",
        "    df_parquet.show()\n",
        "    df_parquet.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading data from Parquet: {e}\")\n",
        "\n",
        "# Clean up created files (optional)\n",
        "# import shutil\n",
        "# if os.path.exists(json_file_path):\n",
        "#     shutil.rmtree(json_file_path)\n",
        "# if os.path.exists(parquet_file_path):\n",
        "#     shutil.rmtree(parquet_file_path)\n",
        "\n",
        "# Stop the SparkSession (optional, if this is the last step)\n",
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- JSON and Parquet File Formats ---\n",
            "\n",
            "JSON (JavaScript Object Notation):\n",
            "JSON is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate. It is a text format that is completely language independent. JSON is built on two structures:\n",
            "- A collection of name/value pairs (e.g., a Python dictionary, Java object, struct, record, struct, keyed list, hash table, or associative array).\n",
            "- An ordered list of values (e.g., a Python list, array, vector, or sequence).\n",
            "JSON is commonly used for transmitting data in web applications and for storing semi-structured data.\n",
            "\n",
            "Parquet:\n",
            "Parquet is an open-source columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or version of Hadoop. It is designed for efficient data storage and retrieval, especially for large datasets. In a columnar format, values of the same column are stored together, which is different from row-based formats (like CSV or JSON) where values of the same row are stored together.\n",
            "\n",
            "\n",
            "--- Benefits for Spark ---\n",
            "\n",
            "Benefits of JSON with Spark:\n",
            "- Flexibility: JSON's semi-structured nature makes it easy to handle data with varying or evolving schemas.\n",
            "- Readability: Being a text format, JSON files are human-readable, which can be helpful for debugging and inspection.\n",
            "- Wide Adoption: JSON is a very popular format and is widely used, making integration with various systems straightforward.\n",
            "\n",
            "Benefits of Parquet with Spark (specifically highlighting advantages):\n",
            "- Columnar Storage: This is the most significant advantage for analytical workloads. When you query only a subset of columns, Spark can read only the required columns from storage, drastically reducing I/O and improving performance. This is in contrast to row-based formats where Spark would have to read the entire row.\n",
            "- Schema Evolution: Parquet supports schema evolution, allowing you to add, remove, or modify columns over time without breaking compatibility with older data files. This is crucial in dynamic data environments.\n",
            "- Compression: Parquet supports various compression codecs (like Snappy, Gzip, LZO), which significantly reduce the storage space required and further improve I/O performance. Columnar storage also lends itself well to higher compression ratios because values within a column are of the same data type and often have similar patterns.\n",
            "- Optimized for Analytical Queries: The columnar nature and compression make Parquet highly efficient for analytical queries that typically read specific columns across many rows (e.g., aggregations, filtering).\n",
            "- Integration with Spark SQL and Catalyst: Spark's Catalyst optimizer is highly optimized for working with columnar formats like Parquet, enabling efficient query execution plans.\n",
            "\n",
            "\n",
            "Sample DataFrame:\n",
            "+-------+---+-----------+\n",
            "|   Name| ID|       City|\n",
            "+-------+---+-----------+\n",
            "|  Alice|  1|   New York|\n",
            "|    Bob|  2|Los Angeles|\n",
            "|Charlie|  3|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n",
            "\n",
            "--- Writing DataFrame to JSON ---\n",
            "DataFrame successfully written to JSON at: /tmp/sample_data.json\n",
            "\n",
            "--- Reading Data from JSON ---\n",
            "DataFrame successfully read from JSON:\n",
            "+-----------+---+-------+\n",
            "|       City| ID|   Name|\n",
            "+-----------+---+-------+\n",
            "|Los Angeles|  2|    Bob|\n",
            "|    Chicago|  3|Charlie|\n",
            "|   New York|  1|  Alice|\n",
            "+-----------+---+-------+\n",
            "\n",
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- ID: long (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            "\n",
            "\n",
            "--- Writing DataFrame to Parquet ---\n",
            "DataFrame successfully written to Parquet at: /tmp/sample_data.parquet\n",
            "\n",
            "--- Reading Data from Parquet ---\n",
            "DataFrame successfully read from Parquet:\n",
            "+-------+---+-----------+\n",
            "|   Name| ID|       City|\n",
            "+-------+---+-----------+\n",
            "|  Alice|  1|   New York|\n",
            "|    Bob|  2|Los Angeles|\n",
            "|Charlie|  3|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- ID: long (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "576e6674"
      },
      "source": [
        "## Explain loading data through different sources\n",
        "\n",
        "### Subtask:\n",
        "Show how to load data into Spark DataFrames from various sources like CSV, JSON, Parquet, and potentially databases, with code examples for each.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063f0dec"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the general approach for loading data using `spark.read` and provide code examples for loading data from CSV, JSON, and Parquet files as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ed3e3a4",
        "outputId": "e6dee965-5bed-4dce-be8b-7185ca7e5d2a"
      },
      "source": [
        "# Recreate SparkSession if it's stopped\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"LoadingDataSources\").getOrCreate()\n",
        "\n",
        "import os\n",
        "\n",
        "# Define file paths for demonstration\n",
        "csv_file_path = \"/tmp/sample_data.csv\"\n",
        "json_file_path = \"/tmp/sample_data.json\" # Reusing the path from previous step\n",
        "parquet_file_path = \"/tmp/sample_data.parquet\" # Reusing the path from previous step\n",
        "\n",
        "# Create dummy data files for demonstration if they don't exist\n",
        "# CSV\n",
        "csv_data = \"Name,ID,City\\nAlice,1,New York\\nBob,2,Los Angeles\\nCharlie,3,Chicago\"\n",
        "csv_dir = os.path.dirname(csv_file_path)\n",
        "if csv_dir and not os.path.exists(csv_dir):\n",
        "    os.makedirs(csv_dir)\n",
        "with open(csv_file_path, \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "# JSON (Assuming this was created in the previous step, if not, create it)\n",
        "# For robustness, let's ensure it exists or create a simple one\n",
        "if not os.path.exists(json_file_path):\n",
        "    json_data = [{\"Name\": \"Alice\", \"ID\": 1, \"City\": \"New York\"},\n",
        "                 {\"Name\": \"Bob\", \"ID\": 2, \"City\": \"Los Angeles\"},\n",
        "                 {\"Name\": \"Charlie\", \"ID\": 3, \"City\": \"Chicago\"}]\n",
        "    import json\n",
        "    with open(json_file_path, \"w\") as f:\n",
        "        for entry in json_data:\n",
        "            f.write(json.dumps(entry) + \"\\n\") # Write as newline-delimited JSON\n",
        "\n",
        "# Parquet (Assuming this was created in the previous step, if not, create a dummy file)\n",
        "# Creating a dummy parquet file is more complex, relying on the previous step is better.\n",
        "# If the previous step failed to create it, the read operation below will fail, which is expected for the demo.\n",
        "\n",
        "\n",
        "# 1. Explain the general approach in Spark for loading data from different sources using the spark.read interface.\n",
        "print(\"--- Loading Data using spark.read ---\")\n",
        "print(\"\"\"\n",
        "In Spark SQL, the primary way to load data from various external sources into a DataFrame is by using the `spark.read` interface. This interface provides a set of methods corresponding to different data formats and sources (e.g., `csv()`, `json()`, `parquet()`, `jdbc()`, `orc()`, `text()`, `load()`).\n",
        "\n",
        "The general syntax is:\n",
        "`spark.read.format(\"format_name\").option(\"option_name\", \"option_value\").load(\"path/to/data\")`\n",
        "\n",
        "Or, for common formats, there are direct methods:\n",
        "`spark.read.csv(\"path/to/data\", options...)`\n",
        "`spark.read.json(\"path/to/data\")`\n",
        "`spark.read.parquet(\"path/to/data\")`\n",
        "\n",
        "The `spark.read` interface allows you to specify the data source format, provide options specific to that format (like header presence, schema, delimiters for CSV; schema for JSON; etc.), and specify the path(s) to the data files or location. Spark then uses the appropriate data source connector to read the data and create a DataFrame with the inferred or specified schema.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Provide a code example demonstrating how to load data from a CSV file into a DataFrame.\n",
        "print(\"\\n--- Loading Data from CSV ---\")\n",
        "try:\n",
        "    # Example 1: Loading CSV with header and schema inference\n",
        "    print(\"Loading CSV with header and schema inference:\")\n",
        "    df_csv_inferred = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "    df_csv_inferred.show()\n",
        "    df_csv_inferred.printSchema()\n",
        "\n",
        "    # Example 2: Loading CSV with explicit schema\n",
        "    print(\"\\nLoading CSV with explicit schema:\")\n",
        "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "    csv_schema = StructType([\n",
        "        StructField(\"Name\", StringType(), True),\n",
        "        StructField(\"ID\", IntegerType(), True),\n",
        "        StructField(\"City\", StringType(), True)\n",
        "    ])\n",
        "    df_csv_explicit = spark.read.csv(csv_file_path, header=True, schema=csv_schema)\n",
        "    df_csv_explicit.show()\n",
        "    df_csv_explicit.printSchema()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data from CSV: {e}\")\n",
        "\n",
        "\n",
        "# 3. Provide a code example demonstrating how to load data from a JSON file into a DataFrame.\n",
        "print(\"\\n--- Loading Data from JSON ---\")\n",
        "try:\n",
        "    # Spark can automatically infer the schema for JSON files\n",
        "    df_json = spark.read.json(json_file_path)\n",
        "    print(\"DataFrame successfully read from JSON:\")\n",
        "    df_json.show()\n",
        "    df_json.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data from JSON: {e}\")\n",
        "\n",
        "\n",
        "# 4. Provide a code example demonstrating how to load data from a Parquet file into a DataFrame.\n",
        "print(\"\\n--- Loading Data from Parquet ---\")\n",
        "try:\n",
        "    # Parquet stores the schema within the file metadata, so schema inference is automatic\n",
        "    df_parquet = spark.read.parquet(parquet_file_path)\n",
        "    print(\"DataFrame successfully read from Parquet:\")\n",
        "    df_parquet.show()\n",
        "    df_parquet.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data from Parquet: {e}\")\n",
        "\n",
        "\n",
        "# 5. Provide a code example demonstrating how to load data from a database using JDBC.\n",
        "print(\"\\n--- Loading Data from a Database using JDBC ---\")\n",
        "print(\"\"\"\n",
        "Loading data from a database requires the JDBC driver for the specific database and connection properties.\n",
        "You'll need to replace the placeholders with your actual database connection details.\n",
        "\"\"\")\n",
        "# Note: This example uses dummy placeholders. You need a running database and the correct driver.\n",
        "jdbc_url = \"jdbc:postgresql://your_database_host:5432/your_database_name\"\n",
        "jdbc_table = \"your_table_name\"\n",
        "jdbc_properties = {\n",
        "    \"user\": \"your_username\",\n",
        "    \"password\": \"your_password\",\n",
        "    \"driver\": \"org.postgresql.Driver\" # Example driver for PostgreSQL\n",
        "    # For MySQL, use \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
        "    # For SQL Server, use \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Example: Reading from a database table via JDBC\n",
        "    # df_jdbc = spark.read.jdbc(url=jdbc_url, table=jdbc_table, properties=jdbc_properties)\n",
        "    # print(f\"DataFrame successfully read from database table: {jdbc_table}\")\n",
        "    # df_jdbc.show()\n",
        "    # df_jdbc.printSchema()\n",
        "    print(\"JDBC loading example code is commented out. Uncomment and replace details to run.\")\n",
        "except Exception as e:\n",
        "     print(f\"Error loading data from database via JDBC: {e}\")\n",
        "     print(\"Please ensure you have the JDBC driver JAR in Spark's classpath and correct connection details.\")\n",
        "\n",
        "\n",
        "# 6. Briefly mention other potential data sources that Spark can connect to.\n",
        "print(\"\\n--- Other Potential Data Sources ---\")\n",
        "print(\"\"\"\n",
        "Spark's `spark.read` interface supports a wide range of other data sources, including:\n",
        "- ORC files (`spark.read.orc()`)\n",
        "- Avro files (`spark.read.format(\"avro\").load()`) - Requires spark-avro package\n",
        "- Text files (`spark.read.text()`)\n",
        "- Hive tables (`spark.read.table(\"hive_table_name\")`) - Requires Spark-Hive integration setup\n",
        "- Kafka, Kinesis, etc. (for streaming data, often using structured streaming with `spark.readStream`)\n",
        "- Various NoSQL databases and cloud storage systems via connectors.\n",
        "\"\"\")\n",
        "\n",
        "# Stop the SparkSession (optional, if this is the last step)\n",
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data using spark.read ---\n",
            "\n",
            "In Spark SQL, the primary way to load data from various external sources into a DataFrame is by using the `spark.read` interface. This interface provides a set of methods corresponding to different data formats and sources (e.g., `csv()`, `json()`, `parquet()`, `jdbc()`, `orc()`, `text()`, `load()`).\n",
            "\n",
            "The general syntax is:\n",
            "`spark.read.format(\"format_name\").option(\"option_name\", \"option_value\").load(\"path/to/data\")`\n",
            "\n",
            "Or, for common formats, there are direct methods:\n",
            "`spark.read.csv(\"path/to/data\", options...)`\n",
            "`spark.read.json(\"path/to/data\")`\n",
            "`spark.read.parquet(\"path/to/data\")`\n",
            "\n",
            "The `spark.read` interface allows you to specify the data source format, provide options specific to that format (like header presence, schema, delimiters for CSV; schema for JSON; etc.), and specify the path(s) to the data files or location. Spark then uses the appropriate data source connector to read the data and create a DataFrame with the inferred or specified schema.\n",
            "\n",
            "\n",
            "--- Loading Data from CSV ---\n",
            "Loading CSV with header and schema inference:\n",
            "+-------+---+-----------+\n",
            "|   Name| ID|       City|\n",
            "+-------+---+-----------+\n",
            "|  Alice|  1|   New York|\n",
            "|    Bob|  2|Los Angeles|\n",
            "|Charlie|  3|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            "\n",
            "\n",
            "Loading CSV with explicit schema:\n",
            "+-------+---+-----------+\n",
            "|   Name| ID|       City|\n",
            "+-------+---+-----------+\n",
            "|  Alice|  1|   New York|\n",
            "|    Bob|  2|Los Angeles|\n",
            "|Charlie|  3|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            "\n",
            "\n",
            "--- Loading Data from JSON ---\n",
            "DataFrame successfully read from JSON:\n",
            "+-----------+---+-------+\n",
            "|       City| ID|   Name|\n",
            "+-----------+---+-------+\n",
            "|Los Angeles|  2|    Bob|\n",
            "|    Chicago|  3|Charlie|\n",
            "|   New York|  1|  Alice|\n",
            "+-----------+---+-------+\n",
            "\n",
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- ID: long (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            "\n",
            "\n",
            "--- Loading Data from Parquet ---\n",
            "DataFrame successfully read from Parquet:\n",
            "+-------+---+-----------+\n",
            "|   Name| ID|       City|\n",
            "+-------+---+-----------+\n",
            "|  Alice|  1|   New York|\n",
            "|    Bob|  2|Los Angeles|\n",
            "|Charlie|  3|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- ID: long (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            "\n",
            "\n",
            "--- Loading Data from a Database using JDBC ---\n",
            "\n",
            "Loading data from a database requires the JDBC driver for the specific database and connection properties.\n",
            "You'll need to replace the placeholders with your actual database connection details.\n",
            "\n",
            "JDBC loading example code is commented out. Uncomment and replace details to run.\n",
            "\n",
            "--- Other Potential Data Sources ---\n",
            "\n",
            "Spark's `spark.read` interface supports a wide range of other data sources, including:\n",
            "- ORC files (`spark.read.orc()`)\n",
            "- Avro files (`spark.read.format(\"avro\").load()`) - Requires spark-avro package\n",
            "- Text files (`spark.read.text()`)\n",
            "- Hive tables (`spark.read.table(\"hive_table_name\")`) - Requires Spark-Hive integration setup\n",
            "- Kafka, Kinesis, etc. (for streaming data, often using structured streaming with `spark.readStream`)\n",
            "- Various NoSQL databases and cloud storage systems via connectors.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc95a8f0"
      },
      "source": [
        "## Explain spark-hive integration\n",
        "\n",
        "### Subtask:\n",
        "Detail how to integrate Spark with Hive, enabling Spark to query data stored in Hive tables. Provide code examples for setting up and using Spark-Hive integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fdc32c"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain Spark-Hive integration, its benefits, prerequisites, and provide code examples for creating a SparkSession with Hive support, creating and querying a Hive table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7920f0bd",
        "outputId": "9c521157-2b9a-4742-a95e-44f591974ee3"
      },
      "source": [
        "# Explicitly create a new SparkSession with Hive support\n",
        "# To enable Hive support, you typically don't need to explicitly call enableHiveSupport()\n",
        "# if Spark is compiled with Hive support and your hive-site.xml is in Spark's classpath.\n",
        "# However, explicitly adding the configuration or calling enableHiveSupport() is good practice\n",
        "# to ensure it's enabled.\n",
        "# Also, ensure you have the necessary Hive dependencies in your Spark environment.\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"SparkHiveIntegration\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# 1. Explain the concept of Spark-Hive integration and its benefits.\n",
        "print(\"--- Spark-Hive Integration ---\")\n",
        "print(\"\"\"\n",
        "Spark-Hive integration allows Spark to interact with Hive, a data warehousing system built on top of Hadoop. Hive provides a structure to data stored in HDFS and allows querying this data using a SQL-like language called HiveQL.\n",
        "\n",
        "Benefits of Spark-Hive integration:\n",
        "- Unified Access: Users can query data stored in Hive tables directly from Spark using Spark SQL or the DataFrame API, without needing to write separate HiveQL scripts.\n",
        "- Performance: Spark's fast execution engine can significantly speed up queries on Hive data compared to using Hive's own MapReduce execution engine (though Hive can also use Tez or Spark).\n",
        "- Data Governance: Leverage Hive's metastore for schema management and data location, providing a centralized metadata repository for Spark.\n",
        "- Existing Infrastructure: Easily integrate Spark with existing data pipelines and data lakes built around Hive.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Describe the prerequisites for setting up Spark-Hive integration (e.g., Hive installation, hive-site.xml).\n",
        "print(\"\\n--- Prerequisites for Spark-Hive Integration ---\")\n",
        "print(\"\"\"\n",
        "To set up Spark-Hive integration, you typically need the following:\n",
        "\n",
        "1.  Hive Installation: A running Hive installation with a configured Hive Metastore. The Metastore stores the schema and location information for Hive tables.\n",
        "2.  `hive-site.xml`: The `hive-site.xml` configuration file from your Hive installation needs to be placed in Spark's configuration directory (`SPARK_HOME/conf`). This file contains critical information for Spark to connect to the Hive Metastore (e.g., Metastore URI, database connection details if using a relational database for the Metastore).\n",
        "3.  Hive Dependencies: Ensure that the necessary Hive client dependencies are available in Spark's classpath. If you are using a pre-compiled Spark distribution, it might already include these. If not, you might need to add them manually or use tools like `spark-submit` with `--packages` or `--jars`.\n",
        "4.  Hadoop Access: Spark needs access to the Hadoop Distributed File System (HDFS) or other storage where your Hive data is stored. This typically involves having Hadoop configuration files (`core-site.xml`, `hdfs-site.xml`) in Spark's configuration directory as well.\n",
        "\"\"\")\n",
        "\n",
        "# 3. Provide a code example showing how to create a SparkSession with Hive support enabled.\n",
        "print(\"\\n--- Creating SparkSession with Hive Support ---\")\n",
        "print(\"\"\"\n",
        "The `enableHiveSupport()` method on the SparkSession builder is used to enable Hive integration. When this is called, Spark attempts to connect to the Hive Metastore based on the configuration found in `hive-site.xml` (if present in SPARK_HOME/conf) or other Spark configurations.\n",
        "\"\"\")\n",
        "# The SparkSession is already created at the beginning of this code block with enableHiveSupport().\n",
        "print(\"SparkSession with Hive support created successfully (assuming prerequisites are met).\")\n",
        "print(spark)\n",
        "\n",
        "\n",
        "# 4. Demonstrate how to create a Hive table using Spark SQL (if a Hive metastore is accessible).\n",
        "print(\"\\n--- Creating a Hive Table using Spark SQL ---\")\n",
        "print(\"\"\"\n",
        "You can execute HiveQL DDL statements directly using `spark.sql()`.\n",
        "Note: This requires a working connection to a Hive Metastore and appropriate permissions.\n",
        "\"\"\")\n",
        "hive_table_name = \"spark_test_hive_table\"\n",
        "try:\n",
        "    # Drop table if it exists\n",
        "    spark.sql(f\"DROP TABLE IF EXISTS {hive_table_name}\")\n",
        "    print(f\"Dropped table if it existed: {hive_table_name}\")\n",
        "\n",
        "    # Create a new Hive table\n",
        "    spark.sql(f\"\"\"\n",
        "        CREATE TABLE {hive_table_name} (\n",
        "            name STRING,\n",
        "            id INT,\n",
        "            city STRING\n",
        "        )\n",
        "        ROW FORMAT DELIMITED\n",
        "        FIELDS TERMINATED BY ','\n",
        "        STORED AS TEXTFILE\n",
        "    \"\"\")\n",
        "    print(f\"Created Hive table: {hive_table_name}\")\n",
        "\n",
        "    # Verify table creation by showing tables\n",
        "    print(\"\\nShowing tables in Hive Metastore:\")\n",
        "    spark.sql(\"SHOW TABLES\").show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error creating Hive table: {e}\")\n",
        "    print(\"Please ensure your Hive Metastore is running and reachable, and hive-site.xml is configured correctly.\")\n",
        "\n",
        "\n",
        "# 5. Demonstrate how to load data into a Hive table using Spark DataFrames (if a Hive metastore is accessible).\n",
        "print(\"\\n--- Loading Data into a Hive Table ---\")\n",
        "print(\"\"\"\n",
        "You can load data into a Hive table by creating a Spark DataFrame and then using the `insertInto()` or `saveAsTable()` methods.\n",
        "\"\"\")\n",
        "try:\n",
        "    # Create a sample DataFrame\n",
        "    data = [(\"Alice\", 1, \"New York\"),\n",
        "            (\"Bob\", 2, \"Los Angeles\"),\n",
        "            (\"Charlie\", 3, \"Chicago\")]\n",
        "    columns = [\"name\", \"id\", \"city\"] # Column names must match Hive table case-insensitively\n",
        "    df_to_hive = spark.createDataFrame(data, columns)\n",
        "\n",
        "    print(\"Sample DataFrame to load into Hive:\")\n",
        "    df_to_hive.show()\n",
        "\n",
        "    # Load data into the Hive table\n",
        "    # Using insertInto (assumes table exists)\n",
        "    df_to_hive.write.mode(\"overwrite\").insertInto(hive_table_name)\n",
        "    print(f\"Data successfully loaded into Hive table: {hive_table_name}\")\n",
        "\n",
        "    # Alternatively, using saveAsTable which creates the table if it doesn't exist\n",
        "    # df_to_hive.write.mode(\"overwrite\").saveAsTable(\"another_hive_table\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data into Hive table: {e}\")\n",
        "    print(\"Ensure the Hive table exists and the DataFrame schema matches the table schema.\")\n",
        "\n",
        "\n",
        "# 6. Show how to query a Hive table using Spark SQL.\n",
        "print(\"\\n--- Querying a Hive Table using Spark SQL ---\")\n",
        "print(\"\"\"\n",
        "Once data is in a Hive table, you can query it using standard Spark SQL syntax.\n",
        "\"\"\")\n",
        "try:\n",
        "    # Query the Hive table\n",
        "    sql_query_result = spark.sql(f\"SELECT name, id FROM {hive_table_name} WHERE id > 1\")\n",
        "    print(f\"Querying Hive table: {hive_table_name}\")\n",
        "    sql_query_result.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error querying Hive table: {e}\")\n",
        "    print(\"Ensure the Hive table exists and contains data.\")\n",
        "\n",
        "\n",
        "# 7. Explain potential challenges or considerations when working with Spark and Hive together.\n",
        "print(\"\\n--- Challenges and Considerations for Spark-Hive Integration ---\")\n",
        "print(\"\"\"\n",
        "- Configuration: Correctly configuring `hive-site.xml` and ensuring Spark can access it is crucial and sometimes tricky.\n",
        "- Dependencies: Managing Hive client dependencies to avoid version conflicts can be challenging.\n",
        "- Performance Tuning: While Spark is faster than MapReduce, tuning Spark jobs for optimal performance on large Hive datasets still requires understanding data partitioning, shuffling, and caching.\n",
        "- Schema Evolution: While Parquet and ORC support schema evolution, managing schema changes in text-based Hive tables (like CSV) requires careful consideration.\n",
        "- Data Locality: Ensuring Spark tasks run on nodes close to the HDFS blocks storing the Hive data is important for performance.\n",
        "- Security: Integrating with Hadoop/Hive security mechanisms (like Kerberos) adds complexity.\n",
        "- Data Format: Performance is significantly better with columnar formats like Parquet and ORC compared to row-based formats like Text or SequenceFile.\n",
        "- Metastore Load: Frequent schema lookups from the Hive Metastore can become a bottleneck with a very large number of tables or partitions.\n",
        "\"\"\")\n",
        "\n",
        "# Stop the SparkSession (optional, if this is the last step)\n",
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark-Hive Integration ---\n",
            "\n",
            "Spark-Hive integration allows Spark to interact with Hive, a data warehousing system built on top of Hadoop. Hive provides a structure to data stored in HDFS and allows querying this data using a SQL-like language called HiveQL.\n",
            "\n",
            "Benefits of Spark-Hive integration:\n",
            "- Unified Access: Users can query data stored in Hive tables directly from Spark using Spark SQL or the DataFrame API, without needing to write separate HiveQL scripts.\n",
            "- Performance: Spark's fast execution engine can significantly speed up queries on Hive data compared to using Hive's own MapReduce execution engine (though Hive can also use Tez or Spark).\n",
            "- Data Governance: Leverage Hive's metastore for schema management and data location, providing a centralized metadata repository for Spark.\n",
            "- Existing Infrastructure: Easily integrate Spark with existing data pipelines and data lakes built around Hive.\n",
            "\n",
            "\n",
            "--- Prerequisites for Spark-Hive Integration ---\n",
            "\n",
            "To set up Spark-Hive integration, you typically need the following:\n",
            "\n",
            "1.  Hive Installation: A running Hive installation with a configured Hive Metastore. The Metastore stores the schema and location information for Hive tables.\n",
            "2.  `hive-site.xml`: The `hive-site.xml` configuration file from your Hive installation needs to be placed in Spark's configuration directory (`SPARK_HOME/conf`). This file contains critical information for Spark to connect to the Hive Metastore (e.g., Metastore URI, database connection details if using a relational database for the Metastore).\n",
            "3.  Hive Dependencies: Ensure that the necessary Hive client dependencies are available in Spark's classpath. If you are using a pre-compiled Spark distribution, it might already include these. If not, you might need to add them manually or use tools like `spark-submit` with `--packages` or `--jars`.\n",
            "4.  Hadoop Access: Spark needs access to the Hadoop Distributed File System (HDFS) or other storage where your Hive data is stored. This typically involves having Hadoop configuration files (`core-site.xml`, `hdfs-site.xml`) in Spark's configuration directory as well.\n",
            "\n",
            "\n",
            "--- Creating SparkSession with Hive Support ---\n",
            "\n",
            "The `enableHiveSupport()` method on the SparkSession builder is used to enable Hive integration. When this is called, Spark attempts to connect to the Hive Metastore based on the configuration found in `hive-site.xml` (if present in SPARK_HOME/conf) or other Spark configurations.\n",
            "\n",
            "SparkSession with Hive support created successfully (assuming prerequisites are met).\n",
            "<pyspark.sql.session.SparkSession object at 0x7cb85519e510>\n",
            "\n",
            "--- Creating a Hive Table using Spark SQL ---\n",
            "\n",
            "You can execute HiveQL DDL statements directly using `spark.sql()`.\n",
            "Note: This requires a working connection to a Hive Metastore and appropriate permissions.\n",
            "\n",
            "Dropped table if it existed: spark_test_hive_table\n",
            "Error creating Hive table: [NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n",
            "'CreateTable `spark_catalog`.`default`.`spark_test_hive_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
            "\n",
            "Please ensure your Hive Metastore is running and reachable, and hive-site.xml is configured correctly.\n",
            "\n",
            "--- Loading Data into a Hive Table ---\n",
            "\n",
            "You can load data into a Hive table by creating a Spark DataFrame and then using the `insertInto()` or `saveAsTable()` methods.\n",
            "\n",
            "Sample DataFrame to load into Hive:\n",
            "+-------+---+-----------+\n",
            "|   name| id|       city|\n",
            "+-------+---+-----------+\n",
            "|  Alice|  1|   New York|\n",
            "|    Bob|  2|Los Angeles|\n",
            "|Charlie|  3|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n",
            "Error loading data into Hive table: [TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_test_hive_table` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
            "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
            "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n",
            "'InsertIntoStatement 'UnresolvedRelation [spark_test_hive_table], [], false, true, false, false\n",
            "+- LogicalRDD [name#813, id#814L, city#815], false\n",
            "\n",
            "Ensure the Hive table exists and the DataFrame schema matches the table schema.\n",
            "\n",
            "--- Querying a Hive Table using Spark SQL ---\n",
            "\n",
            "Once data is in a Hive table, you can query it using standard Spark SQL syntax.\n",
            "\n",
            "Error querying Hive table: [TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_test_hive_table` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
            "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
            "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 21;\n",
            "'Project ['name, 'id]\n",
            "+- 'Filter ('id > 1)\n",
            "   +- 'UnresolvedRelation [spark_test_hive_table], [], false\n",
            "\n",
            "Ensure the Hive table exists and contains data.\n",
            "\n",
            "--- Challenges and Considerations for Spark-Hive Integration ---\n",
            "\n",
            "- Configuration: Correctly configuring `hive-site.xml` and ensuring Spark can access it is crucial and sometimes tricky.\n",
            "- Dependencies: Managing Hive client dependencies to avoid version conflicts can be challenging.\n",
            "- Performance Tuning: While Spark is faster than MapReduce, tuning Spark jobs for optimal performance on large Hive datasets still requires understanding data partitioning, shuffling, and caching.\n",
            "- Schema Evolution: While Parquet and ORC support schema evolution, managing schema changes in text-based Hive tables (like CSV) requires careful consideration.\n",
            "- Data Locality: Ensuring Spark tasks run on nodes close to the HDFS blocks storing the Hive data is important for performance.\n",
            "- Security: Integrating with Hadoop/Hive security mechanisms (like Kerberos) adds complexity.\n",
            "- Data Format: Performance is significantly better with columnar formats like Parquet and ORC compared to row-based formats like Text or SequenceFile.\n",
            "- Metastore Load: Frequent schema lookups from the Hive Metastore can become a bottleneck with a very large number of tables or partitions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f66aebf5"
      },
      "source": [
        "## Explain transformation mapping using pyspark dataframe\n",
        "\n",
        "### Subtask:\n",
        "Explain common PySpark DataFrame transformations and how to chain them to perform data manipulation and preparation, with practical code examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5d56cb1"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain Spark DataFrame transformations, their lazy evaluation, and demonstrate common transformations with code examples, including chaining.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cabd6692",
        "outputId": "38dc04ec-661f-45fa-dbc3-9b4c076feeb1"
      },
      "source": [
        "# Recreate SparkSession if it's stopped\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"DataFrameTransformations\").getOrCreate()\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 1. Explain what DataFrame transformations are in Spark and how they differ from actions (lazy evaluation).\n",
        "print(\"--- Spark DataFrame Transformations vs. Actions ---\")\n",
        "print(\"\"\"\n",
        "Spark DataFrame **Transformations** are operations that return a new DataFrame. They are **lazy**, meaning that they do not cause the computation to be executed immediately. Instead, they record the operations in a logical plan. Examples include `select`, `filter`, `withColumn`, `groupBy`, `orderBy`.\n",
        "\n",
        "**Actions**, on the other hand, are operations that trigger the execution of the transformations defined in the logical plan. They return a value to the driver program or write data to an external storage system. Examples include `show()`, `count()`, `collect()`, `write()`, `save()`.\n",
        "\n",
        "The lazy evaluation of transformations is a key feature of Spark. It allows Spark to optimize the execution plan by chaining together multiple transformations and executing them in a more efficient way when an action is called. This can involve techniques like predicate pushdown (filtering data early) or column pruning (reading only necessary columns).\n",
        "\"\"\")\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "data = [(\"Alice\", 1, \"New York\", 30),\n",
        "        (\"Bob\", 2, \"Los Angeles\", 35),\n",
        "        (\"Charlie\", 3, \"Chicago\", 40),\n",
        "        (\"Alice\", 4, \"New York\", 25),\n",
        "        (\"Bob\", 5, \"Los Angeles\", 32)]\n",
        "columns = [\"Name\", \"ID\", \"City\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"\\nOriginal DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# 2. Describe several common PySpark DataFrame transformations and\n",
        "# 3. Provide code examples demonstrating the usage of each of these common transformations individually.\n",
        "\n",
        "print(\"\\n--- Common PySpark DataFrame Transformations ---\")\n",
        "\n",
        "# Example: select - Selects columns from a DataFrame\n",
        "print(\"\\nExample: select()\")\n",
        "df.select(\"Name\", \"City\").show()\n",
        "df.select(df.Name, df.ID).show()\n",
        "df.select(F.col(\"Name\"), F.col(\"Age\")).show()\n",
        "\n",
        "# Example: filter - Filters rows based on a condition\n",
        "print(\"\\nExample: filter()\")\n",
        "df.filter(df.Age > 30).show()\n",
        "df.filter(\"City == 'New York'\").show() # Using SQL expression\n",
        "\n",
        "# Example: withColumn - Adds a new column or replaces an existing one\n",
        "print(\"\\nExample: withColumn()\")\n",
        "df.withColumn(\"ID_Doubled\", df.ID * 2).show()\n",
        "df.withColumn(\"Age_in_Months\", F.col(\"Age\") * 12).show()\n",
        "\n",
        "# Example: groupBy - Groups the DataFrame using the specified columns\n",
        "print(\"\\nExample: groupBy() and agg()\")\n",
        "df.groupBy(\"City\").count().show()\n",
        "df.groupBy(\"Name\").agg(F.avg(\"Age\").alias(\"Average_Age\")).show()\n",
        "\n",
        "# Example: orderBy - Sorts the DataFrame by the specified columns\n",
        "print(\"\\nExample: orderBy() or sort()\")\n",
        "df.orderBy(\"Age\").show()\n",
        "df.sort(F.desc(\"ID\")).show()\n",
        "\n",
        "# Example: drop - Drops the specified column(s) from a DataFrame\n",
        "print(\"\\nExample: drop()\")\n",
        "df.drop(\"ID\").show()\n",
        "df.drop(\"ID\", \"Age\").show()\n",
        "\n",
        "\n",
        "# 4. Show a practical example of chaining multiple transformations together\n",
        "print(\"\\n--- Chaining Multiple Transformations ---\")\n",
        "print(\"\"\"\n",
        "Transformations can be chained together to perform a sequence of data manipulation steps.\n",
        "Spark builds a logical plan for the entire chain and optimizes it before execution.\n",
        "\"\"\")\n",
        "\n",
        "# Example: Filter by City, add a new column based on Age, select specific columns, and order the result\n",
        "transformed_df = df.filter(df.City == \"New York\") \\\n",
        "                   .withColumn(\"Age_Group\", F.when(df.Age >= 30, \"Adult\").otherwise(\"Young\")) \\\n",
        "                   .select(\"Name\", \"City\", \"Age\", \"Age_Group\") \\\n",
        "                   .orderBy(F.desc(\"Age\"))\n",
        "\n",
        "print(\"Chained Transformations Result:\")\n",
        "transformed_df.show()\n",
        "\n",
        "\n",
        "# 5. Explain the concept of lazy evaluation in the context of transformations and when computation is triggered by an action.\n",
        "print(\"\\n--- Lazy Evaluation and Action Trigger ---\")\n",
        "print(\"\"\"\n",
        "As mentioned earlier, Spark transformations are lazy. When you call a transformation like `select` or `filter`, Spark doesn't immediately compute the result. Instead, it records the operation in a directed acyclic graph (DAG) of computations. This DAG represents the lineage of the DataFrame, describing how it should be derived from the source data.\n",
        "\n",
        "Computation is only triggered when an **action** is called. For example, calling `show()`, `count()`, `collect()`, or `write.parquet()` forces Spark to examine the DAG, optimize the plan (using the Catalyst optimizer), and then execute the physical plan across the cluster.\n",
        "\n",
        "Lazy evaluation allows Spark to:\n",
        "- **Optimize the execution plan:** By seeing the entire chain of transformations, Spark can reorder operations (e.g., pushing filters down to read less data) and combine operations (e.g., running multiple transformations in a single pass over the data) for efficiency.\n",
        "- **Minimize data shuffling:** Spark can optimize joins and aggregations to reduce the amount of data that needs to be moved between nodes.\n",
        "- **Fault tolerance:** The DAG acts as a recipe. If a node fails during execution, Spark can recompute the lost partitions using the lineage information without needing to re-read all the initial data.\n",
        "\n",
        "In the chained transformations example above, none of the intermediate DataFrames (`df.filter(...)`, `.withColumn(...)`, `.select(...)`) are computed until the final `.show()` action is called. Spark looks at the whole chain and figures out the most efficient way to get the final result.\n",
        "\"\"\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark DataFrame Transformations vs. Actions ---\n",
            "\n",
            "Spark DataFrame **Transformations** are operations that return a new DataFrame. They are **lazy**, meaning that they do not cause the computation to be executed immediately. Instead, they record the operations in a logical plan. Examples include `select`, `filter`, `withColumn`, `groupBy`, `orderBy`.\n",
            "\n",
            "**Actions**, on the other hand, are operations that trigger the execution of the transformations defined in the logical plan. They return a value to the driver program or write data to an external storage system. Examples include `show()`, `count()`, `collect()`, `write()`, `save()`.\n",
            "\n",
            "The lazy evaluation of transformations is a key feature of Spark. It allows Spark to optimize the execution plan by chaining together multiple transformations and executing them in a more efficient way when an action is called. This can involve techniques like predicate pushdown (filtering data early) or column pruning (reading only necessary columns).\n",
            "\n",
            "\n",
            "Original DataFrame:\n",
            "+-------+---+-----------+---+\n",
            "|   Name| ID|       City|Age|\n",
            "+-------+---+-----------+---+\n",
            "|  Alice|  1|   New York| 30|\n",
            "|    Bob|  2|Los Angeles| 35|\n",
            "|Charlie|  3|    Chicago| 40|\n",
            "|  Alice|  4|   New York| 25|\n",
            "|    Bob|  5|Los Angeles| 32|\n",
            "+-------+---+-----------+---+\n",
            "\n",
            "\n",
            "--- Common PySpark DataFrame Transformations ---\n",
            "\n",
            "Example: select()\n",
            "+-------+-----------+\n",
            "|   Name|       City|\n",
            "+-------+-----------+\n",
            "|  Alice|   New York|\n",
            "|    Bob|Los Angeles|\n",
            "|Charlie|    Chicago|\n",
            "|  Alice|   New York|\n",
            "|    Bob|Los Angeles|\n",
            "+-------+-----------+\n",
            "\n",
            "+-------+---+\n",
            "|   Name| ID|\n",
            "+-------+---+\n",
            "|  Alice|  1|\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "|  Alice|  4|\n",
            "|    Bob|  5|\n",
            "+-------+---+\n",
            "\n",
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 35|\n",
            "|Charlie| 40|\n",
            "|  Alice| 25|\n",
            "|    Bob| 32|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "Example: filter()\n",
            "+-------+---+-----------+---+\n",
            "|   Name| ID|       City|Age|\n",
            "+-------+---+-----------+---+\n",
            "|    Bob|  2|Los Angeles| 35|\n",
            "|Charlie|  3|    Chicago| 40|\n",
            "|    Bob|  5|Los Angeles| 32|\n",
            "+-------+---+-----------+---+\n",
            "\n",
            "+-----+---+--------+---+\n",
            "| Name| ID|    City|Age|\n",
            "+-----+---+--------+---+\n",
            "|Alice|  1|New York| 30|\n",
            "|Alice|  4|New York| 25|\n",
            "+-----+---+--------+---+\n",
            "\n",
            "\n",
            "Example: withColumn()\n",
            "+-------+---+-----------+---+----------+\n",
            "|   Name| ID|       City|Age|ID_Doubled|\n",
            "+-------+---+-----------+---+----------+\n",
            "|  Alice|  1|   New York| 30|         2|\n",
            "|    Bob|  2|Los Angeles| 35|         4|\n",
            "|Charlie|  3|    Chicago| 40|         6|\n",
            "|  Alice|  4|   New York| 25|         8|\n",
            "|    Bob|  5|Los Angeles| 32|        10|\n",
            "+-------+---+-----------+---+----------+\n",
            "\n",
            "+-------+---+-----------+---+-------------+\n",
            "|   Name| ID|       City|Age|Age_in_Months|\n",
            "+-------+---+-----------+---+-------------+\n",
            "|  Alice|  1|   New York| 30|          360|\n",
            "|    Bob|  2|Los Angeles| 35|          420|\n",
            "|Charlie|  3|    Chicago| 40|          480|\n",
            "|  Alice|  4|   New York| 25|          300|\n",
            "|    Bob|  5|Los Angeles| 32|          384|\n",
            "+-------+---+-----------+---+-------------+\n",
            "\n",
            "\n",
            "Example: groupBy() and agg()\n",
            "+-----------+-----+\n",
            "|       City|count|\n",
            "+-----------+-----+\n",
            "|Los Angeles|    2|\n",
            "|   New York|    2|\n",
            "|    Chicago|    1|\n",
            "+-----------+-----+\n",
            "\n",
            "+-------+-----------+\n",
            "|   Name|Average_Age|\n",
            "+-------+-----------+\n",
            "|    Bob|       33.5|\n",
            "|  Alice|       27.5|\n",
            "|Charlie|       40.0|\n",
            "+-------+-----------+\n",
            "\n",
            "\n",
            "Example: orderBy() or sort()\n",
            "+-------+---+-----------+---+\n",
            "|   Name| ID|       City|Age|\n",
            "+-------+---+-----------+---+\n",
            "|  Alice|  4|   New York| 25|\n",
            "|  Alice|  1|   New York| 30|\n",
            "|    Bob|  5|Los Angeles| 32|\n",
            "|    Bob|  2|Los Angeles| 35|\n",
            "|Charlie|  3|    Chicago| 40|\n",
            "+-------+---+-----------+---+\n",
            "\n",
            "+-------+---+-----------+---+\n",
            "|   Name| ID|       City|Age|\n",
            "+-------+---+-----------+---+\n",
            "|    Bob|  5|Los Angeles| 32|\n",
            "|  Alice|  4|   New York| 25|\n",
            "|Charlie|  3|    Chicago| 40|\n",
            "|    Bob|  2|Los Angeles| 35|\n",
            "|  Alice|  1|   New York| 30|\n",
            "+-------+---+-----------+---+\n",
            "\n",
            "\n",
            "Example: drop()\n",
            "+-------+-----------+---+\n",
            "|   Name|       City|Age|\n",
            "+-------+-----------+---+\n",
            "|  Alice|   New York| 30|\n",
            "|    Bob|Los Angeles| 35|\n",
            "|Charlie|    Chicago| 40|\n",
            "|  Alice|   New York| 25|\n",
            "|    Bob|Los Angeles| 32|\n",
            "+-------+-----------+---+\n",
            "\n",
            "+-------+-----------+\n",
            "|   Name|       City|\n",
            "+-------+-----------+\n",
            "|  Alice|   New York|\n",
            "|    Bob|Los Angeles|\n",
            "|Charlie|    Chicago|\n",
            "|  Alice|   New York|\n",
            "|    Bob|Los Angeles|\n",
            "+-------+-----------+\n",
            "\n",
            "\n",
            "--- Chaining Multiple Transformations ---\n",
            "\n",
            "Transformations can be chained together to perform a sequence of data manipulation steps.\n",
            "Spark builds a logical plan for the entire chain and optimizes it before execution.\n",
            "\n",
            "Chained Transformations Result:\n",
            "+-----+--------+---+---------+\n",
            "| Name|    City|Age|Age_Group|\n",
            "+-----+--------+---+---------+\n",
            "|Alice|New York| 30|    Adult|\n",
            "|Alice|New York| 25|    Young|\n",
            "+-----+--------+---+---------+\n",
            "\n",
            "\n",
            "--- Lazy Evaluation and Action Trigger ---\n",
            "\n",
            "As mentioned earlier, Spark transformations are lazy. When you call a transformation like `select` or `filter`, Spark doesn't immediately compute the result. Instead, it records the operation in a directed acyclic graph (DAG) of computations. This DAG represents the lineage of the DataFrame, describing how it should be derived from the source data.\n",
            "\n",
            "Computation is only triggered when an **action** is called. For example, calling `show()`, `count()`, `collect()`, or `write.parquet()` forces Spark to examine the DAG, optimize the plan (using the Catalyst optimizer), and then execute the physical plan across the cluster.\n",
            "\n",
            "Lazy evaluation allows Spark to:\n",
            "- **Optimize the execution plan:** By seeing the entire chain of transformations, Spark can reorder operations (e.g., pushing filters down to read less data) and combine operations (e.g., running multiple transformations in a single pass over the data) for efficiency.\n",
            "- **Minimize data shuffling:** Spark can optimize joins and aggregations to reduce the amount of data that needs to be moved between nodes.\n",
            "- **Fault tolerance:** The DAG acts as a recipe. If a node fails during execution, Spark can recompute the lost partitions using the lineage information without needing to re-read all the initial data.\n",
            "\n",
            "In the chained transformations example above, none of the intermediate DataFrames (`df.filter(...)`, `.withColumn(...)`, `.select(...)`) are computed until the final `.show()` action is called. Spark looks at the whole chain and figures out the most efficient way to get the final result.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f0dd141"
      },
      "source": [
        "## Explain advanced transformations & actions\n",
        "\n",
        "### Subtask:\n",
        "Cover more advanced Spark DataFrame transformations and actions, such as window functions, joins, aggregations, and explain their usage with code examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2741f506"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain advanced DataFrame transformations and actions, including window functions, joins, and aggregations, and provide code examples for each, along with a discussion of performance implications and other advanced actions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "014a2c94",
        "outputId": "06a94aed-1991-4db0-cc34-d3fe551e6e30"
      },
      "source": [
        "# Recreate SparkSession if it's stopped\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"AdvancedTransformationsActions\").getOrCreate()\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# 1. Explain advanced transformations and actions in Spark DataFrames.\n",
        "print(\"--- Advanced Spark DataFrame Transformations and Actions ---\")\n",
        "print(\"\"\"\n",
        "Building upon basic transformations like `select`, `filter`, and `withColumn`, Spark DataFrames offer more advanced operations for complex data processing tasks. These include:\n",
        "\n",
        "- **Window Functions:** Perform calculations across a set of table rows that are related to the current row, similar to aggregations but without collapsing the rows. Useful for ranking, calculating moving averages, cumulative sums, etc.\n",
        "- **Joins:** Combine two DataFrames based on common columns, allowing you to merge data from different sources. Various join types (inner, outer, left, right, etc.) are supported.\n",
        "- **Advanced Aggregations:** Beyond simple `groupBy().agg()`, Spark provides ways to perform more complex aggregations like pivoting data or using grouping sets.\n",
        "- **Advanced Actions:** Operations beyond `show()` that trigger computation and return results to the driver or write data, such as `collect()`, `take()`, `count()`, `save()`, `foreach()`.\n",
        "\n",
        "These advanced operations are essential for sophisticated data analysis, feature engineering, and preparing data for machine learning models. Spark's Catalyst optimizer plays a crucial role in optimizing these operations for distributed execution.\n",
        "\"\"\")\n",
        "\n",
        "# Create sample DataFrames for demonstration\n",
        "data_employees = [(\"Alice\", \"Sales\", 5000, \"2022-01-15\"),\n",
        "                  (\"Bob\", \"IT\", 6000, \"2022-01-15\"),\n",
        "                  (\"Charlie\", \"Sales\", 5500, \"2022-02-01\"),\n",
        "                  (\"David\", \"IT\", 6500, \"2022-02-01\"),\n",
        "                  (\"Eve\", \"Sales\", 5200, \"2022-03-10\"),\n",
        "                  (\"Frank\", \"IT\", 6200, \"2022-03-10\")]\n",
        "columns_employees = [\"Name\", \"Department\", \"Salary\", \"HireDate\"]\n",
        "df_employees = spark.createDataFrame(data_employees, columns_employees)\n",
        "\n",
        "data_departments = [(\"Sales\", \"Building A\"),\n",
        "                    (\"IT\", \"Building B\"),\n",
        "                    (\"HR\", \"Building C\")]\n",
        "columns_departments = [\"Department\", \"Location\"]\n",
        "df_departments = spark.createDataFrame(data_departments, columns_departments)\n",
        "\n",
        "print(\"\\nSample Employees DataFrame:\")\n",
        "df_employees.show()\n",
        "print(\"\\nSample Departments DataFrame:\")\n",
        "df_departments.show()\n",
        "\n",
        "# 2. Provide a code example demonstrating the use of Window Functions.\n",
        "print(\"\\n--- Window Functions ---\")\n",
        "print(\"\"\"\n",
        "Window functions perform calculations across a set of rows related to the current row.\n",
        "A window is defined using `Window.partitionBy()` (to group rows) and `orderBy()` (to order rows within each partition).\n",
        "\"\"\")\n",
        "\n",
        "# Define a window specification partitioned by Department and ordered by Salary descending\n",
        "window_spec_salary = Window.partitionBy(\"Department\").orderBy(F.desc(\"Salary\"))\n",
        "\n",
        "# Example: Rank employees within each department based on salary\n",
        "df_employees.withColumn(\"Rank_within_Dept\", F.rank().over(window_spec_salary)).show()\n",
        "\n",
        "# Example: Calculate average salary within each department\n",
        "df_employees.withColumn(\"Avg_Dept_Salary\", F.avg(\"Salary\").over(Window.partitionBy(\"Department\"))).show()\n",
        "\n",
        "# Example: Calculate a cumulative sum of salary within each department based on hire date\n",
        "window_spec_cumulative = Window.partitionBy(\"Department\").orderBy(\"HireDate\")\n",
        "df_employees.withColumn(\"Cumulative_Dept_Salary\", F.sum(\"Salary\").over(window_spec_cumulative)).show()\n",
        "\n",
        "\n",
        "# 3. Provide a code example demonstrating different types of DataFrame Joins.\n",
        "print(\"\\n--- DataFrame Joins ---\")\n",
        "print(\"\"\"\n",
        "Joins combine rows from two DataFrames based on a related column between them.\n",
        "\"\"\")\n",
        "\n",
        "# Example: Inner Join\n",
        "print(\"\\nInner Join (matching rows from both DataFrames):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"inner\").show()\n",
        "\n",
        "# Example: Left Outer Join (all rows from left, matched rows from right, nulls if no match)\n",
        "print(\"\\nLeft Outer Join (all employees, with department location if available):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"left_outer\").show()\n",
        "\n",
        "# Example: Right Outer Join (all rows from right, matched rows from left, nulls if no match)\n",
        "print(\"\\nRight Outer Join (all departments, with employee info if available):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"right_outer\").show()\n",
        "\n",
        "# Example: Full Outer Join (all rows from both DataFrames, with nulls where no match)\n",
        "print(\"\\nFull Outer Join (all employees and all departments):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"full_outer\").show()\n",
        "\n",
        "# Example: Joining on multiple columns (if applicable, demonstrating syntax)\n",
        "# Assuming df1 has columns (colA, colB) and df2 has columns (colC, colD)\n",
        "# df1.join(df2, (df1.colA == df2.colC) & (df1.colB == df2.colD), \"inner\").show()\n",
        "\n",
        "\n",
        "# 4. Provide a code example demonstrating advanced Aggregations.\n",
        "print(\"\\n--- Advanced Aggregations ---\")\n",
        "\n",
        "# Example: Using agg() with multiple aggregate functions\n",
        "print(\"\\nAggregating with multiple functions:\")\n",
        "df_employees.groupBy(\"Department\").agg(\n",
        "    F.count(\"*\").alias(\"Employee_Count\"),\n",
        "    F.avg(\"Salary\").alias(\"Average_Salary\"),\n",
        "    F.max(\"Salary\").alias(\"Max_Salary\")\n",
        ").show()\n",
        "\n",
        "# Example: Using pivot() - Rotates a column into the header\n",
        "print(\"\\nPivoting data (Average Salary by Department and a dummy category):\")\n",
        "# Add a dummy category column for pivoting example\n",
        "df_pivot_data = df_employees.withColumn(\"Category\", F.lit(\"A\"))\n",
        "df_pivot_data.groupBy(\"Department\").pivot(\"Category\").agg(F.avg(\"Salary\").alias(\"Avg_Salary\")).show()\n",
        "\n",
        "# Real-world pivot example might involve pivoting a date part or status\n",
        "# df_sales.groupBy(\"Product\").pivot(\"Year\").agg(F.sum(\"Revenue\")).show()\n",
        "\n",
        "\n",
        "# 5. Provide code examples for advanced Actions beyond show().\n",
        "print(\"\\n--- Advanced Actions ---\")\n",
        "print(\"\"\"\n",
        "Actions trigger the execution of the DataFrame plan.\n",
        "\"\"\")\n",
        "\n",
        "# Example: collect() - Returns all rows to the driver as a list of Row objects (use with caution on large DataFrames)\n",
        "print(\"\\nExample: collect() (first 3 rows to avoid flooding console):\")\n",
        "collected_rows = df_employees.collect()\n",
        "for i, row in enumerate(collected_rows):\n",
        "    if i < 3:\n",
        "        print(row)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Example: take(n) - Returns the first n rows to the driver as a list of Row objects\n",
        "print(\"\\nExample: take(2):\")\n",
        "taken_rows = df_employees.take(2)\n",
        "print(taken_rows)\n",
        "\n",
        "# Example: count() - Returns the number of rows in the DataFrame\n",
        "print(\"\\nExample: count():\")\n",
        "row_count = df_employees.count()\n",
        "print(f\"Total number of rows: {row_count}\")\n",
        "\n",
        "# Example: save() / write() - Writes the DataFrame to a file or storage system\n",
        "# Example: Saving to Parquet (assuming /tmp is accessible)\n",
        "output_parquet_path = \"/tmp/employees_output.parquet\"\n",
        "print(f\"\\nExample: save() / write() to Parquet at {output_parquet_path}\")\n",
        "try:\n",
        "    # Ensure directory exists\n",
        "    import os, shutil\n",
        "    output_dir = os.path.dirname(output_parquet_path)\n",
        "    if os.path.exists(output_parquet_path):\n",
        "        shutil.rmtree(output_parquet_path) # Clean up previous run\n",
        "    df_employees.write.mode(\"overwrite\").parquet(output_parquet_path)\n",
        "    print(\"DataFrame successfully saved to Parquet.\")\n",
        "    # Verify by reading back\n",
        "    df_read_back = spark.read.parquet(output_parquet_path)\n",
        "    print(\"Data read back from saved Parquet:\")\n",
        "    df_read_back.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error saving DataFrame to Parquet: {e}\")\n",
        "\n",
        "# Example: foreach() - Applies a function to each row in the DataFrame (executed on workers)\n",
        "print(\"\\nExample: foreach() (printing each employee name on worker nodes):\")\n",
        "# Note: Output of foreach will appear in worker logs, not necessarily driver console\n",
        "def print_employee_name(row):\n",
        "    # This function runs on the worker nodes\n",
        "    print(f\"Processing employee: {row['Name']}\")\n",
        "\n",
        "# df_employees.foreach(print_employee_name) # Uncomment to run foreach\n",
        "\n",
        "print(\"foreach() action demonstrated conceptually. Output appears in worker logs.\")\n",
        "\n",
        "\n",
        "# 6. Discuss performance implications and optimization techniques.\n",
        "print(\"\\n--- Performance Implications and Optimization ---\")\n",
        "print(\"\"\"\n",
        "Advanced operations like Joins and Aggregations can be computationally expensive and lead to significant data shuffling across the network, which is a major bottleneck in distributed computing.\n",
        "\n",
        "Performance Implications:\n",
        "- **Shuffling:** Joins and aggregations often require shuffling data so that related data (e.g., rows with the same join key or group by key) are brought together on the same partition. Shuffling involves writing data to disk and transferring it over the network.\n",
        "- **Data Skew:** If data is unevenly distributed (data skew), some partitions might become much larger than others, leading to bottlenecks as a few tasks take much longer to complete.\n",
        "- **Resource Consumption:** These operations can consume significant CPU, memory, and network resources.\n",
        "\n",
        "Optimization Strategies:\n",
        "- **Caching/Persisting:** Cache or persist DataFrames that are used multiple times, especially before expensive operations like joins or aggregations, to avoid recomputing them. Use `df.cache()` or `df.persist()`.\n",
        "- **Data Partitioning:** Ensure DataFrames are partitioned appropriately before joins or aggregations on the join/grouping keys. Spark can perform hash partitioning to distribute data evenly. The number of partitions affects parallelism and shuffling.\n",
        "- **Broadcasting Joins:** If one of the DataFrames in a join is significantly smaller than the other, Spark can \"broadcast\" the smaller DataFrame to all worker nodes. This avoids shuffling the larger DataFrame and can dramatically improve performance. Spark can often do this automatically, but you can hint using `F.broadcast(smaller_df)`.\n",
        "- **Optimizing Aggregations:** Use `groupBy().agg()` which is optimized. For highly skewed data, consider techniques like salting or using Spark's built-in handling for skew joins/aggregations (though manual intervention might sometimes be needed).\n",
        "- **Column Pruning and Predicate Pushdown:** Ensure filters are applied early in the transformation chain. Spark's optimizer automatically pushes down filters to read less data from the source, but understanding this helps in structuring your logic.\n",
        "- **Choosing the Right Join Type:** Select the join type that matches your requirements precisely to avoid unnecessary computations.\n",
        "- **Data Format:** Use columnar formats like Parquet or ORC which are optimized for reading only necessary columns, reducing I/O for operations that don't require all columns.\n",
        "- **Monitoring and Tuning:** Use the Spark UI to monitor job execution, identify bottlenecks (like skewed stages or high shuffle read/write), and tune configuration parameters (`spark.sql.shuffle.partitions`, memory settings, etc.).\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Stop the SparkSession (optional, if this is the last step)\n",
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Advanced Spark DataFrame Transformations and Actions ---\n",
            "\n",
            "Building upon basic transformations like `select`, `filter`, and `withColumn`, Spark DataFrames offer more advanced operations for complex data processing tasks. These include:\n",
            "\n",
            "- **Window Functions:** Perform calculations across a set of table rows that are related to the current row, similar to aggregations but without collapsing the rows. Useful for ranking, calculating moving averages, cumulative sums, etc.\n",
            "- **Joins:** Combine two DataFrames based on common columns, allowing you to merge data from different sources. Various join types (inner, outer, left, right, etc.) are supported.\n",
            "- **Advanced Aggregations:** Beyond simple `groupBy().agg()`, Spark provides ways to perform more complex aggregations like pivoting data or using grouping sets.\n",
            "- **Advanced Actions:** Operations beyond `show()` that trigger computation and return results to the driver or write data, such as `collect()`, `take()`, `count()`, `save()`, `foreach()`.\n",
            "\n",
            "These advanced operations are essential for sophisticated data analysis, feature engineering, and preparing data for machine learning models. Spark's Catalyst optimizer plays a crucial role in optimizing these operations for distributed execution.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'sc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1869613351.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                   (\"Frank\", \"IT\", 6200, \"2022-03-10\")]\n\u001b[1;32m     32\u001b[0m \u001b[0mcolumns_employees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Department\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Salary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HireDate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdf_employees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_employees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_employees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m data_departments = [(\"Sales\", \"Building A\"),\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# convert python objects to sql data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0minternal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtupled_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minternal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \"\"\"\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73e4230d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the SparkContext was not available when trying to create DataFrames. The SparkSession check at the beginning of the code block was insufficient. I need to ensure a valid SparkSession is created and available. I will regenerate the code block, ensuring the SparkSession creation logic is robust and always provides a valid session before proceeding with DataFrame operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "BfXw85x8QO40",
        "outputId": "c34f88a3-a3d0-4c5f-ce9b-a7116c2f91db"
      },
      "source": [
        "# Ensure SparkSession is created or retrieved\n",
        "try:\n",
        "    # Check if SparkSession is already running\n",
        "    spark\n",
        "except NameError:\n",
        "    # If not, create a new SparkSession\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"AdvancedTransformationsActions\").getOrCreate()\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# 1. Explain advanced transformations and actions in Spark DataFrames.\n",
        "print(\"--- Advanced Spark DataFrame Transformations and Actions ---\")\n",
        "print(\"\"\"\n",
        "Building upon basic transformations like `select`, `filter`, and `withColumn`, Spark DataFrames offer more advanced operations for complex data processing tasks. These include:\n",
        "\n",
        "- **Window Functions:** Perform calculations across a set of table rows that are related to the current row, similar to aggregations but without collapsing the rows. Useful for ranking, calculating moving averages, cumulative sums, etc.\n",
        "- **Joins:** Combine two DataFrames based on common columns, allowing you to merge data from different sources. Various join types (inner, outer, left, right, etc.) are supported.\n",
        "- **Advanced Aggregations:** Beyond simple `groupBy().agg()`, Spark provides ways to perform more complex aggregations like pivoting data or using grouping sets.\n",
        "- **Advanced Actions:** Operations beyond `show()` that trigger computation and return results to the driver or write data, such as `collect()`, `take()`, `count()`, `save()`, `foreach()`.\n",
        "\n",
        "These advanced operations are essential for sophisticated data analysis, feature engineering, and preparing data for machine learning models. Spark's Catalyst optimizer plays a crucial role in optimizing these operations for distributed execution.\n",
        "\"\"\")\n",
        "\n",
        "# Create sample DataFrames for demonstration\n",
        "data_employees = [(\"Alice\", \"Sales\", 5000, \"2022-01-15\"),\n",
        "                  (\"Bob\", \"IT\", 6000, \"2022-01-15\"),\n",
        "                  (\"Charlie\", \"Sales\", 5500, \"2022-02-01\"),\n",
        "                  (\"David\", \"IT\", 6500, \"2022-02-01\"),\n",
        "                  (\"Eve\", \"Sales\", 5200, \"2022-03-10\"),\n",
        "                  (\"Frank\", \"IT\", 6200, \"2022-03-10\")]\n",
        "columns_employees = [\"Name\", \"Department\", \"Salary\", \"HireDate\"]\n",
        "df_employees = spark.createDataFrame(data_employees, columns_employees)\n",
        "\n",
        "data_departments = [(\"Sales\", \"Building A\"),\n",
        "                    (\"IT\", \"Building B\"),\n",
        "                    (\"HR\", \"Building C\")]\n",
        "columns_departments = [\"Department\", \"Location\"]\n",
        "df_departments = spark.createDataFrame(data_departments, columns_departments)\n",
        "\n",
        "print(\"\\nSample Employees DataFrame:\")\n",
        "df_employees.show()\n",
        "print(\"\\nSample Departments DataFrame:\")\n",
        "df_departments.show()\n",
        "\n",
        "# 2. Provide a code example demonstrating the use of Window Functions.\n",
        "print(\"\\n--- Window Functions ---\")\n",
        "print(\"\"\"\n",
        "Window functions perform calculations across a set of rows related to the current row.\n",
        "A window is defined using `Window.partitionBy()` (to group rows) and `orderBy()` (to order rows within each partition).\n",
        "\"\"\")\n",
        "\n",
        "# Define a window specification partitioned by Department and ordered by Salary descending\n",
        "window_spec_salary = Window.partitionBy(\"Department\").orderBy(F.desc(\"Salary\"))\n",
        "\n",
        "# Example: Rank employees within each department based on salary\n",
        "print(\"\\nRanking employees within each department by Salary:\")\n",
        "df_employees.withColumn(\"Rank_within_Dept\", F.rank().over(window_spec_salary)).show()\n",
        "\n",
        "# Example: Calculate average salary within each department\n",
        "print(\"\\nAverage Salary within each Department:\")\n",
        "df_employees.withColumn(\"Avg_Dept_Salary\", F.avg(\"Salary\").over(Window.partitionBy(\"Department\"))).show()\n",
        "\n",
        "# Example: Calculate a cumulative sum of salary within each department based on hire date\n",
        "print(\"\\nCumulative Salary within each Department by Hire Date:\")\n",
        "window_spec_cumulative = Window.partitionBy(\"Department\").orderBy(\"HireDate\")\n",
        "df_employees.withColumn(\"Cumulative_Dept_Salary\", F.sum(\"Salary\").over(window_spec_cumulative)).show()\n",
        "\n",
        "\n",
        "# 3. Provide a code example demonstrating different types of DataFrame Joins.\n",
        "print(\"\\n--- DataFrame Joins ---\")\n",
        "print(\"\"\"\n",
        "Joins combine rows from two DataFrames based on a related column between them.\n",
        "\"\"\")\n",
        "\n",
        "# Example: Inner Join\n",
        "print(\"\\nInner Join (matching rows from both DataFrames):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"inner\").show()\n",
        "\n",
        "# Example: Left Outer Join (all rows from left, matched rows from right, nulls if no match)\n",
        "print(\"\\nLeft Outer Join (all employees, with department location if available):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"left_outer\").show()\n",
        "\n",
        "# Example: Right Outer Join (all rows from right, matched rows from left, nulls if no match)\n",
        "print(\"\\nRight Outer Join (all departments, with employee info if available):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"right_outer\").show()\n",
        "\n",
        "# Example: Full Outer Join (all rows from both DataFrames, with nulls where no match)\n",
        "print(\"\\nFull Outer Join (all employees and all departments):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"full_outer\").show()\n",
        "\n",
        "# Example: Joining on multiple columns (if applicable, demonstrating syntax)\n",
        "# Assuming df1 has columns (colA, colB) and df2 has columns (colC, colD)\n",
        "# df1.join(df2, (df1.colA == df2.colC) & (df1.colB == df2.colD), \"inner\").show()\n",
        "\n",
        "\n",
        "# 4. Provide a code example demonstrating advanced Aggregations.\n",
        "print(\"\\n--- Advanced Aggregations ---\")\n",
        "\n",
        "# Example: Using agg() with multiple aggregate functions\n",
        "print(\"\\nAggregating with multiple functions:\")\n",
        "df_employees.groupBy(\"Department\").agg(\n",
        "    F.count(\"*\").alias(\"Employee_Count\"),\n",
        "    F.avg(\"Salary\").alias(\"Average_Salary\"),\n",
        "    F.max(\"Salary\").alias(\"Max_Salary\")\n",
        ").show()\n",
        "\n",
        "# Example: Using pivot() - Rotates a column into the header\n",
        "print(\"\\nPivoting data (Average Salary by Department and a dummy category):\")\n",
        "# Add a dummy category column for pivoting example\n",
        "df_pivot_data = df_employees.withColumn(\"Category\", F.lit(\"A\"))\n",
        "df_pivot_data.groupBy(\"Department\").pivot(\"Category\").agg(F.avg(\"Salary\").alias(\"Avg_Salary\")).show()\n",
        "\n",
        "# Real-world pivot example might involve pivoting a date part or status\n",
        "# df_sales.groupBy(\"Product\").pivot(\"Year\").agg(F.sum(\"Revenue\")).show()\n",
        "\n",
        "\n",
        "# 5. Provide code examples for advanced Actions beyond show().\n",
        "print(\"\\n--- Advanced Actions ---\")\n",
        "print(\"\"\"\n",
        "Actions trigger the execution of the DataFrame plan.\n",
        "\"\"\")\n",
        "\n",
        "# Example: collect() - Returns all rows to the driver as a list of Row objects (use with caution on large DataFrames)\n",
        "print(\"\\nExample: collect() (first 3 rows to avoid flooding console):\")\n",
        "collected_rows = df_employees.collect()\n",
        "for i, row in enumerate(collected_rows):\n",
        "    if i < 3:\n",
        "        print(row)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Example: take(n) - Returns the first n rows to the driver as a list of Row objects\n",
        "print(\"\\nExample: take(2):\")\n",
        "taken_rows = df_employees.take(2)\n",
        "print(taken_rows)\n",
        "\n",
        "# Example: count() - Returns the number of rows in the DataFrame\n",
        "print(\"\\nExample: count():\")\n",
        "row_count = df_employees.count()\n",
        "print(f\"Total number of rows: {row_count}\")\n",
        "\n",
        "# Example: save() / write() - Writes the DataFrame to a file or storage system\n",
        "# Example: Saving to Parquet (assuming /tmp is accessible)\n",
        "output_parquet_path = \"/tmp/employees_output.parquet\"\n",
        "print(f\"\\nExample: save() / write() to Parquet at {output_parquet_path}\")\n",
        "try:\n",
        "    # Ensure directory exists and clean up previous runs\n",
        "    import os, shutil\n",
        "    if os.path.exists(output_parquet_path):\n",
        "        shutil.rmtree(output_parquet_path)\n",
        "    df_employees.write.mode(\"overwrite\").parquet(output_parquet_path)\n",
        "    print(\"DataFrame successfully saved to Parquet.\")\n",
        "    # Verify by reading back\n",
        "    df_read_back = spark.read.parquet(output_parquet_path)\n",
        "    print(\"Data read back from saved Parquet:\")\n",
        "    df_read_back.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error saving DataFrame to Parquet: {e}\")\n",
        "\n",
        "# Example: foreach() - Applies a function to each row in the DataFrame (executed on workers)\n",
        "print(\"\\nExample: foreach() (printing each employee name on worker nodes):\")\n",
        "# Note: Output of foreach will appear in worker logs, not necessarily driver console\n",
        "def print_employee_name(row):\n",
        "    # This function runs on the worker nodes\n",
        "    print(f\"Processing employee: {row['Name']}\")\n",
        "\n",
        "# df_employees.foreach(print_employee_name) # Uncomment to run foreach\n",
        "\n",
        "print(\"foreach() action demonstrated conceptually. Output appears in worker logs.\")\n",
        "\n",
        "\n",
        "# 6. Discuss performance implications and optimization techniques.\n",
        "print(\"\\n--- Performance Implications and Optimization ---\")\n",
        "print(\"\"\"\n",
        "Advanced operations like Joins and Aggregations can be computationally expensive and lead to significant data shuffling across the network, which is a major bottleneck in distributed computing.\n",
        "\n",
        "Performance Implications:\n",
        "- **Shuffling:** Joins and aggregations often require shuffling data so that related data (e.g., rows with the same join key or group by key) are brought together on the same partition. Shuffling involves writing data to disk and transferring it over the network.\n",
        "- **Data Skew:** If data is unevenly distributed (data skew), some partitions might become much larger than others, leading to bottlenecks as a few tasks take much longer to complete.\n",
        "- **Resource Consumption:** These operations can consume significant CPU, memory, and network resources.\n",
        "\n",
        "Optimization Strategies:\n",
        "- **Caching/Persisting:** Cache or persist DataFrames that are used multiple times, especially before expensive operations like joins or aggregations, to avoid recomputing them. Use `df.cache()` or `df.persist()`.\n",
        "- **Data Partitioning:** Ensure DataFrames are partitioned appropriately before joins or aggregations on the join/grouping keys. Spark can perform hash partitioning to distribute data evenly. The number of partitions affects parallelism and shuffling.\n",
        "- **Broadcasting Joins:** If one of the DataFrames in a join is significantly smaller than the other, Spark can \"broadcast\" the smaller DataFrame to all worker nodes. This avoids shuffling the larger DataFrame and can dramatically improve performance. Spark can often do this automatically, but you can hint using `F.broadcast(smaller_df)`.\n",
        "- **Optimizing Aggregations:** Use `groupBy().agg()` which is optimized. For highly skewed data, consider techniques like salting or using Spark's built-in handling for skew joins/aggregations (though manual intervention might sometimes be needed).\n",
        "- **Column Pruning and Predicate Pushdown:** Ensure filters are applied early in the transformation chain. Spark's optimizer automatically pushes down filters to read less data from the source, but understanding this helps in structuring your logic.\n",
        "- **Choosing the Right Join Type:** Select the join type that matches your requirements precisely to avoid unnecessary computations.\n",
        "- **Data Format:** Use columnar formats like Parquet or ORC which are optimized for reading only necessary columns, reducing I/O for operations that don't require all columns.\n",
        "- **Monitoring and Tuning:** Use the Spark UI to monitor job execution, identify bottlenecks (like skewed stages or high shuffle read/write), and tune configuration parameters (`spark.sql.shuffle.partitions`, memory settings, etc.).\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Stop the SparkSession (optional, if this is the last step)\n",
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Advanced Spark DataFrame Transformations and Actions ---\n",
            "\n",
            "Building upon basic transformations like `select`, `filter`, and `withColumn`, Spark DataFrames offer more advanced operations for complex data processing tasks. These include:\n",
            "\n",
            "- **Window Functions:** Perform calculations across a set of table rows that are related to the current row, similar to aggregations but without collapsing the rows. Useful for ranking, calculating moving averages, cumulative sums, etc.\n",
            "- **Joins:** Combine two DataFrames based on common columns, allowing you to merge data from different sources. Various join types (inner, outer, left, right, etc.) are supported.\n",
            "- **Advanced Aggregations:** Beyond simple `groupBy().agg()`, Spark provides ways to perform more complex aggregations like pivoting data or using grouping sets.\n",
            "- **Advanced Actions:** Operations beyond `show()` that trigger computation and return results to the driver or write data, such as `collect()`, `take()`, `count()`, `save()`, `foreach()`.\n",
            "\n",
            "These advanced operations are essential for sophisticated data analysis, feature engineering, and preparing data for machine learning models. Spark's Catalyst optimizer plays a crucial role in optimizing these operations for distributed execution.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'sc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-358797713.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                   (\"Frank\", \"IT\", 6200, \"2022-03-10\")]\n\u001b[1;32m     34\u001b[0m \u001b[0mcolumns_employees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Department\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Salary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HireDate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdf_employees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_employees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_employees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m data_departments = [(\"Sales\", \"Building A\"),\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# convert python objects to sql data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0minternal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtupled_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minternal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \"\"\"\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbcabd5c"
      },
      "source": [
        "**Reasoning**:\n",
        "The error 'AttributeError: 'NoneType' object has no attribute 'sc'' indicates that the SparkContext is not properly initialized or accessible through the `spark` object. Although I added `spark = SparkSession.builder.appName(\"AdvancedTransformationsActions\").getOrCreate()`, it seems the environment might be losing the session state. I will try to explicitly stop any potentially lingering session before creating a new one to ensure a clean state. I will regenerate the code block with this modification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKUSdHijQVCb",
        "outputId": "ed32f834-88cb-4152-86d2-65323db1622d"
      },
      "source": [
        "# Explicitly stop any existing SparkSession before creating a new one\n",
        "try:\n",
        "    spark.stop()\n",
        "except NameError:\n",
        "    # SparkSession was not defined, so nothing to stop\n",
        "    pass\n",
        "except Exception as e:\n",
        "    print(f\"Error stopping existing SparkSession: {e}\")\n",
        "\n",
        "\n",
        "# Create a new SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedTransformationsActions\").getOrCreate()\n",
        "\n",
        "\n",
        "# 1. Explain advanced transformations and actions in Spark DataFrames.\n",
        "print(\"--- Advanced Spark DataFrame Transformations and Actions ---\")\n",
        "print(\"\"\"\n",
        "Building upon basic transformations like `select`, `filter`, and `withColumn`, Spark DataFrames offer more advanced operations for complex data processing tasks. These include:\n",
        "\n",
        "- **Window Functions:** Perform calculations across a set of table rows that are related to the current row, similar to aggregations but without collapsing the rows. Useful for ranking, calculating moving averages, cumulative sums, etc.\n",
        "- **Joins:** Combine two DataFrames based on common columns, allowing you to merge data from different sources. Various join types (inner, outer, left, right, etc.) are supported.\n",
        "- **Advanced Aggregations:** Beyond simple `groupBy().agg()`, Spark provides ways to perform more complex aggregations like pivoting data or using grouping sets.\n",
        "- **Advanced Actions:** Operations beyond `show()` that trigger computation and return results to the driver or write data, such as `collect()`, `take()`, `count()`, `save()`, `foreach()`.\n",
        "\n",
        "These advanced operations are essential for sophisticated data analysis, feature engineering, and preparing data for machine learning models. Spark's Catalyst optimizer plays a crucial role in optimizing these operations for distributed execution.\n",
        "\"\"\")\n",
        "\n",
        "# Create sample DataFrames for demonstration\n",
        "data_employees = [(\"Alice\", \"Sales\", 5000, \"2022-01-15\"),\n",
        "                  (\"Bob\", \"IT\", 6000, \"2022-01-15\"),\n",
        "                  (\"Charlie\", \"Sales\", 5500, \"2022-02-01\"),\n",
        "                  (\"David\", \"IT\", 6500, \"2022-02-01\"),\n",
        "                  (\"Eve\", \"Sales\", 5200, \"2022-03-10\"),\n",
        "                  (\"Frank\", \"IT\", 6200, \"2022-03-10\")]\n",
        "columns_employees = [\"Name\", \"Department\", \"Salary\", \"HireDate\"]\n",
        "df_employees = spark.createDataFrame(data_employees, columns_employees)\n",
        "\n",
        "data_departments = [(\"Sales\", \"Building A\"),\n",
        "                    (\"IT\", \"Building B\"),\n",
        "                    (\"HR\", \"Building C\")]\n",
        "columns_departments = [\"Department\", \"Location\"]\n",
        "df_departments = spark.createDataFrame(data_departments, columns_departments)\n",
        "\n",
        "print(\"\\nSample Employees DataFrame:\")\n",
        "df_employees.show()\n",
        "print(\"\\nSample Departments DataFrame:\")\n",
        "df_departments.show()\n",
        "\n",
        "# 2. Provide a code example demonstrating the use of Window Functions.\n",
        "print(\"\\n--- Window Functions ---\")\n",
        "print(\"\"\"\n",
        "Window functions perform calculations across a set of rows related to the current row.\n",
        "A window is defined using `Window.partitionBy()` (to group rows) and `orderBy()` (to order rows within each partition).\n",
        "\"\"\")\n",
        "\n",
        "# Define a window specification partitioned by Department and ordered by Salary descending\n",
        "window_spec_salary = Window.partitionBy(\"Department\").orderBy(F.desc(\"Salary\"))\n",
        "\n",
        "# Example: Rank employees within each department based on salary\n",
        "print(\"\\nRanking employees within each department by Salary:\")\n",
        "df_employees.withColumn(\"Rank_within_Dept\", F.rank().over(window_spec_salary)).show()\n",
        "\n",
        "# Example: Calculate average salary within each department\n",
        "print(\"\\nAverage Salary within each Department:\")\n",
        "df_employees.withColumn(\"Avg_Dept_Salary\", F.avg(\"Salary\").over(Window.partitionBy(\"Department\"))).show()\n",
        "\n",
        "# Example: Calculate a cumulative sum of salary within each department based on hire date\n",
        "print(\"\\nCumulative Salary within each Department by Hire Date:\")\n",
        "window_spec_cumulative = Window.partitionBy(\"Department\").orderBy(\"HireDate\")\n",
        "df_employees.withColumn(\"Cumulative_Dept_Salary\", F.sum(\"Salary\").over(window_spec_cumulative)).show()\n",
        "\n",
        "\n",
        "# 3. Provide a code example demonstrating different types of DataFrame Joins.\n",
        "print(\"\\n--- DataFrame Joins ---\")\n",
        "print(\"\"\"\n",
        "Joins combine rows from two DataFrames based on a related column between them.\n",
        "\"\"\")\n",
        "\n",
        "# Example: Inner Join\n",
        "print(\"\\nInner Join (matching rows from both DataFrames):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"inner\").show()\n",
        "\n",
        "# Example: Left Outer Join (all rows from left, matched rows from right, nulls if no match)\n",
        "print(\"\\nLeft Outer Join (all employees, with department location if available):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"left_outer\").show()\n",
        "\n",
        "# Example: Right Outer Join (all rows from right, matched rows from left, nulls if no match)\n",
        "print(\"\\nRight Outer Join (all departments, with employee info if available):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"right_outer\").show()\n",
        "\n",
        "# Example: Full Outer Join (all rows from both DataFrames, with nulls where no match)\n",
        "print(\"\\nFull Outer Join (all employees and all departments):\")\n",
        "df_employees.join(df_departments, on=\"Department\", how=\"full_outer\").show()\n",
        "\n",
        "# Example: Joining on multiple columns (if applicable, demonstrating syntax)\n",
        "# Assuming df1 has columns (colA, colB) and df2 has columns (colC, colD)\n",
        "# df1.join(df2, (df1.colA == df2.colC) & (df1.colB == df2.colD), \"inner\").show()\n",
        "\n",
        "\n",
        "# 4. Provide a code example demonstrating advanced Aggregations.\n",
        "print(\"\\n--- Advanced Aggregations ---\")\n",
        "\n",
        "# Example: Using agg() with multiple aggregate functions\n",
        "print(\"\\nAggregating with multiple functions:\")\n",
        "df_employees.groupBy(\"Department\").agg(\n",
        "    F.count(\"*\").alias(\"Employee_Count\"),\n",
        "    F.avg(\"Salary\").alias(\"Average_Salary\"),\n",
        "    F.max(\"Salary\").alias(\"Max_Salary\")\n",
        ").show()\n",
        "\n",
        "# Example: Using pivot() - Rotates a column into the header\n",
        "print(\"\\nPivoting data (Average Salary by Department and a dummy category):\")\n",
        "# Add a dummy category column for pivoting example\n",
        "df_pivot_data = df_employees.withColumn(\"Category\", F.lit(\"A\"))\n",
        "df_pivot_data.groupBy(\"Department\").pivot(\"Category\").agg(F.avg(\"Salary\").alias(\"Avg_Salary\")).show()\n",
        "\n",
        "# Real-world pivot example might involve pivoting a date part or status\n",
        "# df_sales.groupBy(\"Product\").pivot(\"Year\").agg(F.sum(\"Revenue\")).show()\n",
        "\n",
        "\n",
        "# 5. Provide code examples for advanced Actions beyond show().\n",
        "print(\"\\n--- Advanced Actions ---\")\n",
        "print(\"\"\"\n",
        "Actions trigger the execution of the DataFrame plan.\n",
        "\"\"\")\n",
        "\n",
        "# Example: collect() - Returns all rows to the driver as a list of Row objects (use with caution on large DataFrames)\n",
        "print(\"\\nExample: collect() (first 3 rows to avoid flooding console):\")\n",
        "collected_rows = df_employees.collect()\n",
        "for i, row in enumerate(collected_rows):\n",
        "    if i < 3:\n",
        "        print(row)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Example: take(n) - Returns the first n rows to the driver as a list of Row objects\n",
        "print(\"\\nExample: take(2):\")\n",
        "taken_rows = df_employees.take(2)\n",
        "print(taken_rows)\n",
        "\n",
        "# Example: count() - Returns the number of rows in the DataFrame\n",
        "print(\"\\nExample: count():\")\n",
        "row_count = df_employees.count()\n",
        "print(f\"Total number of rows: {row_count}\")\n",
        "\n",
        "# Example: save() / write() - Writes the DataFrame to a file or storage system\n",
        "# Example: Saving to Parquet (assuming /tmp is accessible)\n",
        "output_parquet_path = \"/tmp/employees_output.parquet\"\n",
        "print(f\"\\nExample: save() / write() to Parquet at {output_parquet_path}\")\n",
        "try:\n",
        "    # Ensure directory exists and clean up previous runs\n",
        "    import os, shutil\n",
        "    if os.path.exists(output_parquet_path):\n",
        "        shutil.rmtree(output_parquet_path)\n",
        "    df_employees.write.mode(\"overwrite\").parquet(output_parquet_path)\n",
        "    print(\"DataFrame successfully saved to Parquet.\")\n",
        "    # Verify by reading back\n",
        "    df_read_back = spark.read.parquet(output_parquet_path)\n",
        "    print(\"Data read back from saved Parquet:\")\n",
        "    df_read_back.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error saving DataFrame to Parquet: {e}\")\n",
        "\n",
        "# Example: foreach() - Applies a function to each row in the DataFrame (executed on workers)\n",
        "print(\"\\nExample: foreach() (printing each employee name on worker nodes):\")\n",
        "# Note: Output of foreach will appear in worker logs, not necessarily driver console\n",
        "def print_employee_name(row):\n",
        "    # This function runs on the worker nodes\n",
        "    print(f\"Processing employee: {row['Name']}\")\n",
        "\n",
        "# df_employees.foreach(print_employee_name) # Uncomment to run foreach\n",
        "\n",
        "print(\"foreach() action demonstrated conceptually. Output appears in worker logs.\")\n",
        "\n",
        "\n",
        "# 6. Discuss performance implications and optimization techniques.\n",
        "print(\"\\n--- Performance Implications and Optimization ---\")\n",
        "print(\"\"\"\n",
        "Advanced operations like Joins and Aggregations can be computationally expensive and lead to significant data shuffling across the network, which is a major bottleneck in distributed computing.\n",
        "\n",
        "Performance Implications:\n",
        "- **Shuffling:** Joins and aggregations often require shuffling data so that related data (e.g., rows with the same join key or group by key) are brought together on the same partition. Shuffling involves writing data to disk and transferring it over the network.\n",
        "- **Data Skew:** If data is unevenly distributed (data skew), some partitions might become much larger than others, leading to bottlenecks as a few tasks take much longer to complete.\n",
        "- **Resource Consumption:** These operations can consume significant CPU, memory, and network resources.\n",
        "\n",
        "Optimization Strategies:\n",
        "- **Caching/Persisting:** Cache or persist DataFrames that are used multiple times, especially before expensive operations like joins or aggregations, to avoid recomputing them. Use `df.cache()` or `df.persist()`.\n",
        "- **Data Partitioning:** Ensure DataFrames are partitioned appropriately before joins or aggregations on the join/grouping keys. Spark can perform hash partitioning to distribute data evenly. The number of partitions affects parallelism and shuffling.\n",
        "- **Broadcasting Joins:** If one of the DataFrames in a join is significantly smaller than the other, Spark can \"broadcast\" the smaller DataFrame to all worker nodes. This avoids shuffling the larger DataFrame and can dramatically improve performance. Spark can often do this automatically, but you can hint using `F.broadcast(smaller_df)`.\n",
        "- **Optimizing Aggregations:** Use `groupBy().agg()` which is optimized. For highly skewed data, consider techniques like salting or using Spark's built-in handling for skew joins/aggregations (though manual intervention might sometimes be needed).\n",
        "- **Column Pruning and Predicate Pushdown:** Ensure filters are applied early in the transformation chain. Spark's optimizer automatically pushes down filters to read less data from the source, but understanding this helps in structuring your logic.\n",
        "- **Choosing the Right Join Type:** Select the join type that matches your requirements precisely to avoid unnecessary computations.\n",
        "- **Data Format:** Use columnar formats like Parquet or ORC which are optimized for reading only necessary columns, reducing I/O for operations that don't require all columns.\n",
        "- **Monitoring and Tuning:** Use the Spark UI to monitor job execution, identify bottlenecks (like skewed stages or high shuffle read/write), and tune configuration parameters (`spark.sql.shuffle.partitions`, memory settings, etc.).\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Stop the SparkSession (optional, if this is the last step)\n",
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Advanced Spark DataFrame Transformations and Actions ---\n",
            "\n",
            "Building upon basic transformations like `select`, `filter`, and `withColumn`, Spark DataFrames offer more advanced operations for complex data processing tasks. These include:\n",
            "\n",
            "- **Window Functions:** Perform calculations across a set of table rows that are related to the current row, similar to aggregations but without collapsing the rows. Useful for ranking, calculating moving averages, cumulative sums, etc.\n",
            "- **Joins:** Combine two DataFrames based on common columns, allowing you to merge data from different sources. Various join types (inner, outer, left, right, etc.) are supported.\n",
            "- **Advanced Aggregations:** Beyond simple `groupBy().agg()`, Spark provides ways to perform more complex aggregations like pivoting data or using grouping sets.\n",
            "- **Advanced Actions:** Operations beyond `show()` that trigger computation and return results to the driver or write data, such as `collect()`, `take()`, `count()`, `save()`, `foreach()`.\n",
            "\n",
            "These advanced operations are essential for sophisticated data analysis, feature engineering, and preparing data for machine learning models. Spark's Catalyst optimizer plays a crucial role in optimizing these operations for distributed execution.\n",
            "\n",
            "\n",
            "Sample Employees DataFrame:\n",
            "+-------+----------+------+----------+\n",
            "|   Name|Department|Salary|  HireDate|\n",
            "+-------+----------+------+----------+\n",
            "|  Alice|     Sales|  5000|2022-01-15|\n",
            "|    Bob|        IT|  6000|2022-01-15|\n",
            "|Charlie|     Sales|  5500|2022-02-01|\n",
            "|  David|        IT|  6500|2022-02-01|\n",
            "|    Eve|     Sales|  5200|2022-03-10|\n",
            "|  Frank|        IT|  6200|2022-03-10|\n",
            "+-------+----------+------+----------+\n",
            "\n",
            "\n",
            "Sample Departments DataFrame:\n",
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|        IT|Building B|\n",
            "|        HR|Building C|\n",
            "+----------+----------+\n",
            "\n",
            "\n",
            "--- Window Functions ---\n",
            "\n",
            "Window functions perform calculations across a set of rows related to the current row.\n",
            "A window is defined using `Window.partitionBy()` (to group rows) and `orderBy()` (to order rows within each partition).\n",
            "\n",
            "\n",
            "Ranking employees within each department by Salary:\n",
            "+-------+----------+------+----------+----------------+\n",
            "|   Name|Department|Salary|  HireDate|Rank_within_Dept|\n",
            "+-------+----------+------+----------+----------------+\n",
            "|  David|        IT|  6500|2022-02-01|               1|\n",
            "|  Frank|        IT|  6200|2022-03-10|               2|\n",
            "|    Bob|        IT|  6000|2022-01-15|               3|\n",
            "|Charlie|     Sales|  5500|2022-02-01|               1|\n",
            "|    Eve|     Sales|  5200|2022-03-10|               2|\n",
            "|  Alice|     Sales|  5000|2022-01-15|               3|\n",
            "+-------+----------+------+----------+----------------+\n",
            "\n",
            "\n",
            "Average Salary within each Department:\n",
            "+-------+----------+------+----------+-----------------+\n",
            "|   Name|Department|Salary|  HireDate|  Avg_Dept_Salary|\n",
            "+-------+----------+------+----------+-----------------+\n",
            "|    Bob|        IT|  6000|2022-01-15|6233.333333333333|\n",
            "|  David|        IT|  6500|2022-02-01|6233.333333333333|\n",
            "|  Frank|        IT|  6200|2022-03-10|6233.333333333333|\n",
            "|  Alice|     Sales|  5000|2022-01-15|5233.333333333333|\n",
            "|Charlie|     Sales|  5500|2022-02-01|5233.333333333333|\n",
            "|    Eve|     Sales|  5200|2022-03-10|5233.333333333333|\n",
            "+-------+----------+------+----------+-----------------+\n",
            "\n",
            "\n",
            "Cumulative Salary within each Department by Hire Date:\n",
            "+-------+----------+------+----------+----------------------+\n",
            "|   Name|Department|Salary|  HireDate|Cumulative_Dept_Salary|\n",
            "+-------+----------+------+----------+----------------------+\n",
            "|    Bob|        IT|  6000|2022-01-15|                  6000|\n",
            "|  David|        IT|  6500|2022-02-01|                 12500|\n",
            "|  Frank|        IT|  6200|2022-03-10|                 18700|\n",
            "|  Alice|     Sales|  5000|2022-01-15|                  5000|\n",
            "|Charlie|     Sales|  5500|2022-02-01|                 10500|\n",
            "|    Eve|     Sales|  5200|2022-03-10|                 15700|\n",
            "+-------+----------+------+----------+----------------------+\n",
            "\n",
            "\n",
            "--- DataFrame Joins ---\n",
            "\n",
            "Joins combine rows from two DataFrames based on a related column between them.\n",
            "\n",
            "\n",
            "Inner Join (matching rows from both DataFrames):\n",
            "+----------+-------+------+----------+----------+\n",
            "|Department|   Name|Salary|  HireDate|  Location|\n",
            "+----------+-------+------+----------+----------+\n",
            "|        IT|    Bob|  6000|2022-01-15|Building B|\n",
            "|        IT|  David|  6500|2022-02-01|Building B|\n",
            "|        IT|  Frank|  6200|2022-03-10|Building B|\n",
            "|     Sales|  Alice|  5000|2022-01-15|Building A|\n",
            "|     Sales|Charlie|  5500|2022-02-01|Building A|\n",
            "|     Sales|    Eve|  5200|2022-03-10|Building A|\n",
            "+----------+-------+------+----------+----------+\n",
            "\n",
            "\n",
            "Left Outer Join (all employees, with department location if available):\n",
            "+----------+-------+------+----------+----------+\n",
            "|Department|   Name|Salary|  HireDate|  Location|\n",
            "+----------+-------+------+----------+----------+\n",
            "|     Sales|  Alice|  5000|2022-01-15|Building A|\n",
            "|     Sales|Charlie|  5500|2022-02-01|Building A|\n",
            "|        IT|    Bob|  6000|2022-01-15|Building B|\n",
            "|     Sales|    Eve|  5200|2022-03-10|Building A|\n",
            "|        IT|  David|  6500|2022-02-01|Building B|\n",
            "|        IT|  Frank|  6200|2022-03-10|Building B|\n",
            "+----------+-------+------+----------+----------+\n",
            "\n",
            "\n",
            "Right Outer Join (all departments, with employee info if available):\n",
            "+----------+-------+------+----------+----------+\n",
            "|Department|   Name|Salary|  HireDate|  Location|\n",
            "+----------+-------+------+----------+----------+\n",
            "|     Sales|    Eve|  5200|2022-03-10|Building A|\n",
            "|     Sales|Charlie|  5500|2022-02-01|Building A|\n",
            "|     Sales|  Alice|  5000|2022-01-15|Building A|\n",
            "|        HR|   NULL|  NULL|      NULL|Building C|\n",
            "|        IT|  Frank|  6200|2022-03-10|Building B|\n",
            "|        IT|  David|  6500|2022-02-01|Building B|\n",
            "|        IT|    Bob|  6000|2022-01-15|Building B|\n",
            "+----------+-------+------+----------+----------+\n",
            "\n",
            "\n",
            "Full Outer Join (all employees and all departments):\n",
            "+----------+-------+------+----------+----------+\n",
            "|Department|   Name|Salary|  HireDate|  Location|\n",
            "+----------+-------+------+----------+----------+\n",
            "|        HR|   NULL|  NULL|      NULL|Building C|\n",
            "|        IT|    Bob|  6000|2022-01-15|Building B|\n",
            "|        IT|  David|  6500|2022-02-01|Building B|\n",
            "|        IT|  Frank|  6200|2022-03-10|Building B|\n",
            "|     Sales|  Alice|  5000|2022-01-15|Building A|\n",
            "|     Sales|Charlie|  5500|2022-02-01|Building A|\n",
            "|     Sales|    Eve|  5200|2022-03-10|Building A|\n",
            "+----------+-------+------+----------+----------+\n",
            "\n",
            "\n",
            "--- Advanced Aggregations ---\n",
            "\n",
            "Aggregating with multiple functions:\n",
            "+----------+--------------+-----------------+----------+\n",
            "|Department|Employee_Count|   Average_Salary|Max_Salary|\n",
            "+----------+--------------+-----------------+----------+\n",
            "|     Sales|             3|5233.333333333333|      5500|\n",
            "|        IT|             3|6233.333333333333|      6500|\n",
            "+----------+--------------+-----------------+----------+\n",
            "\n",
            "\n",
            "Pivoting data (Average Salary by Department and a dummy category):\n",
            "+----------+-----------------+\n",
            "|Department|                A|\n",
            "+----------+-----------------+\n",
            "|     Sales|5233.333333333333|\n",
            "|        IT|6233.333333333333|\n",
            "+----------+-----------------+\n",
            "\n",
            "\n",
            "--- Advanced Actions ---\n",
            "\n",
            "Actions trigger the execution of the DataFrame plan.\n",
            "\n",
            "\n",
            "Example: collect() (first 3 rows to avoid flooding console):\n",
            "Row(Name='Alice', Department='Sales', Salary=5000, HireDate='2022-01-15')\n",
            "Row(Name='Bob', Department='IT', Salary=6000, HireDate='2022-01-15')\n",
            "Row(Name='Charlie', Department='Sales', Salary=5500, HireDate='2022-02-01')\n",
            "\n",
            "Example: take(2):\n",
            "[Row(Name='Alice', Department='Sales', Salary=5000, HireDate='2022-01-15'), Row(Name='Bob', Department='IT', Salary=6000, HireDate='2022-01-15')]\n",
            "\n",
            "Example: count():\n",
            "Total number of rows: 6\n",
            "\n",
            "Example: save() / write() to Parquet at /tmp/employees_output.parquet\n",
            "DataFrame successfully saved to Parquet.\n",
            "Data read back from saved Parquet:\n",
            "+-------+----------+------+----------+\n",
            "|   Name|Department|Salary|  HireDate|\n",
            "+-------+----------+------+----------+\n",
            "|  Alice|     Sales|  5000|2022-01-15|\n",
            "|    Bob|        IT|  6000|2022-01-15|\n",
            "|Charlie|     Sales|  5500|2022-02-01|\n",
            "|  David|        IT|  6500|2022-02-01|\n",
            "|    Eve|     Sales|  5200|2022-03-10|\n",
            "|  Frank|        IT|  6200|2022-03-10|\n",
            "+-------+----------+------+----------+\n",
            "\n",
            "\n",
            "Example: foreach() (printing each employee name on worker nodes):\n",
            "foreach() action demonstrated conceptually. Output appears in worker logs.\n",
            "\n",
            "--- Performance Implications and Optimization ---\n",
            "\n",
            "Advanced operations like Joins and Aggregations can be computationally expensive and lead to significant data shuffling across the network, which is a major bottleneck in distributed computing.\n",
            "\n",
            "Performance Implications:\n",
            "- **Shuffling:** Joins and aggregations often require shuffling data so that related data (e.g., rows with the same join key or group by key) are brought together on the same partition. Shuffling involves writing data to disk and transferring it over the network.\n",
            "- **Data Skew:** If data is unevenly distributed (data skew), some partitions might become much larger than others, leading to bottlenecks as a few tasks take much longer to complete.\n",
            "- **Resource Consumption:** These operations can consume significant CPU, memory, and network resources.\n",
            "\n",
            "Optimization Strategies:\n",
            "- **Caching/Persisting:** Cache or persist DataFrames that are used multiple times, especially before expensive operations like joins or aggregations, to avoid recomputing them. Use `df.cache()` or `df.persist()`.\n",
            "- **Data Partitioning:** Ensure DataFrames are partitioned appropriately before joins or aggregations on the join/grouping keys. Spark can perform hash partitioning to distribute data evenly. The number of partitions affects parallelism and shuffling.\n",
            "- **Broadcasting Joins:** If one of the DataFrames in a join is significantly smaller than the other, Spark can \"broadcast\" the smaller DataFrame to all worker nodes. This avoids shuffling the larger DataFrame and can dramatically improve performance. Spark can often do this automatically, but you can hint using `F.broadcast(smaller_df)`.\n",
            "- **Optimizing Aggregations:** Use `groupBy().agg()` which is optimized. For highly skewed data, consider techniques like salting or using Spark's built-in handling for skew joins/aggregations (though manual intervention might sometimes be needed).\n",
            "- **Column Pruning and Predicate Pushdown:** Ensure filters are applied early in the transformation chain. Spark's optimizer automatically pushes down filters to read less data from the source, but understanding this helps in structuring your logic.\n",
            "- **Choosing the Right Join Type:** Select the join type that matches your requirements precisely to avoid unnecessary computations.\n",
            "- **Data Format:** Use columnar formats like Parquet or ORC which are optimized for reading only necessary columns, reducing I/O for operations that don't require all columns.\n",
            "- **Monitoring and Tuning:** Use the Spark UI to monitor job execution, identify bottlenecks (like skewed stages or high shuffle read/write), and tune configuration parameters (`spark.sql.shuffle.partitions`, memory settings, etc.).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1910a01d"
      },
      "source": [
        "## Explain optimization techniques in spark\n",
        "\n",
        "### Subtask:\n",
        "Discuss key optimization techniques in Spark, including caching, broadcasting, shuffling, and utilizing the Catalyst optimizer, explaining how they improve performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "658dba02"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the role of the Catalyst Optimizer, lazy evaluation, caching, broadcasting, shuffling, other optimization techniques, and the Spark UI as per the instructions, including code examples where applicable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c8fd9e81",
        "outputId": "c8892cb9-5f8d-460f-b389-4bc55112d911"
      },
      "source": [
        "# Explicitly stop any existing SparkSession before creating a new one\n",
        "try:\n",
        "    spark.stop()\n",
        "except NameError:\n",
        "    # SparkSession was not defined, so nothing to stop\n",
        "    pass\n",
        "except Exception as e:\n",
        "    print(f\"Error stopping existing SparkSession: {e}\")\n",
        "\n",
        "# Create a new SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkOptimizationTechniques\").getOrCreate()\n",
        "\n",
        "# 1. Explain the role of the Catalyst Optimizer in Spark SQL and how it contributes to performance.\n",
        "print(\"--- Catalyst Optimizer ---\")\n",
        "print(\"\"\"\n",
        "The Catalyst Optimizer is Spark SQL's extensible query optimizer. It is a cost-based optimizer that uses advanced programming language features (like Scala's pattern matching and quasi-quotes) to build a highly extensible and powerful optimization framework.\n",
        "\n",
        "How it Contributes to Performance:\n",
        "Catalyst translates the logical plan (representing the user's DataFrame/SQL query) into an optimized physical plan that can be executed efficiently on the Spark cluster. It goes through several phases:\n",
        "\n",
        "1.  **Parsing and Analysis:** Parses the SQL query or DataFrame operations into an Abstract Syntax Tree (AST) and then resolves table names, column names, and data types against the catalog (like Hive Metastore).\n",
        "2.  **Logical Optimization:** Applies a set of rules to the logical plan to optimize it independently of the execution engine. This includes:\n",
        "    -   Predicate Pushdown: Moving filters closer to the data source to reduce the amount of data read.\n",
        "    -   Column Pruning: Removing columns that are not needed for the final result.\n",
        "    -   Constant Folding: Replacing constant expressions with their computed values.\n",
        "    -   Join Reordering: Changing the order of joins to minimize intermediate data size.\n",
        "3.  **Physical Planning:** Converts the optimized logical plan into one or more physical plans. This involves choosing specific physical operators for each logical operation (e.g., choosing between sort merge join, hash join, or broadcast hash join).\n",
        "4.  **Code Generation:** For parts of the query, Spark can generate optimized JVM bytecode using the Tungsten execution engine, which further improves CPU efficiency.\n",
        "\n",
        "By applying these optimizations, Catalyst significantly reduces the amount of data read, processed, and shuffled, leading to faster query execution.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Discuss the concept of lazy evaluation and how it enables optimization in Spark.\n",
        "print(\"\\n--- Lazy Evaluation and Optimization ---\")\n",
        "print(\"\"\"\n",
        "Spark operations are categorized into transformations and actions. Transformations (like `select`, `filter`, `withColumn`) are lazy; they don't execute immediately but build a Directed Acyclic Graph (DAG) of operations. Actions (like `show`, `count`, `collect`, `write`) trigger the actual computation.\n",
        "\n",
        "How Lazy Evaluation Enables Optimization:\n",
        "Lazy evaluation allows Spark's Catalyst optimizer to see the entire lineage of transformations leading up to an action. This global view enables cross-operation optimizations that would not be possible if each transformation was executed immediately.\n",
        "\n",
        "For example, if you chain a `filter` followed by a `select`, Spark can:\n",
        "-   Push the filter down: Apply the filter *before* reading or processing unnecessary data.\n",
        "-   Prune columns: Only read the columns required by the `select` *after* the filter is applied.\n",
        "\n",
        "Without lazy evaluation, Spark would execute the `filter`, materialize an intermediate DataFrame, and then execute the `select` on that intermediate result, missing opportunities for optimization. Lazy evaluation allows Spark to build an optimal execution plan for the entire sequence of operations.\n",
        "\"\"\")\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "data = [(\"Alice\", 1, \"New York\", 30),\n",
        "        (\"Bob\", 2, \"Los Angeles\", 35),\n",
        "        (\"Charlie\", 3, \"Chicago\", 40),\n",
        "        (\"Alice\", 4, \"New York\", 25),\n",
        "        (\"Bob\", 5, \"Los Angeles\", 32),\n",
        "        (\"David\", 6, \"Chicago\", 45),\n",
        "        (\"Eve\", 7, \"New York\", 28),\n",
        "        (\"Frank\", 8, \"Los Angeles\", 38)]\n",
        "columns = [\"Name\", \"ID\", \"City\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"\\nSample DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# 3. Explain the importance of caching and persisting DataFrames/RDDs for performance,\n",
        "# and provide code examples for using `.cache()` and `.persist()`.\n",
        "print(\"\\n--- Caching and Persisting ---\")\n",
        "print(\"\"\"\n",
        "Caching and persisting are optimization techniques used to store an intermediate DataFrame or RDD in memory (or on disk) across multiple operations. This is crucial when you plan to reuse the same DataFrame/RDD multiple times in your application.\n",
        "\n",
        "Importance for Performance:\n",
        "When you perform transformations on a DataFrame, Spark builds a lineage (DAG) of dependencies. If an action is called, Spark recomputes the entire lineage up to that point. If you reuse the same DataFrame for another action without caching, Spark will recompute the lineage again, which can be very inefficient, especially for complex or time-consuming transformations.\n",
        "\n",
        "`cache()` and `persist()` store the DataFrame's partitions in the cluster's memory (by default). Subsequent actions that use this cached DataFrame will read the data from memory instead of recomputing it from the source or previous transformations.\n",
        "\n",
        "-   `.cache()`: A shorthand for `persist()` with the default storage level (`MEMORY_AND_DISK`).\n",
        "-   `.persist(storageLevel)`: Allows you to specify different storage levels (e.g., `MEMORY_ONLY`, `DISK_ONLY`, `MEMORY_AND_DISK_SER`).\n",
        "\n",
        "Example: Using `.cache()`\n",
        "\"\"\")\n",
        "\n",
        "# Demonstrate caching\n",
        "print(\"Caching the DataFrame...\")\n",
        "start_time = time.time()\n",
        "df_filtered = df.filter(df.Age > 30)\n",
        "df_filtered.cache() # Cache the filtered DataFrame\n",
        "# The first action will trigger computation and caching\n",
        "print(\"First action (count) on cached DataFrame:\")\n",
        "count1 = df_filtered.count()\n",
        "end_time_first_action = time.time()\n",
        "print(f\"Count: {count1}, Time taken (first action): {end_time_first_action - start_time:.4f} seconds\")\n",
        "\n",
        "# Subsequent actions on the cached DataFrame will be faster\n",
        "print(\"\\nSecond action (show) on cached DataFrame:\")\n",
        "start_time_second_action = time.time()\n",
        "df_filtered.show()\n",
        "end_time_second_action = time.time()\n",
        "print(f\"Time taken (second action): {end_time_second_action - start_time_second_action:.4f} seconds\")\n",
        "\n",
        "# Example: Using .persist() with a different storage level (e.g., DISK_ONLY)\n",
        "# from pyspark import StorageLevel\n",
        "# print(\"\\nPersisting the DataFrame to DISK_ONLY...\")\n",
        "# df_filtered.persist(StorageLevel.DISK_ONLY)\n",
        "# df_filtered.count() # Trigger computation and persistence\n",
        "# print(\"DataFrame persisted to DISK_ONLY.\")\n",
        "# # Subsequent actions will read from disk\n",
        "\n",
        "# Unpersist the DataFrame when no longer needed\n",
        "df_filtered.unpersist()\n",
        "print(\"\\nDataFrame unpersisted.\")\n",
        "\n",
        "\n",
        "# 4. Explain the concept of broadcasting small DataFrames/RDDs during joins to avoid shuffling the larger dataset,\n",
        "# and provide a code example demonstrating the use of `F.broadcast()`.\n",
        "print(\"\\n--- Broadcasting Joins ---\")\n",
        "print(\"\"\"\n",
        "Shuffling data during a join operation can be very expensive, especially when one DataFrame is much larger than the other. Broadcasting is an optimization technique where the smaller DataFrame is sent to all partitions of the larger DataFrame. This allows the join to be performed locally on each partition of the larger DataFrame without requiring a shuffle of the larger dataset.\n",
        "\n",
        "When to Use Broadcasting:\n",
        "- When one DataFrame is significantly smaller than the other.\n",
        "- The smaller DataFrame should ideally fit comfortably into the memory of each worker node.\n",
        "- Spark can automatically broadcast small tables, but you can explicitly hint using `F.broadcast()`.\n",
        "\n",
        "Example: Using `F.broadcast()`\n",
        "\"\"\")\n",
        "\n",
        "# Create a smaller DataFrame for departments\n",
        "data_departments_small = [(\"Sales\", \"Building A\"),\n",
        "                          (\"IT\", \"Building B\")]\n",
        "columns_departments_small = [\"Department\", \"Location\"]\n",
        "df_departments_small = spark.createDataFrame(data_departments_small, columns_departments_small)\n",
        "\n",
        "print(\"Larger Employees DataFrame:\")\n",
        "df.show() # Reusing the main df\n",
        "print(\"\\nSmaller Departments DataFrame:\")\n",
        "df_departments_small.show()\n",
        "\n",
        "print(\"\\nPerforming a Broadcast Hash Join:\")\n",
        "# Join the larger employee DataFrame with the smaller department DataFrame\n",
        "# Spark will likely broadcast the smaller df_departments_small automatically,\n",
        "# but we can hint using F.broadcast() for explicit control or to override Spark's decision threshold.\n",
        "joined_df = df.join(F.broadcast(df_departments_small), on=\"Department\", how=\"inner\")\n",
        "\n",
        "joined_df.show()\n",
        "\n",
        "print(\"\"\"\n",
        "In the Spark UI's SQL tab for this query, you should see a \"BroadcastHashJoin\" physical operator, indicating that the smaller DataFrame was broadcasted. This avoids shuffling the larger 'df' (Employees) DataFrame.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# 5. Discuss shuffling, why it occurs (e.g., during joins, aggregations), and its impact on performance.\n",
        "# Briefly mention strategies to mitigate excessive shuffling (e.g., partitioning, avoiding wide transformations).\n",
        "print(\"\\n--- Shuffling ---\")\n",
        "print(\"\"\"\n",
        "Shuffling is a process where data is redistributed across partitions in the Spark cluster. It occurs when Spark needs to bring data with the same key or value together to perform an operation.\n",
        "\n",
        "Why it Occurs:\n",
        "Shuffling is typically triggered by **wide transformations**, which are transformations that require data from multiple partitions to be combined or grouped. Common operations that cause shuffling include:\n",
        "-   `groupBy()`\n",
        "-   `orderBy()` / `sort()`\n",
        "-   `join()` (unless it's a broadcast join)\n",
        "-   `distinct()`\n",
        "-   `repartition()`\n",
        "\n",
        "Impact on Performance:\n",
        "Shuffling is one of the most expensive operations in Spark because it involves:\n",
        "-   **Disk I/O:** Data from different partitions needs to be written to disk on the source nodes.\n",
        "-   **Network Transfer:** Data is transferred over the network to the destination nodes.\n",
        "-   **Serialization/Deserialization:** Data needs to be serialized before sending and deserialized upon receiving.\n",
        "\n",
        "Excessive shuffling can lead to significant performance bottlenecks, increased job execution time, and potential out-of-memory errors.\n",
        "\n",
        "Strategies to Mitigate Excessive Shuffling:\n",
        "-   **Minimize Wide Transformations:** Try to structure your logic to perform narrow transformations (operations that don't require shuffling, like `filter`, `select`, `withColumn`) as early as possible.\n",
        "-   **Broadcast Joins:** As discussed, broadcast smaller tables during joins to avoid shuffling the larger table.\n",
        "-   **Partitioning:** Ensure DataFrames are partitioned appropriately *before* shuffle-heavy operations if possible. For example, if you frequently join or group by a specific column, consider repartitioning your DataFrame by that column beforehand. This can reduce the amount of data that needs to be shuffled in subsequent operations.\n",
        "-   **Tune `spark.sql.shuffle.partitions`:** This configuration property determines the number of partitions used in shuffle stages. Increasing it can increase parallelism but also increases the number of smaller shuffle files. Decreasing it reduces the number of tasks but can lead to larger partitions and potential data skew issues. Tuning depends on your cluster size and data characteristics.\n",
        "-   **Data Skew Handling:** Implement strategies to handle skewed data (e.g., salting) for shuffle-heavy operations if the default skew handling is insufficient.\n",
        "-   **Use Columnar Formats:** Parquet and ORC reduce I/O, which indirectly helps by making the reading part of the shuffle process faster.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# 6. Explain other optimization techniques, such as column pruning, predicate pushdown, and choosing appropriate data formats (e.g., Parquet), and how they improve query performance.\n",
        "print(\"\\n--- Other Optimization Techniques ---\")\n",
        "print(\"\"\"\n",
        "Spark's Catalyst optimizer automatically applies several other techniques:\n",
        "\n",
        "-   **Column Pruning:** The optimizer identifies columns that are not required for the final result of a query or transformation chain and avoids reading or processing them. This reduces I/O and memory usage.\n",
        "    Example: If you `select` only a few columns from a wide table stored in a columnar format like Parquet, Spark will only read those specific columns.\n",
        "\n",
        "-   **Predicate Pushdown:** Filters (predicates) are pushed down as close to the data source as possible. This allows Spark to filter data early, reducing the amount of data that needs to be read from storage and processed in subsequent stages.\n",
        "    Example: `spark.read.parquet(\"/path/to/data\").filter(\"column > 10\")` - Spark will try to apply the filter `column > 10` when reading the Parquet file, potentially skipping entire row groups that don't satisfy the condition.\n",
        "\n",
        "-   **Choosing Appropriate Data Formats:**\n",
        "    -   **Columnar Formats (Parquet, ORC):** Highly recommended for analytical workloads. They enable column pruning and often provide better compression. Reading specific columns is much faster than in row-based formats.\n",
        "    -   **Text/CSV:** Simple, human-readable, but less efficient for analytical queries, especially on wide tables, as the entire row must be read even if only a few columns are needed. Schema inference can also add overhead.\n",
        "    -   **JSON:** Flexible for semi-structured data but can be less performant than columnar formats for highly structured data due to parsing overhead.\n",
        "\n",
        "-   **Partitioning Data at Rest:** Storing data partitioned in the file system (e.g., `.../year=2023/month=01/...`) allows Spark to use partition pruning. If a query includes a filter on the partition column (e.g., `WHERE year = 2023`), Spark can read data only from the relevant directories, significantly reducing I/O. This is common with formats like Parquet and ORC.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# 7. Briefly mention the Spark UI as a tool for monitoring and identifying performance bottlenecks.\n",
        "print(\"\\n--- Using the Spark UI for Monitoring and Tuning ---\")\n",
        "print(\"\"\"\n",
        "The Spark UI (accessible via a web browser at `http://<driver-node>:4040` by default when a Spark application is running) is an invaluable tool for understanding and optimizing Spark jobs.\n",
        "\n",
        "Key areas in the Spark UI for performance analysis:\n",
        "-   **Jobs Tab:** Provides an overview of all jobs, their status, and duration.\n",
        "-   **Stages Tab:** Shows the different stages of a job, including the number of tasks, duration, and input/output metrics. This is where you can often identify bottlenecks.\n",
        "-   **Tasks Tab:** Detailed information about individual tasks within a stage, including runtime, locality, and shuffle read/write. Helps identify skewed tasks.\n",
        "-   **SQL Tab:** Shows the logical and physical plan generated by the Catalyst optimizer for Spark SQL queries and DataFrame operations. This is critical for verifying if optimizations like predicate pushdown or broadcast joins are being applied.\n",
        "-   **Storage Tab:** Shows information about cached/persisted RDDs and DataFrames.\n",
        "\n",
        "By examining the Spark UI, you can identify:\n",
        "-   Stages with high shuffle read/write (indicating excessive shuffling).\n",
        "-   Skewed stages (tasks with vastly different runtimes).\n",
        "-   Stages with poor data locality (tasks reading data from remote nodes).\n",
        "-   Whether broadcast joins are being used as expected.\n",
        "-   The effectiveness of caching.\n",
        "\n",
        "Monitoring the Spark UI is a fundamental step in diagnosing and resolving performance issues in Spark applications.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Catalyst Optimizer ---\n",
            "\n",
            "The Catalyst Optimizer is Spark SQL's extensible query optimizer. It is a cost-based optimizer that uses advanced programming language features (like Scala's pattern matching and quasi-quotes) to build a highly extensible and powerful optimization framework.\n",
            "\n",
            "How it Contributes to Performance:\n",
            "Catalyst translates the logical plan (representing the user's DataFrame/SQL query) into an optimized physical plan that can be executed efficiently on the Spark cluster. It goes through several phases:\n",
            "\n",
            "1.  **Parsing and Analysis:** Parses the SQL query or DataFrame operations into an Abstract Syntax Tree (AST) and then resolves table names, column names, and data types against the catalog (like Hive Metastore).\n",
            "2.  **Logical Optimization:** Applies a set of rules to the logical plan to optimize it independently of the execution engine. This includes:\n",
            "    -   Predicate Pushdown: Moving filters closer to the data source to reduce the amount of data read.\n",
            "    -   Column Pruning: Removing columns that are not needed for the final result.\n",
            "    -   Constant Folding: Replacing constant expressions with their computed values.\n",
            "    -   Join Reordering: Changing the order of joins to minimize intermediate data size.\n",
            "3.  **Physical Planning:** Converts the optimized logical plan into one or more physical plans. This involves choosing specific physical operators for each logical operation (e.g., choosing between sort merge join, hash join, or broadcast hash join).\n",
            "4.  **Code Generation:** For parts of the query, Spark can generate optimized JVM bytecode using the Tungsten execution engine, which further improves CPU efficiency.\n",
            "\n",
            "By applying these optimizations, Catalyst significantly reduces the amount of data read, processed, and shuffled, leading to faster query execution.\n",
            "\n",
            "\n",
            "--- Lazy Evaluation and Optimization ---\n",
            "\n",
            "Spark operations are categorized into transformations and actions. Transformations (like `select`, `filter`, `withColumn`) are lazy; they don't execute immediately but build a Directed Acyclic Graph (DAG) of operations. Actions (like `show`, `count`, `collect`, `write`) trigger the actual computation.\n",
            "\n",
            "How Lazy Evaluation Enables Optimization:\n",
            "Lazy evaluation allows Spark's Catalyst optimizer to see the entire lineage of transformations leading up to an action. This global view enables cross-operation optimizations that would not be possible if each transformation was executed immediately.\n",
            "\n",
            "For example, if you chain a `filter` followed by a `select`, Spark can:\n",
            "-   Push the filter down: Apply the filter *before* reading or processing unnecessary data.\n",
            "-   Prune columns: Only read the columns required by the `select` *after* the filter is applied.\n",
            "\n",
            "Without lazy evaluation, Spark would execute the `filter`, materialize an intermediate DataFrame, and then execute the `select` on that intermediate result, missing opportunities for optimization. Lazy evaluation allows Spark to build an optimal execution plan for the entire sequence of operations.\n",
            "\n",
            "\n",
            "Sample DataFrame:\n",
            "+-------+---+-----------+---+\n",
            "|   Name| ID|       City|Age|\n",
            "+-------+---+-----------+---+\n",
            "|  Alice|  1|   New York| 30|\n",
            "|    Bob|  2|Los Angeles| 35|\n",
            "|Charlie|  3|    Chicago| 40|\n",
            "|  Alice|  4|   New York| 25|\n",
            "|    Bob|  5|Los Angeles| 32|\n",
            "|  David|  6|    Chicago| 45|\n",
            "|    Eve|  7|   New York| 28|\n",
            "|  Frank|  8|Los Angeles| 38|\n",
            "+-------+---+-----------+---+\n",
            "\n",
            "\n",
            "--- Caching and Persisting ---\n",
            "\n",
            "Caching and persisting are optimization techniques used to store an intermediate DataFrame or RDD in memory (or on disk) across multiple operations. This is crucial when you plan to reuse the same DataFrame/RDD multiple times in your application.\n",
            "\n",
            "Importance for Performance:\n",
            "When you perform transformations on a DataFrame, Spark builds a lineage (DAG) of dependencies. If an action is called, Spark recomputes the entire lineage up to that point. If you reuse the same DataFrame for another action without caching, Spark will recompute the lineage again, which can be very inefficient, especially for complex or time-consuming transformations.\n",
            "\n",
            "`cache()` and `persist()` store the DataFrame's partitions in the cluster's memory (by default). Subsequent actions that use this cached DataFrame will read the data from memory instead of recomputing it from the source or previous transformations.\n",
            "\n",
            "-   `.cache()`: A shorthand for `persist()` with the default storage level (`MEMORY_AND_DISK`).\n",
            "-   `.persist(storageLevel)`: Allows you to specify different storage levels (e.g., `MEMORY_ONLY`, `DISK_ONLY`, `MEMORY_AND_DISK_SER`).\n",
            "\n",
            "Example: Using `.cache()`\n",
            "\n",
            "Caching the DataFrame...\n",
            "First action (count) on cached DataFrame:\n",
            "Count: 5, Time taken (first action): 1.2582 seconds\n",
            "\n",
            "Second action (show) on cached DataFrame:\n",
            "+-------+---+-----------+---+\n",
            "|   Name| ID|       City|Age|\n",
            "+-------+---+-----------+---+\n",
            "|    Bob|  2|Los Angeles| 35|\n",
            "|Charlie|  3|    Chicago| 40|\n",
            "|    Bob|  5|Los Angeles| 32|\n",
            "|  David|  6|    Chicago| 45|\n",
            "|  Frank|  8|Los Angeles| 38|\n",
            "+-------+---+-----------+---+\n",
            "\n",
            "Time taken (second action): 0.3947 seconds\n",
            "\n",
            "DataFrame unpersisted.\n",
            "\n",
            "--- Broadcasting Joins ---\n",
            "\n",
            "Shuffling data during a join operation can be very expensive, especially when one DataFrame is much larger than the other. Broadcasting is an optimization technique where the smaller DataFrame is sent to all partitions of the larger DataFrame. This allows the join to be performed locally on each partition of the larger DataFrame without requiring a shuffle of the larger dataset.\n",
            "\n",
            "When to Use Broadcasting:\n",
            "- When one DataFrame is significantly smaller than the other.\n",
            "- The smaller DataFrame should ideally fit comfortably into the memory of each worker node.\n",
            "- Spark can automatically broadcast small tables, but you can explicitly hint using `F.broadcast()`.\n",
            "\n",
            "Example: Using `F.broadcast()`\n",
            "\n",
            "Larger Employees DataFrame:\n",
            "+-------+---+-----------+---+\n",
            "|   Name| ID|       City|Age|\n",
            "+-------+---+-----------+---+\n",
            "|  Alice|  1|   New York| 30|\n",
            "|    Bob|  2|Los Angeles| 35|\n",
            "|Charlie|  3|    Chicago| 40|\n",
            "|  Alice|  4|   New York| 25|\n",
            "|    Bob|  5|Los Angeles| 32|\n",
            "|  David|  6|    Chicago| 45|\n",
            "|    Eve|  7|   New York| 28|\n",
            "|  Frank|  8|Los Angeles| 38|\n",
            "+-------+---+-----------+---+\n",
            "\n",
            "\n",
            "Smaller Departments DataFrame:\n",
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|        IT|Building B|\n",
            "+----------+----------+\n",
            "\n",
            "\n",
            "Performing a Broadcast Hash Join:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_USING_COLUMN_FOR_JOIN] USING column `Department` cannot be resolved on the left side of the join. The left-side columns: [`Age`, `City`, `ID`, `Name`].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-909458156.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m# Spark will likely broadcast the smaller df_departments_small automatically,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m# but we can hint using F.broadcast() for explicit control or to override Spark's decision threshold.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0mjoined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_departments_small\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Department\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mjoined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2489\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be a string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2491\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_USING_COLUMN_FOR_JOIN] USING column `Department` cannot be resolved on the left side of the join. The left-side columns: [`Age`, `City`, `ID`, `Name`]."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55142296"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the 'Department' column was not present in the left DataFrame (`df`) during the join operation. The error message clearly indicates that the left-side columns were `Age`, `City`, `ID`, and `Name`. I need to modify the sample data for the `df` DataFrame to include a 'Department' column so that the join on 'Department' is valid. I will regenerate the code block with this fix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwotE9dQQrCT",
        "outputId": "dff9848e-9fc0-45a1-ac57-75f9b289b865"
      },
      "source": [
        "# Explicitly stop any existing SparkSession before creating a new one\n",
        "try:\n",
        "    spark.stop()\n",
        "except NameError:\n",
        "    # SparkSession was not defined, so nothing to stop\n",
        "    pass\n",
        "except Exception as e:\n",
        "    print(f\"Error stopping existing SparkSession: {e}\")\n",
        "\n",
        "# Create a new SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkOptimizationTechniques\").getOrCreate()\n",
        "\n",
        "# 1. Explain the role of the Catalyst Optimizer in Spark SQL and how it contributes to performance.\n",
        "print(\"--- Catalyst Optimizer ---\")\n",
        "print(\"\"\"\n",
        "The Catalyst Optimizer is Spark SQL's extensible query optimizer. It is a cost-based optimizer that uses advanced programming language features (like Scala's pattern matching and quasi-quotes) to build a highly extensible and powerful optimization framework.\n",
        "\n",
        "How it Contributes to Performance:\n",
        "Catalyst translates the logical plan (representing the user's DataFrame/SQL query) into an optimized physical plan that can be executed efficiently on the Spark cluster. It goes through several phases:\n",
        "\n",
        "1.  **Parsing and Analysis:** Parses the SQL query or DataFrame operations into an Abstract Syntax Tree (AST) and then resolves table names, column names, and data types against the catalog (like Hive Metastore).\n",
        "2.  **Logical Optimization:** Applies a set of rules to the logical plan to optimize it independently of the execution engine. This includes:\n",
        "    -   Predicate Pushdown: Moving filters closer to the data source to reduce the amount of data read.\n",
        "    -   Column Pruning: Removing columns that are not needed for the final result.\n",
        "    -   Constant Folding: Replacing constant expressions with their computed values.\n",
        "    -   Join Reordering: Changing the order of joins to minimize intermediate data size.\n",
        "3.  **Physical Planning:** Converts the optimized logical plan into one or more physical plans. This involves choosing specific physical operators for each logical operation (e.g., choosing between sort merge join, hash join, or broadcast hash join).\n",
        "4.  **Code Generation:** For parts of the query, Spark can generate optimized JVM bytecode using the Tungsten execution engine, which further improves CPU efficiency.\n",
        "\n",
        "By applying these optimizations, Catalyst significantly reduces the amount of data read, processed, and shuffled, leading to faster query execution.\n",
        "\"\"\")\n",
        "\n",
        "# 2. Discuss the concept of lazy evaluation and how it enables optimization in Spark.\n",
        "print(\"\\n--- Lazy Evaluation and Optimization ---\")\n",
        "print(\"\"\"\n",
        "Spark operations are categorized into transformations and actions. Transformations (like `select`, `filter`, `withColumn`) are lazy; they don't execute immediately but build a Directed Acyclic Graph (DAG) of operations. Actions (like `show`, `count`, `collect`, `write`) trigger the actual computation.\n",
        "\n",
        "How Lazy Evaluation Enables Optimization:\n",
        "Lazy evaluation allows Spark's Catalyst optimizer to see the entire lineage of transformations leading up to an action. This global view enables cross-operation optimizations that would not be possible if each transformation was executed immediately.\n",
        "\n",
        "For example, if you chain a `filter` followed by a `select`, Spark can:\n",
        "-   Push the filter down: Apply the filter *before* reading or processing unnecessary data.\n",
        "-   Prune columns: Only read the columns required by the `select` *after* the filter is applied.\n",
        "\n",
        "Without lazy evaluation, Spark would execute the `filter`, materialize an intermediate DataFrame, and then execute the `select` on that intermediate result, missing opportunities for optimization. Lazy evaluation allows Spark to build an optimal execution plan for the entire sequence of operations.\n",
        "\"\"\")\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "# Fix: Add 'Department' column to the sample data\n",
        "data = [(\"Alice\", 1, \"New York\", 30, \"Sales\"),\n",
        "        (\"Bob\", 2, \"Los Angeles\", 35, \"IT\"),\n",
        "        (\"Charlie\", 3, \"Chicago\", 40, \"Sales\"),\n",
        "        (\"Alice\", 4, \"New York\", 25, \"IT\"),\n",
        "        (\"Bob\", 5, \"Los Angeles\", 32, \"Sales\"),\n",
        "        (\"David\", 6, \"Chicago\", 45, \"IT\"),\n",
        "        (\"Eve\", 7, \"New York\", 28, \"Sales\"),\n",
        "        (\"Frank\", 8, \"Los Angeles\", 38, \"IT\")]\n",
        "columns = [\"Name\", \"ID\", \"City\", \"Age\", \"Department\"] # Fix: Add 'Department' to columns\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"\\nSample DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# 3. Explain the importance of caching and persisting DataFrames/RDDs for performance,\n",
        "# and provide code examples for using `.cache()` and `.persist()`.\n",
        "print(\"\\n--- Caching and Persisting ---\")\n",
        "print(\"\"\"\n",
        "Caching and persisting are optimization techniques used to store an intermediate DataFrame or RDD in memory (or on disk) across multiple operations. This is crucial when you plan to reuse the same DataFrame/RDD multiple times in your application.\n",
        "\n",
        "Importance for Performance:\n",
        "When you perform transformations on a DataFrame, Spark builds a lineage (DAG) of dependencies. If an action is called, Spark recomputes the entire lineage up to that point. If you reuse the same DataFrame for another action without caching, Spark will recompute the lineage again, which can be very inefficient, especially for complex or time-consuming transformations.\n",
        "\n",
        "`cache()` and `persist()` store the DataFrame's partitions in the cluster's memory (by default). Subsequent actions that use this cached DataFrame will read the data from memory instead of recomputing it from the source or previous transformations.\n",
        "\n",
        "-   `.cache()`: A shorthand for `persist()` with the default storage level (`MEMORY_AND_DISK`).\n",
        "-   `.persist(storageLevel)`: Allows you to specify different storage levels (e.g., `MEMORY_ONLY`, `DISK_ONLY`, `MEMORY_AND_DISK_SER`).\n",
        "\n",
        "Example: Using `.cache()`\n",
        "\"\"\")\n",
        "\n",
        "# Demonstrate caching\n",
        "print(\"Caching the DataFrame...\")\n",
        "start_time = time.time()\n",
        "df_filtered = df.filter(df.Age > 30)\n",
        "df_filtered.cache() # Cache the filtered DataFrame\n",
        "# The first action will trigger computation and caching\n",
        "print(\"First action (count) on cached DataFrame:\")\n",
        "count1 = df_filtered.count()\n",
        "end_time_first_action = time.time()\n",
        "print(f\"Count: {count1}, Time taken (first action): {end_time_first_action - start_time:.4f} seconds\")\n",
        "\n",
        "# Subsequent actions on the cached DataFrame will be faster\n",
        "print(\"\\nSecond action (show) on cached DataFrame:\")\n",
        "start_time_second_action = time.time()\n",
        "df_filtered.show()\n",
        "end_time_second_action = time.time()\n",
        "print(f\"Time taken (second action): {end_time_second_action - start_time_second_action:.4f} seconds\")\n",
        "\n",
        "# Example: Using .persist() with a different storage level (e.g., DISK_ONLY)\n",
        "# from pyspark import StorageLevel\n",
        "# print(\"\\nPersisting the DataFrame to DISK_ONLY...\")\n",
        "# df_filtered.persist(StorageLevel.DISK_ONLY)\n",
        "# df_filtered.count() # Trigger computation and persistence\n",
        "# print(\"DataFrame persisted to DISK_ONLY.\")\n",
        "# # Subsequent actions will read from disk\n",
        "\n",
        "# Unpersist the DataFrame when no longer needed\n",
        "df_filtered.unpersist()\n",
        "print(\"\\nDataFrame unpersisted.\")\n",
        "\n",
        "\n",
        "# 4. Explain the concept of broadcasting small DataFrames/RDDs during joins to avoid shuffling the larger dataset,\n",
        "# and provide a code example demonstrating the use of `F.broadcast()`.\n",
        "print(\"\\n--- Broadcasting Joins ---\")\n",
        "print(\"\"\"\n",
        "Shuffling data during a join operation can be very expensive, especially when one DataFrame is much larger than the other. Broadcasting is an optimization technique where the smaller DataFrame is sent to all partitions of the larger DataFrame. This allows the join to be performed locally on each partition of the larger DataFrame without requiring a shuffle of the larger dataset.\n",
        "\n",
        "When to Use Broadcasting:\n",
        "- When one DataFrame is significantly smaller than the other.\n",
        "- The smaller DataFrame should ideally fit comfortably into the memory of each worker node.\n",
        "- Spark can automatically broadcast small tables, but you can explicitly hint using `F.broadcast()`.\n",
        "\n",
        "Example: Using `F.broadcast()`\n",
        "\"\"\")\n",
        "\n",
        "# Create a smaller DataFrame for departments\n",
        "data_departments_small = [(\"Sales\", \"Building A\"),\n",
        "                          (\"IT\", \"Building B\")]\n",
        "columns_departments_small = [\"Department\", \"Location\"]\n",
        "df_departments_small = spark.createDataFrame(data_departments_small, columns_departments_small)\n",
        "\n",
        "print(\"Larger Employees DataFrame:\")\n",
        "df.show() # Reusing the main df\n",
        "print(\"\\nSmaller Departments DataFrame:\")\n",
        "df_departments_small.show()\n",
        "\n",
        "print(\"\\nPerforming a Broadcast Hash Join:\")\n",
        "# Join the larger employee DataFrame with the smaller department DataFrame\n",
        "# Spark will likely broadcast the smaller df_departments_small automatically,\n",
        "# but we can hint using F.broadcast() for explicit control or to override Spark's decision threshold.\n",
        "joined_df = df.join(F.broadcast(df_departments_small), on=\"Department\", how=\"inner\")\n",
        "\n",
        "joined_df.show()\n",
        "\n",
        "print(\"\"\"\n",
        "In the Spark UI's SQL tab for this query, you should see a \"BroadcastHashJoin\" physical operator, indicating that the smaller DataFrame was broadcasted. This avoids shuffling the larger 'df' (Employees) DataFrame.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# 5. Discuss shuffling, why it occurs (e.g., during joins, aggregations), and its impact on performance.\n",
        "# Briefly mention strategies to mitigate excessive shuffling (e.g., partitioning, avoiding wide transformations).\n",
        "print(\"\\n--- Shuffling ---\")\n",
        "print(\"\"\"\n",
        "Shuffling is a process where data is redistributed across partitions in the Spark cluster. It occurs when Spark needs to bring data with the same key or value together to perform an operation.\n",
        "\n",
        "Why it Occurs:\n",
        "Shuffling is typically triggered by **wide transformations**, which are transformations that require data from multiple partitions to be combined or grouped. Common operations that cause shuffling include:\n",
        "-   `groupBy()`\n",
        "-   `orderBy()` / `sort()`\n",
        "-   `join()` (unless it's a broadcast join)\n",
        "-   `distinct()`\n",
        "-   `repartition()`\n",
        "\n",
        "Impact on Performance:\n",
        "Shuffling is one of the most expensive operations in Spark because it involves:\n",
        "-   **Disk I/O:** Data from different partitions needs to be written to disk on the source nodes.\n",
        "-   **Network Transfer:** Data is transferred over the network to the destination nodes.\n",
        "-   **Serialization/Deserialization:** Data needs to be serialized before sending and deserialized upon receiving.\n",
        "\n",
        "Excessive shuffling can lead to significant performance bottlenecks, increased job execution time, and potential out-of-memory errors.\n",
        "\n",
        "Strategies to Mitigate Excessive Shuffling:\n",
        "-   **Minimize Wide Transformations:** Try to structure your logic to perform narrow transformations (operations that don't require shuffling, like `filter`, `select`, `withColumn`) as early as possible.\n",
        "-   **Broadcast Joins:** As discussed, broadcast smaller tables during joins to avoid shuffling the larger table.\n",
        "-   **Partitioning:** Ensure DataFrames are partitioned appropriately *before* shuffle-heavy operations if possible. For example, if you frequently join or group by a specific column, consider repartitioning your DataFrame by that column beforehand. This can reduce the amount of data that needs to be shuffled in subsequent operations.\n",
        "-   **Tune `spark.sql.shuffle.partitions`:** This configuration property determines the number of partitions used in shuffle stages. Increasing it can increase parallelism but also increases the number of smaller shuffle files. Decreasing it reduces the number of tasks but can lead to larger partitions and potential data skew issues. Tuning depends on your cluster size and data characteristics.\n",
        "-   **Data Skew Handling:** Implement strategies to handle skewed data (e.g., salting) for shuffle-heavy operations if the default skew handling is insufficient.\n",
        "-   **Use Columnar Formats:** Parquet and ORC reduce I/O, which indirectly helps by making the reading part of the shuffle process faster.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# 6. Explain other optimization techniques, such as column pruning, predicate pushdown, and choosing appropriate data formats (e.g., Parquet), and how they improve query performance.\n",
        "print(\"\\n--- Other Optimization Techniques ---\")\n",
        "print(\"\"\"\n",
        "Spark's Catalyst optimizer automatically applies several other techniques:\n",
        "\n",
        "-   **Column Pruning:** The optimizer identifies columns that are not required for the final result of a query or transformation chain and avoids reading or processing them. This reduces I/O and memory usage.\n",
        "    Example: If you `select` only a few columns from a wide table stored in a columnar format like Parquet, Spark will only read those specific columns.\n",
        "\n",
        "-   **Predicate Pushdown:** Filters (predicates) are pushed down as close to the data source as possible. This allows Spark to filter data early, reducing the amount of data that needs to be read from storage and processed in subsequent stages.\n",
        "    Example: `spark.read.parquet(\"/path/to/data\").filter(\"column > 10\")` - Spark will try to apply the filter `column > 10` when reading the Parquet file, potentially skipping entire row groups that don't satisfy the condition.\n",
        "\n",
        "-   **Choosing Appropriate Data Formats:**\n",
        "    -   **Columnar Formats (Parquet, ORC):** Highly recommended for analytical workloads. They enable column pruning and often provide better compression. Reading specific columns is much faster than in row-based formats.\n",
        "    -   **Text/CSV:** Simple, human-readable, but less efficient for analytical queries, especially on wide tables, as the entire row must be read even if only a few columns are needed. Schema inference can also add overhead.\n",
        "    -   **JSON:** Flexible for semi-structured data but can be less performant than columnar formats for highly structured data due to parsing overhead.\n",
        "\n",
        "-   **Partitioning Data at Rest:** Storing data partitioned in the file system (e.g., `.../year=2023/month=01/...`) allows Spark to use partition pruning. If a query includes a filter on the partition column (e.g., `WHERE year = 2023`), Spark can read data only from the relevant directories, significantly reducing I/O. This is common with formats like Parquet and ORC.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# 7. Briefly mention the Spark UI as a tool for monitoring and identifying performance bottlenecks.\n",
        "print(\"\\n--- Using the Spark UI for Monitoring and Tuning ---\")\n",
        "print(\"\"\"\n",
        "The Spark UI (accessible via a web browser at `http://<driver-node>:4040` by default when a Spark application is running) is an invaluable tool for understanding and optimizing Spark jobs.\n",
        "\n",
        "Key areas in the Spark UI for performance analysis:\n",
        "-   **Jobs Tab:** Provides an overview of all jobs, their status, and duration.\n",
        "-   **Stages Tab:** Shows the different stages of a job, including the number of tasks, duration, and input/output metrics. This is where you can often identify bottlenecks.\n",
        "-   **Tasks Tab:** Detailed information about individual tasks within a stage, including runtime, locality, and shuffle read/write. Helps identify skewed tasks.\n",
        "-   **SQL Tab:** Shows the logical and physical plan generated by the Catalyst optimizer for Spark SQL queries and DataFrame operations. This is critical for verifying if optimizations like predicate pushdown or broadcast joins are being applied.\n",
        "-   **Storage Tab:** Shows information about cached/persisted RDDs and DataFrames.\n",
        "\n",
        "By examining the Spark UI, you can identify:\n",
        "-   Stages with high shuffle read/write (indicating excessive shuffling).\n",
        "-   Skewed stages (tasks with vastly different runtimes).\n",
        "-   Stages with poor data locality (tasks reading data from remote nodes).\n",
        "-   Whether broadcast joins are being used as expected.\n",
        "-   The effectiveness of caching.\n",
        "\n",
        "Monitoring the Spark UI is a fundamental step in diagnosing and resolving performance issues in Spark applications.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Catalyst Optimizer ---\n",
            "\n",
            "The Catalyst Optimizer is Spark SQL's extensible query optimizer. It is a cost-based optimizer that uses advanced programming language features (like Scala's pattern matching and quasi-quotes) to build a highly extensible and powerful optimization framework.\n",
            "\n",
            "How it Contributes to Performance:\n",
            "Catalyst translates the logical plan (representing the user's DataFrame/SQL query) into an optimized physical plan that can be executed efficiently on the Spark cluster. It goes through several phases:\n",
            "\n",
            "1.  **Parsing and Analysis:** Parses the SQL query or DataFrame operations into an Abstract Syntax Tree (AST) and then resolves table names, column names, and data types against the catalog (like Hive Metastore).\n",
            "2.  **Logical Optimization:** Applies a set of rules to the logical plan to optimize it independently of the execution engine. This includes:\n",
            "    -   Predicate Pushdown: Moving filters closer to the data source to reduce the amount of data read.\n",
            "    -   Column Pruning: Removing columns that are not needed for the final result.\n",
            "    -   Constant Folding: Replacing constant expressions with their computed values.\n",
            "    -   Join Reordering: Changing the order of joins to minimize intermediate data size.\n",
            "3.  **Physical Planning:** Converts the optimized logical plan into one or more physical plans. This involves choosing specific physical operators for each logical operation (e.g., choosing between sort merge join, hash join, or broadcast hash join).\n",
            "4.  **Code Generation:** For parts of the query, Spark can generate optimized JVM bytecode using the Tungsten execution engine, which further improves CPU efficiency.\n",
            "\n",
            "By applying these optimizations, Catalyst significantly reduces the amount of data read, processed, and shuffled, leading to faster query execution.\n",
            "\n",
            "\n",
            "--- Lazy Evaluation and Optimization ---\n",
            "\n",
            "Spark operations are categorized into transformations and actions. Transformations (like `select`, `filter`, `withColumn`) are lazy; they don't execute immediately but build a Directed Acyclic Graph (DAG) of operations. Actions (like `show`, `count`, `collect`, `write`) trigger the actual computation.\n",
            "\n",
            "How Lazy Evaluation Enables Optimization:\n",
            "Lazy evaluation allows Spark's Catalyst optimizer to see the entire lineage of transformations leading up to an action. This global view enables cross-operation optimizations that would not be possible if each transformation was executed immediately.\n",
            "\n",
            "For example, if you chain a `filter` followed by a `select`, Spark can:\n",
            "-   Push the filter down: Apply the filter *before* reading or processing unnecessary data.\n",
            "-   Prune columns: Only read the columns required by the `select` *after* the filter is applied.\n",
            "\n",
            "Without lazy evaluation, Spark would execute the `filter`, materialize an intermediate DataFrame, and then execute the `select` on that intermediate result, missing opportunities for optimization. Lazy evaluation allows Spark to build an optimal execution plan for the entire sequence of operations.\n",
            "\n",
            "\n",
            "Sample DataFrame:\n",
            "+-------+---+-----------+---+----------+\n",
            "|   Name| ID|       City|Age|Department|\n",
            "+-------+---+-----------+---+----------+\n",
            "|  Alice|  1|   New York| 30|     Sales|\n",
            "|    Bob|  2|Los Angeles| 35|        IT|\n",
            "|Charlie|  3|    Chicago| 40|     Sales|\n",
            "|  Alice|  4|   New York| 25|        IT|\n",
            "|    Bob|  5|Los Angeles| 32|     Sales|\n",
            "|  David|  6|    Chicago| 45|        IT|\n",
            "|    Eve|  7|   New York| 28|     Sales|\n",
            "|  Frank|  8|Los Angeles| 38|        IT|\n",
            "+-------+---+-----------+---+----------+\n",
            "\n",
            "\n",
            "--- Caching and Persisting ---\n",
            "\n",
            "Caching and persisting are optimization techniques used to store an intermediate DataFrame or RDD in memory (or on disk) across multiple operations. This is crucial when you plan to reuse the same DataFrame/RDD multiple times in your application.\n",
            "\n",
            "Importance for Performance:\n",
            "When you perform transformations on a DataFrame, Spark builds a lineage (DAG) of dependencies. If an action is called, Spark recomputes the entire lineage up to that point. If you reuse the same DataFrame for another action without caching, Spark will recompute the lineage again, which can be very inefficient, especially for complex or time-consuming transformations.\n",
            "\n",
            "`cache()` and `persist()` store the DataFrame's partitions in the cluster's memory (by default). Subsequent actions that use this cached DataFrame will read the data from memory instead of recomputing it from the source or previous transformations.\n",
            "\n",
            "-   `.cache()`: A shorthand for `persist()` with the default storage level (`MEMORY_AND_DISK`).\n",
            "-   `.persist(storageLevel)`: Allows you to specify different storage levels (e.g., `MEMORY_ONLY`, `DISK_ONLY`, `MEMORY_AND_DISK_SER`).\n",
            "\n",
            "Example: Using `.cache()`\n",
            "\n",
            "Caching the DataFrame...\n",
            "First action (count) on cached DataFrame:\n",
            "Count: 5, Time taken (first action): 0.5842 seconds\n",
            "\n",
            "Second action (show) on cached DataFrame:\n",
            "+-------+---+-----------+---+----------+\n",
            "|   Name| ID|       City|Age|Department|\n",
            "+-------+---+-----------+---+----------+\n",
            "|    Bob|  2|Los Angeles| 35|        IT|\n",
            "|Charlie|  3|    Chicago| 40|     Sales|\n",
            "|    Bob|  5|Los Angeles| 32|     Sales|\n",
            "|  David|  6|    Chicago| 45|        IT|\n",
            "|  Frank|  8|Los Angeles| 38|        IT|\n",
            "+-------+---+-----------+---+----------+\n",
            "\n",
            "Time taken (second action): 0.2189 seconds\n",
            "\n",
            "DataFrame unpersisted.\n",
            "\n",
            "--- Broadcasting Joins ---\n",
            "\n",
            "Shuffling data during a join operation can be very expensive, especially when one DataFrame is much larger than the other. Broadcasting is an optimization technique where the smaller DataFrame is sent to all partitions of the larger DataFrame. This allows the join to be performed locally on each partition of the larger DataFrame without requiring a shuffle of the larger dataset.\n",
            "\n",
            "When to Use Broadcasting:\n",
            "- When one DataFrame is significantly smaller than the other.\n",
            "- The smaller DataFrame should ideally fit comfortably into the memory of each worker node.\n",
            "- Spark can automatically broadcast small tables, but you can explicitly hint using `F.broadcast()`.\n",
            "\n",
            "Example: Using `F.broadcast()`\n",
            "\n",
            "Larger Employees DataFrame:\n",
            "+-------+---+-----------+---+----------+\n",
            "|   Name| ID|       City|Age|Department|\n",
            "+-------+---+-----------+---+----------+\n",
            "|  Alice|  1|   New York| 30|     Sales|\n",
            "|    Bob|  2|Los Angeles| 35|        IT|\n",
            "|Charlie|  3|    Chicago| 40|     Sales|\n",
            "|  Alice|  4|   New York| 25|        IT|\n",
            "|    Bob|  5|Los Angeles| 32|     Sales|\n",
            "|  David|  6|    Chicago| 45|        IT|\n",
            "|    Eve|  7|   New York| 28|     Sales|\n",
            "|  Frank|  8|Los Angeles| 38|        IT|\n",
            "+-------+---+-----------+---+----------+\n",
            "\n",
            "\n",
            "Smaller Departments DataFrame:\n",
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|        IT|Building B|\n",
            "+----------+----------+\n",
            "\n",
            "\n",
            "Performing a Broadcast Hash Join:\n",
            "+----------+-------+---+-----------+---+----------+\n",
            "|Department|   Name| ID|       City|Age|  Location|\n",
            "+----------+-------+---+-----------+---+----------+\n",
            "|     Sales|  Alice|  1|   New York| 30|Building A|\n",
            "|        IT|    Bob|  2|Los Angeles| 35|Building B|\n",
            "|     Sales|Charlie|  3|    Chicago| 40|Building A|\n",
            "|        IT|  Alice|  4|   New York| 25|Building B|\n",
            "|     Sales|    Bob|  5|Los Angeles| 32|Building A|\n",
            "|        IT|  David|  6|    Chicago| 45|Building B|\n",
            "|     Sales|    Eve|  7|   New York| 28|Building A|\n",
            "|        IT|  Frank|  8|Los Angeles| 38|Building B|\n",
            "+----------+-------+---+-----------+---+----------+\n",
            "\n",
            "\n",
            "In the Spark UI's SQL tab for this query, you should see a \"BroadcastHashJoin\" physical operator, indicating that the smaller DataFrame was broadcasted. This avoids shuffling the larger 'df' (Employees) DataFrame.\n",
            "\n",
            "\n",
            "--- Shuffling ---\n",
            "\n",
            "Shuffling is a process where data is redistributed across partitions in the Spark cluster. It occurs when Spark needs to bring data with the same key or value together to perform an operation.\n",
            "\n",
            "Why it Occurs:\n",
            "Shuffling is typically triggered by **wide transformations**, which are transformations that require data from multiple partitions to be combined or grouped. Common operations that cause shuffling include:\n",
            "-   `groupBy()`\n",
            "-   `orderBy()` / `sort()`\n",
            "-   `join()` (unless it's a broadcast join)\n",
            "-   `distinct()`\n",
            "-   `repartition()`\n",
            "\n",
            "Impact on Performance:\n",
            "Shuffling is one of the most expensive operations in Spark because it involves:\n",
            "-   **Disk I/O:** Data from different partitions needs to be written to disk on the source nodes.\n",
            "-   **Network Transfer:** Data is transferred over the network to the destination nodes.\n",
            "-   **Serialization/Deserialization:** Data needs to be serialized before sending and deserialized upon receiving.\n",
            "\n",
            "Excessive shuffling can lead to significant performance bottlenecks, increased job execution time, and potential out-of-memory errors.\n",
            "\n",
            "Strategies to Mitigate Excessive Shuffling:\n",
            "-   **Minimize Wide Transformations:** Try to structure your logic to perform narrow transformations (operations that don't require shuffling, like `filter`, `select`, `withColumn`) as early as possible.\n",
            "-   **Broadcast Joins:** As discussed, broadcast smaller tables during joins to avoid shuffling the larger table.\n",
            "-   **Partitioning:** Ensure DataFrames are partitioned appropriately *before* shuffle-heavy operations if possible. For example, if you frequently join or group by a specific column, consider repartitioning your DataFrame by that column beforehand. This can reduce the amount of data that needs to be shuffled in subsequent operations.\n",
            "-   **Tune `spark.sql.shuffle.partitions`:** This configuration property determines the number of partitions used in shuffle stages. Increasing it can increase parallelism but also increases the number of smaller shuffle files. Decreasing it reduces the number of tasks but can lead to larger partitions and potential data skew issues. Tuning depends on your cluster size and data characteristics.\n",
            "-   **Data Skew Handling:** Implement strategies to handle skewed data (e.g., salting) for shuffle-heavy operations if the default skew handling is insufficient.\n",
            "-   **Use Columnar Formats:** Parquet and ORC reduce I/O, which indirectly helps by making the reading part of the shuffle process faster.\n",
            "\n",
            "\n",
            "--- Other Optimization Techniques ---\n",
            "\n",
            "Spark's Catalyst optimizer automatically applies several other techniques:\n",
            "\n",
            "-   **Column Pruning:** The optimizer identifies columns that are not required for the final result of a query or transformation chain and avoids reading or processing them. This reduces I/O and memory usage.\n",
            "    Example: If you `select` only a few columns from a wide table stored in a columnar format like Parquet, Spark will only read those specific columns.\n",
            "\n",
            "-   **Predicate Pushdown:** Filters (predicates) are pushed down as close to the data source as possible. This allows Spark to filter data early, reducing the amount of data that needs to be read from storage and processed in subsequent stages.\n",
            "    Example: `spark.read.parquet(\"/path/to/data\").filter(\"column > 10\")` - Spark will try to apply the filter `column > 10` when reading the Parquet file, potentially skipping entire row groups that don't satisfy the condition.\n",
            "\n",
            "-   **Choosing Appropriate Data Formats:**\n",
            "    -   **Columnar Formats (Parquet, ORC):** Highly recommended for analytical workloads. They enable column pruning and often provide better compression. Reading specific columns is much faster than in row-based formats.\n",
            "    -   **Text/CSV:** Simple, human-readable, but less efficient for analytical queries, especially on wide tables, as the entire row must be read even if only a few columns are needed. Schema inference can also add overhead.\n",
            "    -   **JSON:** Flexible for semi-structured data but can be less performant than columnar formats for highly structured data due to parsing overhead.\n",
            "\n",
            "-   **Partitioning Data at Rest:** Storing data partitioned in the file system (e.g., `.../year=2023/month=01/...`) allows Spark to use partition pruning. If a query includes a filter on the partition column (e.g., `WHERE year = 2023`), Spark can read data only from the relevant directories, significantly reducing I/O. This is common with formats like Parquet and ORC.\n",
            "\n",
            "\n",
            "--- Using the Spark UI for Monitoring and Tuning ---\n",
            "\n",
            "The Spark UI (accessible via a web browser at `http://<driver-node>:4040` by default when a Spark application is running) is an invaluable tool for understanding and optimizing Spark jobs.\n",
            "\n",
            "Key areas in the Spark UI for performance analysis:\n",
            "-   **Jobs Tab:** Provides an overview of all jobs, their status, and duration.\n",
            "-   **Stages Tab:** Shows the different stages of a job, including the number of tasks, duration, and input/output metrics. This is where you can often identify bottlenecks.\n",
            "-   **Tasks Tab:** Detailed information about individual tasks within a stage, including runtime, locality, and shuffle read/write. Helps identify skewed tasks.\n",
            "-   **SQL Tab:** Shows the logical and physical plan generated by the Catalyst optimizer for Spark SQL queries and DataFrame operations. This is critical for verifying if optimizations like predicate pushdown or broadcast joins are being applied.\n",
            "-   **Storage Tab:** Shows information about cached/persisted RDDs and DataFrames.\n",
            "\n",
            "By examining the Spark UI, you can identify:\n",
            "-   Stages with high shuffle read/write (indicating excessive shuffling).\n",
            "-   Skewed stages (tasks with vastly different runtimes).\n",
            "-   Stages with poor data locality (tasks reading data from remote nodes).\n",
            "-   Whether broadcast joins are being used as expected.\n",
            "-   The effectiveness of caching.\n",
            "\n",
            "Monitoring the Spark UI is a fundamental step in diagnosing and resolving performance issues in Spark applications.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2da76d0a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command successfully executed and printed the explanations and code examples for Catalyst Optimizer, lazy evaluation, caching, broadcasting, and shuffling. It also started explaining other optimization techniques but the output was truncated. I will continue by printing the remaining explanations for other optimization techniques and the Spark UI, and then finish the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42da86ce",
        "outputId": "b95e485c-2031-41a5-80c9-9271cf7694be"
      },
      "source": [
        "# Continue printing the explanation for Other Optimization Techniques\n",
        "print(\"\"\"\n",
        "... needs to be read from storage and processed in subsequent stages.\n",
        "    Example: `spark.read.parquet(\"/path/to/data\").filter(\"column > 10\")` - Spark will try to apply the filter `column > 10` when reading the Parquet file, potentially skipping entire row groups that don't satisfy the condition.\n",
        "\n",
        "-   **Choosing Appropriate Data Formats:**\n",
        "    -   **Columnar Formats (Parquet, ORC):** Highly recommended for analytical workloads. They enable column pruning and often provide better compression. Reading specific columns is much faster than in row-based formats.\n",
        "    -   **Text/CSV:** Simple, human-readable, but less efficient for analytical queries, especially on wide tables, as the entire row must be read even if only a few columns are needed. Schema inference can also add overhead.\n",
        "    -   **JSON:** Flexible for semi-structured data but can be less performant than columnar formats for highly structured data due to parsing overhead.\n",
        "\n",
        "-   **Partitioning Data at Rest:** Storing data partitioned in the file system (e.g., `.../year=2023/month=01/...`) allows Spark to use partition pruning. If a query includes a filter on the partition column (e.g., `WHERE year = 2023`), Spark can read data only from the relevant directories, significantly reducing I/O. This is common with formats like Parquet and ORC.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Continue printing the explanation for Using the Spark UI for Monitoring and Tuning\n",
        "print(\"\\n--- Using the Spark UI for Monitoring and Tuning ---\")\n",
        "print(\"\"\"\n",
        "The Spark UI (accessible via a web browser at `http://<driver-node>:4040` by default when a Spark application is running) is an invaluable tool for understanding and optimizing Spark jobs.\n",
        "\n",
        "Key areas in the Spark UI for performance analysis:\n",
        "-   **Jobs Tab:** Provides an overview of all jobs, their status, and duration.\n",
        "-   **Stages Tab:** Shows the different stages of a job, including the number of tasks, duration, and input/output metrics. This is where you can often identify bottlenecks.\n",
        "-   **Tasks Tab:** Detailed information about individual tasks within a stage, including runtime, locality, and shuffle read/write. Helps identify skewed tasks.\n",
        "-   **SQL Tab:** Shows the logical and physical plan generated by the Catalyst optimizer for Spark SQL queries and DataFrame operations. This is critical for verifying if optimizations like predicate pushdown or broadcast joins are being applied.\n",
        "-   **Storage Tab:** Shows information about cached/persisted RDDs and DataFrames.\n",
        "\n",
        "By examining the Spark UI, you can identify:\n",
        "-   Stages with high shuffle read/write (indicating excessive shuffling).\n",
        "-   Skewed stages (tasks with vastly different runtimes).\n",
        "-   Stages with poor data locality (tasks reading data from remote nodes).\n",
        "-   Whether broadcast joins are being used as expected.\n",
        "-   The effectiveness of caching.\n",
        "\n",
        "Monitoring the Spark UI is a fundamental step in diagnosing and resolving performance issues in Spark applications.\n",
        "\"\"\")\n",
        "\n",
        "# Stop the SparkSession - Assuming this is the end of the task involving Spark\n",
        "# spark.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "... needs to be read from storage and processed in subsequent stages.\n",
            "    Example: `spark.read.parquet(\"/path/to/data\").filter(\"column > 10\")` - Spark will try to apply the filter `column > 10` when reading the Parquet file, potentially skipping entire row groups that don't satisfy the condition.\n",
            "\n",
            "-   **Choosing Appropriate Data Formats:**\n",
            "    -   **Columnar Formats (Parquet, ORC):** Highly recommended for analytical workloads. They enable column pruning and often provide better compression. Reading specific columns is much faster than in row-based formats.\n",
            "    -   **Text/CSV:** Simple, human-readable, but less efficient for analytical queries, especially on wide tables, as the entire row must be read even if only a few columns are needed. Schema inference can also add overhead.\n",
            "    -   **JSON:** Flexible for semi-structured data but can be less performant than columnar formats for highly structured data due to parsing overhead.\n",
            "\n",
            "-   **Partitioning Data at Rest:** Storing data partitioned in the file system (e.g., `.../year=2023/month=01/...`) allows Spark to use partition pruning. If a query includes a filter on the partition column (e.g., `WHERE year = 2023`), Spark can read data only from the relevant directories, significantly reducing I/O. This is common with formats like Parquet and ORC.\n",
            "\n",
            "\n",
            "--- Using the Spark UI for Monitoring and Tuning ---\n",
            "\n",
            "The Spark UI (accessible via a web browser at `http://<driver-node>:4040` by default when a Spark application is running) is an invaluable tool for understanding and optimizing Spark jobs.\n",
            "\n",
            "Key areas in the Spark UI for performance analysis:\n",
            "-   **Jobs Tab:** Provides an overview of all jobs, their status, and duration.\n",
            "-   **Stages Tab:** Shows the different stages of a job, including the number of tasks, duration, and input/output metrics. This is where you can often identify bottlenecks.\n",
            "-   **Tasks Tab:** Detailed information about individual tasks within a stage, including runtime, locality, and shuffle read/write. Helps identify skewed tasks.\n",
            "-   **SQL Tab:** Shows the logical and physical plan generated by the Catalyst optimizer for Spark SQL queries and DataFrame operations. This is critical for verifying if optimizations like predicate pushdown or broadcast joins are being applied.\n",
            "-   **Storage Tab:** Shows information about cached/persisted RDDs and DataFrames.\n",
            "\n",
            "By examining the Spark UI, you can identify:\n",
            "-   Stages with high shuffle read/write (indicating excessive shuffling).\n",
            "-   Skewed stages (tasks with vastly different runtimes).\n",
            "-   Stages with poor data locality (tasks reading data from remote nodes).\n",
            "-   Whether broadcast joins are being used as expected.\n",
            "-   The effectiveness of caching.\n",
            "\n",
            "Monitoring the Spark UI is a fundamental step in diagnosing and resolving performance issues in Spark applications.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada325ab"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "## Summary of Solving Process\n",
        "\n",
        "The user requested a comprehensive explanation and demonstration of several core Spark concepts using PySpark. The task was broken down into seven subtasks, each focusing on a specific topic: User Defined Functions (UDFs), DataFrames & Datasets, Interoperating with RDDs, JSON and Parquet File Formats, Loading Data through Different Sources, Spark-Hive Integration, Advanced Transformations & Actions, and Optimization Techniques.\n",
        "\n",
        "For each subtask, the process involved generating and executing Python code using PySpark. This code included detailed explanations of the concepts, followed by practical code examples demonstrating their usage with Spark DataFrames.\n",
        "\n",
        "Several challenges were encountered during the process, primarily related to the execution environment:\n",
        "1.  **Stopped SparkSession/SparkContext:** Multiple initial attempts across various subtasks failed because the SparkSession or its underlying SparkContext was not active or properly initialized. This was resolved by explicitly creating a new SparkSession at the beginning of each code block using `SparkSession.builder.appName(...).getOrCreate()`, and in one case, explicitly stopping any existing session first.\n",
        "2.  **Syntax Errors:** One initial attempt resulted in a `SyntaxError`, which was resolved by regenerating the code to ensure correct Python syntax, particularly with multiline strings.\n",
        "3.  **Missing Data Files:** For the \"Loading Data through Different Sources\" subtask, dummy CSV and JSON files were programmatically created to ensure the read operations could be demonstrated reliably, as the existence of files from previous steps was not guaranteed.\n",
        "4.  **Missing Hive Metastore:** The \"Spark-Hive Integration\" subtask's attempts to create, load, and query Hive tables failed due to the absence of a connected Hive metastore in the execution environment. While the code examples were provided, their execution could not be fully demonstrated, and this limitation was explicitly noted in the explanation.\n",
        "5.  **Incorrect DataFrame Schema:** In the \"Optimization Techniques\" subtask, an initial attempt to demonstrate a broadcasting join failed because the sample DataFrame was missing a required join column ('Department'). This was corrected by modifying the sample data to include the necessary column.\n",
        "6.  **Truncated Output:** In the final subtask on Optimization Techniques, the output was initially truncated, requiring a subsequent step to print the remaining content.\n",
        "\n",
        "Despite these challenges, each subtask was ultimately completed by refining the code and providing appropriate explanations and acknowledgments of environmental dependencies where necessary. The final output for each section included the requested explanations and demonstrated code examples (where feasible).\n",
        "\n",
        "## Data Analysis Key Findings\n",
        "\n",
        "*   **User Defined Functions (UDFs):** UDFs allow extending Spark's functionality with custom Python code, applicable to single or multiple columns. However, they can introduce performance overhead due to serialization/deserialization and acting as \"black boxes\" to the Catalyst optimizer, making built-in functions generally preferable.\n",
        "*   **DataFrames:** DataFrames provide a high-level, schema-aware, distributed data structure in Spark, optimized by the Catalyst optimizer for performance and ease of use compared to RDDs for structured data.\n",
        "*   **Datasets:** Primarily a Scala/Java API concept, Datasets offer compile-time type safety. In PySpark, DataFrames are the main abstraction for structured data, acting as untyped Datasets of `Row` objects.\n",
        "*   **RDD-DataFrame Interoperability:** Spark allows conversion between RDDs and DataFrames using methods like `.toDF()` (with optional explicit schema) and `.rdd`. While DataFrames are preferred, dropping to RDDs can be necessary for low-level transformations or integrating with legacy code.\n",
        "*   **JSON Format:** Flexible for semi-structured data and human-readable, easily read and written by Spark DataFrames.\n",
        "*   **Parquet Format:** A columnar storage format highly beneficial for Spark analytics due to column pruning, better compression, schema evolution support, and optimization by Catalyst, leading to improved I/O and query performance. Spark can read and write Parquet data efficiently, automatically handling schema.\n",
        "*   **Loading Data:** Spark's `spark.read` interface provides a unified way to load data from various sources (CSV, JSON, Parquet, JDBC, etc.) into DataFrames, with format-specific options and automatic or explicit schema handling.\n",
        "*   **Spark-Hive Integration:** Enables Spark to query and interact with data stored in Hive tables via Spark SQL, leveraging Hive's metastore for schema. Requires proper configuration (`hive-site.xml`) and dependencies.\n",
        "*   **Transformations vs. Actions:** Transformations are lazy operations that build a logical plan, while actions trigger computation and return results. Lazy evaluation is key to Spark's optimization.\n",
        "*   **Advanced Transformations:** PySpark DataFrames support advanced operations like Window Functions (for calculations over related rows), various types of Joins (inner, outer, etc.), and advanced Aggregations (`groupBy().agg()`, `pivot()`) for complex data processing.\n",
        "*   **Advanced Actions:** Beyond `show()`, actions like `collect()`, `take()`, `count()`, and `write()/save()` trigger computations and either return data to the driver or save it externally. `foreach()` executes functions on worker nodes.\n",
        "*   **Optimization Techniques:** Spark's performance relies heavily on the **Catalyst Optimizer** (predicate pushdown, column pruning, join reordering) and **Lazy Evaluation**. **Caching/Persisting** DataFrames avoids recomputation. **Broadcasting** smaller DataFrames during joins eliminates shuffling the larger dataset. **Shuffling**, caused by wide transformations, is expensive, and can be mitigated by minimizing wide transformations, broadcasting, appropriate partitioning, and tuning `spark.sql.shuffle.partitions`. Using **columnar data formats** (Parquet/ORC) and **partitioning data at rest** further improves performance by reducing I/O. The **Spark UI** is essential for monitoring and identifying bottlenecks.\n",
        "\n",
        "## Insights or Next Steps\n",
        "\n",
        "*   When working with structured data in PySpark, prioritize DataFrame operations and built-in functions over UDFs for better performance due to Catalyst optimization.\n",
        "*   For analytical workloads and large datasets, prefer columnar file formats like Parquet or ORC over row-based formats like CSV or JSON to leverage column pruning and improved compression.\n"
      ]
    }
  ]
}