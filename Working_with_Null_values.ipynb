{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpavSZa8dLcTFOtw+DAHwq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvikas79/Spark-Tutorials/blob/main/Working_with_Null_values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Spark Introduction\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "REl2y1eZRTBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).option(\"mode\", \"PERMISSIVE\").load(\"netflix_titles.csv\")"
      ],
      "metadata": {
        "id": "6T6wMG0pRSp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X9eZ7PO_Rvcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show()"
      ],
      "metadata": {
        "id": "UJsSdixrSAGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxnAGF20M78M"
      },
      "outputs": [],
      "source": [
        "#Count null / NaN values per column\n",
        "\n",
        "from pyspark.sql.functions import col, sum, isnan, when\n",
        "\n",
        "data.select([\n",
        "    sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)).alias(c)\n",
        "    for c in data.columns]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sVX3gUynO_Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter rows having any null or NaN\n",
        "from pyspark.sql.functions import col, isnan\n",
        "\n",
        "data.filter(\n",
        "    data.select([col(c).isNull() | isnan(col(c)) for c in data.columns])\n",
        "      .reduce(lambda x, y: x | y)).show()\n",
        "#Shows only rows where at least one column has null/NaN."
      ],
      "metadata": {
        "id": "dv0Zd53kO_ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter rows with null in a specific column\n",
        "data.filter(col(\"director\").isNull() | isnan(col(\"director\"))).show()"
      ],
      "metadata": {
        "id": "V5fVjG5aO_Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get total number of missing values in the whole DataFrame\n",
        "from pyspark.sql.functions import col, sum, isnan, when\n",
        "\n",
        "missing_count = data.select([\n",
        "    sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0))\n",
        "    for c in data.columns\n",
        "]).rdd.flatMap(lambda x: x).sum()\n",
        "\n",
        "print(f\"Total missing values: {missing_count}\")\n"
      ],
      "metadata": {
        "id": "fn35bJ9CO_Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace all nulls with a specific value (e.g., 0)\n",
        "\n",
        "df_filled = df.fillna(0)\n"
      ],
      "metadata": {
        "id": "AOy3i3EsO-_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace nulls with different types (string vs numeric)\n",
        "\n",
        "df_filled = df.fillna({\"name\": \"Unknown\", \"age\": 0})\n"
      ],
      "metadata": {
        "id": "sqhcBYvCO-84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace null values in a specific column\n",
        "\n",
        "df_filled = df.fillna({\"age\": 0})\n"
      ],
      "metadata": {
        "id": "jybl0JN5O-7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace null using na.fill() (alias for fillna)\n",
        "\n",
        "df_filled = df.na.fill(\"missing\")     # replace nulls in all string cols\n",
        "df_filled = df.na.fill(0)             # replace nulls in all numeric cols\n",
        "df_filled = df.na.fill({\"city\": \"Unknown\", \"salary\": 0})\n"
      ],
      "metadata": {
        "id": "ZJIdvgeTO-iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace null values with a computed value\n",
        "#Sometimes you want to replace null with a mean, median, or mode:\n",
        "\n",
        "from pyspark.sql.functions import col, mean\n",
        "\n",
        "# Example: replace nulls in \"salary\" with mean salary\n",
        "mean_val = df.select(mean(col(\"salary\"))).collect()[0][0]\n",
        "\n",
        "df_filled = df.na.fill({\"salary\": mean_val})\n"
      ],
      "metadata": {
        "id": "q09sJK5_P5JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace null values using when + otherwise\n",
        "\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "df_filled = df.withColumn(\n",
        "    \"age\",\n",
        "    when(col(\"age\").isNull(), 0).otherwise(col(\"age\"))\n",
        ")\n"
      ],
      "metadata": {
        "id": "PtyTRp5fP8vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace NaN values with a constant\n",
        "\n",
        "from pyspark.sql.functions import when, col, isnan\n",
        "\n",
        "df_filled = df.withColumn(\n",
        "    \"age\",\n",
        "    when(col(\"age\").isNull() | isnan(col(\"age\")), 0).otherwise(col(\"age\"))\n",
        ")\n",
        "#This replaces both null and NaN in the age column with 0."
      ],
      "metadata": {
        "id": "yaJD_pX3P8sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace across multiple columns\n",
        "\n",
        "from pyspark.sql.functions import when, col, isnan\n",
        "\n",
        "for c in [\"age\", \"salary\"]:\n",
        "    df = df.withColumn(\n",
        "        c,\n",
        "        when(col(c).isNull() | isnan(col(c)), 0).otherwise(col(c))\n",
        "    )\n"
      ],
      "metadata": {
        "id": "hn5bLWB1P8ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use na.fill() for null + na.replace() for NaN\n",
        "# Replace nulls\n",
        "\n",
        "df = df.na.fill({\"age\": 0, \"salary\": 0})\n",
        "\n",
        "# Replace NaN values with 0\n",
        "\n",
        "df = df.na.replace(float(\"nan\"), 0)\n"
      ],
      "metadata": {
        "id": "yKxkR1b5P8mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One-liner function for any DataFrame\n",
        "\n",
        "from pyspark.sql.functions import when, col, isnan\n",
        "\n",
        "def replace_null_nan(df, replacements: dict):\n",
        "    for c, val in replacements.items():\n",
        "        df = df.withColumn(\n",
        "            c,\n",
        "            when(col(c).isNull() | isnan(col(c)), val).otherwise(col(c))\n",
        "        )\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "\n",
        "df_filled = replace_null_nan(df, {\"age\": 0, \"salary\": 0, \"city\": \"Unknown\"})\n"
      ],
      "metadata": {
        "id": "Muja6dDaP8kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OkpObhyZP8il"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}